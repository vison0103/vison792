{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os"
      ],
      "metadata": {
        "id": "Z4oAHOR6Eb1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b6c17a-ec98-4747-d379-012ff7a71408"
      },
      "id": "Z4oAHOR6Eb1P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GTSRB\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/myvit/datasets.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\") # 我使用的解压位置"
      ],
      "metadata": {
        "id": "QZTmwROb-Wjn"
      },
      "id": "QZTmwROb-Wjn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BTC\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/myvit/BelgiumTSC.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/BelgiumTSC\") # 我使用的解压位置"
      ],
      "metadata": {
        "id": "gNCFGun_aaBR"
      },
      "id": "gNCFGun_aaBR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TSRD\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/myvit/TSRD_dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/TSRD\") # 我使用的解压位置"
      ],
      "metadata": {
        "id": "O28cIVXIok8q"
      },
      "id": "O28cIVXIok8q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -V\n",
        "import torch\n",
        "torch.version.cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "gAMxFB_1Mr-p",
        "outputId": "cb1f7781-59fa-4fa5-a478-62f779a24a23"
      },
      "id": "gAMxFB_1Mr-p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'12.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/colab/apex\")# 跳转到/content/gdrive/MyDrive/coLab/apex目录"
      ],
      "metadata": {
        "id": "mJb4xFGDM6Lu"
      },
      "id": "mJb4xFGDM6Lu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install packaging\n",
        "!pip install timm\n",
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqdFnqUUjg5j",
        "outputId": "db1cb33b-3ee9-406c-aa8c-7ff2783d3fbf"
      },
      "id": "EqdFnqUUjg5j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/colab/apex\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "d8bQIDs2PF44"
      },
      "id": "d8bQIDs2PF44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c742b9-e2b3-4530-9baa-2e951fe0ee47",
      "metadata": {
        "id": "f5c742b9-e2b3-4530-9baa-2e951fe0ee47"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import classification_report\n",
        "from timm.data.mixup import Mixup\n",
        "from timm.loss import SoftTargetCrossEntropy\n",
        "from timm.models.mobilevit import mobilevit_s\n",
        "#from apex import amp\n",
        "import warnings\n",
        "import json\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from einops import rearrange\n",
        "from torch import nn\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21fd1282-7415-42f1-b9a2-92284d6d2ca1",
      "metadata": {
        "id": "21fd1282-7415-42f1-b9a2-92284d6d2ca1"
      },
      "outputs": [],
      "source": [
        "# 设置全局参数\n",
        "model_lr = 1e-4\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "use_amp=False #是否使用混合精度\n",
        "#classes=54\n",
        "classes=43\n",
        "#classes=53\n",
        "CLIP_GRAD=5.0\n",
        "# 数据预处理7\n",
        "#p.value.\n",
        "#t.TEST使用如t检验或Wilcoxon符号秩检验"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),  # 调整明暗度、对比度、饱和度\n",
        "    transforms.RandomRotation(degrees=15),  # 在[-15, 15]度范围内随机旋转\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 在x和y方向上各随机平移图像的10%\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "mixup_fn = Mixup(\n",
        "    mixup_alpha=0.8, cutmix_alpha=0.0, cutmix_minmax=None,\n",
        "    prob=0.1, switch_prob=0.5, mode='batch',\n",
        "    label_smoothing=0.1, num_classes=classes)\n"
      ],
      "metadata": {
        "id": "xOZFmLZ9LL_p"
      },
      "id": "xOZFmLZ9LL_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#德国\n",
        "dataset_train = datasets.ImageFolder('/content/datasets/train', transform=transform)\n",
        "dataset_test = datasets.ImageFolder('/content/datasets/test', transform=transform_test)\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "#德国\n",
        "print(dataset_train.class_to_idx)\n",
        "with open('/content/datasets/class.txt','w') as file:\n",
        "    file.write(str(dataset_train.class_to_idx))\n",
        "with open('/content/datasets/class.json','w',encoding='utf-8') as file:\n",
        "    file.write(json.dumps(dataset_train.class_to_idx))"
      ],
      "metadata": {
        "id": "ga-A8f4M_7TL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea4f791-346b-48b3-e2c2-cc78efec771c"
      },
      "id": "ga-A8f4M_7TL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'CarefulBikes': 0, 'CarefulChildren': 1, 'CarefulDangerous': 2, 'CarefulPedestrians': 3, 'CarefulSlipping': 4, 'CarefulSnow': 5, 'CarefulTrafficLight': 6, 'ConstructionRoad': 7, 'LeftTurn': 8, 'MainRoutePriority': 9, 'MustGoStraight': 10, 'MustGoStraightOrLeft': 11, 'MustGoStraightOrRight': 12, 'MustLeftSide': 13, 'MustRightSide': 14, 'MustTurnLeft': 15, 'MustTurnRight': 16, 'NeedsToYield': 17, 'NoAccess': 18, 'NoEntry': 19, 'NoOvertaking': 20, 'NoOvertakingRemove': 21, 'NoOvertakingTrucks': 22, 'NoOvertakingTrucksRemove': 23, 'NoTrucks': 24, 'PriorityRoad': 25, 'RightTurn': 26, 'RoadNarrowing': 27, 'SharpTurn': 28, 'SpeedLimit100': 29, 'SpeedLimit120': 30, 'SpeedLimit20': 31, 'SpeedLimit30': 32, 'SpeedLimit50': 33, 'SpeedLimit60': 34, 'SpeedLimit70': 35, 'SpeedLimit80': 36, 'SpeedLimit80Lifted': 37, 'Stop': 38, 'Turntable': 39, 'UnevenRoad': 40, 'Unrestrict': 41, 'WildAnimals': 42}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#比利时\n",
        "dataset_train = datasets.ImageFolder('/content/BelgiumTSC/BelgiumTSC/Training', transform=transform)\n",
        "dataset_test = datasets.ImageFolder('/content/BelgiumTSC/BelgiumTSC/Testing', transform=transform_test)\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(dataset_train.class_to_idx)\n",
        "with open('/content/BelgiumTSC/BelgiumTSC/class.txt','w') as file:\n",
        "    file.write(str(dataset_train.class_to_idx))\n",
        "with open('/content/BelgiumTSC/BelgiumTSC/class.json','w',encoding='utf-8') as file:\n",
        "    file.write(json.dumps(dataset_train.class_to_idx))"
      ],
      "metadata": {
        "id": "_Ah8BgI-bzKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e338dc0-5ef2-4487-fd8d-f2db093dd62a"
      },
      "id": "_Ah8BgI-bzKd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'00000': 0, '00001': 1, '00002': 2, '00003': 3, '00004': 4, '00005': 5, '00006': 6, '00007': 7, '00008': 8, '00010': 9, '00012': 10, '00013': 11, '00014': 12, '00016': 13, '00017': 14, '00018': 15, '00019': 16, '00020': 17, '00021': 18, '00022': 19, '00023': 20, '00024': 21, '00025': 22, '00027': 23, '00028': 24, '00029': 25, '00030': 26, '00031': 27, '00032': 28, '00034': 29, '00035': 30, '00037': 31, '00038': 32, '00039': 33, '00040': 34, '00041': 35, '00042': 36, '00043': 37, '00044': 38, '00045': 39, '00046': 40, '00047': 41, '00049': 42, '00051': 43, '00053': 44, '00054': 45, '00055': 46, '00056': 47, '00057': 48, '00058': 49, '00059': 50, '00060': 51, '00061': 52}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#中国\n",
        "dataset_train = datasets.ImageFolder('/content/TSRD/TSRD_dataset/Train', transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataset_test = datasets.ImageFolder('/content/TSRD/TSRD_dataset/Test', transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(dataset_train.class_to_idx)\n",
        "with open('/content/TSRD/TSRD_dataset/class.txt','w') as file:\n",
        "    file.write(str(dataset_train.class_to_idx))\n",
        "with open('/content/TSRD/TSRD_dataset/class.json','w',encoding='utf-8') as file:\n",
        "    file.write(json.dumps(dataset_train.class_to_idx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_29uBPNup0nI",
        "outputId": "faada686-943d-4c1f-d1aa-12b64f6f9828"
      },
      "id": "_29uBPNup0nI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'000': 0, '001': 1, '002': 2, '003': 3, '004': 4, '005': 5, '006': 6, '007': 7, '008': 8, '010': 9, '011': 10, '012': 11, '013': 12, '014': 13, '015': 14, '016': 15, '017': 16, '020': 17, '021': 18, '022': 19, '023': 20, '024': 21, '025': 22, '026': 23, '027': 24, '028': 25, '029': 26, '030': 27, '031': 28, '032': 29, '034': 30, '035': 31, '036': 32, '037': 33, '038': 34, '039': 35, '040': 36, '041': 37, '042': 38, '043': 39, '044': 40, '045': 41, '046': 42, '047': 43, '048': 44, '049': 45, '050': 46, '051': 47, '052': 48, '053': 49, '054': 50, '055': 51, '056': 52, '057': 53}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tssd\n",
        "dataset_train = datasets.ImageFolder('/content/TSSD/TSSD/Train', transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dataset_test = datasets.ImageFolder('/content/TSSD/TSSD/Test', transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(dataset_train.class_to_idx)\n",
        "with open('/content/TSSD/TSSD/class.txt','w') as file:\n",
        "    file.write(str(dataset_train.class_to_idx))\n",
        "with open('/content/TSSD/TSSD/class.json','w',encoding='utf-8') as file:\n",
        "    file.write(json.dumps(dataset_train.class_to_idx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR2QHxjoVUTh",
        "outputId": "eb3f4a81-7fca-4177-cd60-7c8e5233729f"
      },
      "id": "eR2QHxjoVUTh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'0': 0, '1': 1, '10': 2, '11': 3, '12': 4, '13': 5, '14': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '8': 13, '9': 14}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 检查训练集和测试集的类别数量\n",
        "train_classes = dataset_train.class_to_idx\n",
        "test_classes = dataset_test.class_to_idx\n",
        "\n",
        "# 打印类别数量\n",
        "print(\"Number of classes in training set:\", len(train_classes))\n",
        "print(\"Number of classes in testing set:\", len(test_classes))\n",
        "\n",
        "# 检查类别数量是否相等\n",
        "if len(train_classes) == len(test_classes):\n",
        "    print(\"Both datasets have the same number of classes.\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of classes between datasets.\")\n",
        "\n",
        "# 检查具体的类别是否一致\n",
        "train_class_set = set(train_classes.keys())\n",
        "test_class_set = set(test_classes.keys())\n",
        "\n",
        "if train_class_set == test_class_set:\n",
        "    print(\"All classes match between training and testing sets.\")\n",
        "else:\n",
        "    print(\"There are differences in class names between the datasets.\")\n",
        "    # 找出不匹配的类别\n",
        "    only_in_train = train_class_set - test_class_set\n",
        "    only_in_test = test_class_set - train_class_set\n",
        "    print(\"Classes only in training set:\", only_in_train)\n",
        "    print(\"Classes only in testing set:\", only_in_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76Se5oQfPV0-",
        "outputId": "866853ae-d55b-43b1-8062-5837b19700de"
      },
      "id": "76Se5oQfPV0-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes in training set: 43\n",
            "Number of classes in testing set: 43\n",
            "Both datasets have the same number of classes.\n",
            "All classes match between training and testing sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add CBMA"
      ],
      "metadata": {
        "id": "tKbQaiHqdODz"
      },
      "id": "tKbQaiHqdODz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3b7b359-67aa-4600-9832-8c88c7f17aef",
      "metadata": {
        "id": "a3b7b359-67aa-4600-9832-8c88c7f17aef"
      },
      "outputs": [],
      "source": [
        "#Add CBAM\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "\tif min_value is None:\n",
        "\t\tmin_value = divisor\n",
        "\tnew_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "\t# Make sure that round down does not go down by more than 10%.\n",
        "\tif new_v < 0.9 * v:\n",
        "\t\tnew_v += divisor\n",
        "\treturn new_v\n",
        "\n",
        "\n",
        "def Conv_BN_ReLU(inp, oup, kernel, stride=1):\n",
        "\treturn nn.Sequential(\n",
        "\t\tnn.Conv2d(inp, oup, kernel_size=kernel, stride=stride, padding=1, bias=False),\n",
        "\t\tnn.BatchNorm2d(oup),\n",
        "\t\tnn.ReLU6(inplace=True)\n",
        "\t)\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "\treturn nn.Sequential(\n",
        "\t\tnn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "\t\tnn.BatchNorm2d(oup),\n",
        "\t\tnn.ReLU6(inplace=True)\n",
        "\t)\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "\tdef __init__(self, dim, fn):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.norm = nn.LayerNorm(dim)\n",
        "\t\tself.fn = fn\n",
        "\n",
        "\tdef forward(self, x, **kwargs):\n",
        "\t\treturn self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\tdef __init__(self, dim, hidden_dim, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.ffn = nn.Sequential(\n",
        "\t\t\tnn.Linear(dim, hidden_dim),\n",
        "\t\t\tnn.SiLU(),\n",
        "\t\t\tnn.Dropout(dropout),\n",
        "\t\t\tnn.Linear(hidden_dim, dim),\n",
        "\t\t\tnn.Dropout(dropout)\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\treturn self.ffn(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\tdef __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tinner_dim = dim_head * heads\n",
        "\t\tproject_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "\t\tself.heads = heads\n",
        "\t\tself.scale = dim_head ** -0.5\n",
        "\n",
        "\t\tself.attend = nn.Softmax(dim=-1)\n",
        "\t\tself.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "\n",
        "\t\tself.to_out = nn.Sequential(\n",
        "\t\t\tnn.Linear(inner_dim, dim),\n",
        "\t\t\tnn.Dropout(dropout)\n",
        "\t\t) if project_out else nn.Identity()\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tqkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "\t\tq, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h=self.heads), qkv)\n",
        "\n",
        "\t\tdots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\t\tattn = self.attend(dots)\n",
        "\t\tout = torch.matmul(attn, v)\n",
        "\t\tout = rearrange(out, 'b p h n d -> b p n (h d)')\n",
        "\t\treturn self.to_out(out)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\tdef __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.layers = nn.ModuleList([])\n",
        "\t\tfor _ in range(depth):\n",
        "\t\t\tself.layers.append(nn.ModuleList([\n",
        "\t\t\t\tPreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "\t\t\t\tPreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
        "\t\t\t]))\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tfor attn, ff in self.layers:\n",
        "\t\t\tx = attn(x) + x\n",
        "\t\t\tx = ff(x) + x\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
        "        padding = 3 if kernel_size == 7 else 1\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.ca = ChannelAttention(in_planes, ratio)\n",
        "        self.sa = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x * self.ca(x)\n",
        "        return out * self.sa(out)\n",
        "\n",
        "class MV2Block(nn.Module):\n",
        "    def __init__(self, inp, oup, stride=1, expand_ratio=4):\n",
        "        super(MV2Block, self).__init__()\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = round(inp * expand_ratio)\n",
        "        self.identity = stride == 1 and inp == oup\n",
        "\n",
        "        if expand_ratio == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                # nn.ReLU6(inplace=True),\n",
        "                nn.SiLU(),\n",
        "                # CBAM 注意力机制放置在 3x3 卷积层之后\n",
        "                CBAM(hidden_dim),  # 添加 CBAM 注意力机制\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                # nn.ReLU6(inplace=True),\n",
        "                nn.SiLU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                # nn.ReLU6(inplace=True),\n",
        "                nn.SiLU(),\n",
        "                # CBAM 注意力机制放置在 3x3 卷积层之后\n",
        "                CBAM(hidden_dim),  # 添加 CBAM 注意力机制\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.identity:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileViTBlock(nn.Module):\n",
        "\tdef __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.ph, self.pw = patch_size\n",
        "\n",
        "\t\tself.conv1 = Conv_BN_ReLU(channel, channel, kernel_size)\n",
        "\t\tself.conv2 = conv_1x1_bn(channel, dim)\n",
        "\n",
        "\t\tself.transformer = Transformer(dim, depth, 1, 32, mlp_dim, dropout)\n",
        "\n",
        "\t\tself.conv3 = conv_1x1_bn(dim, channel)\n",
        "\t\tself.conv4 = Conv_BN_ReLU(2 * channel, channel, kernel_size)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\ty = x.clone()\n",
        "\n",
        "\t\t# Local representations\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = self.conv2(x)\n",
        "\n",
        "\t\t# Global representations\n",
        "\t\t_, _, h, w = x.shape\n",
        "\t\tx = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)\n",
        "\t\tx = self.transformer(x)\n",
        "\t\tx = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h // self.ph, w=w // self.pw, ph=self.ph,\n",
        "\t\t              pw=self.pw)\n",
        "\n",
        "\t\t# Fusion\n",
        "\t\tx = self.conv3(x)\n",
        "\t\tx = torch.cat((x, y), 1)\n",
        "\t\tx = self.conv4(x)\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "class MobileVit_v1(nn.Module):\n",
        "\tdef __init__(self, image_size, dims, channels, num_classes, expansion=4, kernel_size=3, patch_size=(2, 2)):\n",
        "\t\tsuper().__init__()\n",
        "\t\tih, iw = image_size\n",
        "\t\tph, pw = patch_size\n",
        "\t\tassert ih % ph == 0 and iw % pw == 0\n",
        "\n",
        "\t\tL = [2, 4, 3]\n",
        "\n",
        "\t\tself.conv1 = Conv_BN_ReLU(3, channels[0], kernel=3, stride=2)\n",
        "\n",
        "\t\tself.mv2 = nn.ModuleList([])\n",
        "\t\tself.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))  # Repeat\n",
        "\t\tself.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
        "\n",
        "\t\tself.mvit = nn.ModuleList([])\n",
        "\t\tself.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0] * 2)))\n",
        "\t\tself.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1] * 4)))\n",
        "\t\tself.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2] * 4)))\n",
        "\n",
        "\t\tself.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
        "\n",
        "\t\tself.pool = nn.AvgPool2d(ih // 32, 1)\n",
        "\t\tself.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = self.mv2[0](x)\n",
        "\n",
        "\t\tx = self.mv2[1](x)\n",
        "\t\tx = self.mv2[2](x)\n",
        "\t\tx = self.mv2[3](x)  # Repeat\n",
        "\n",
        "\t\tx = self.mv2[4](x)\n",
        "\t\tx = self.mvit[0](x)\n",
        "\n",
        "\t\tx = self.mv2[5](x)\n",
        "\t\tx = self.mvit[1](x)\n",
        "\n",
        "\t\tx = self.mv2[6](x)\n",
        "\t\tx = self.mvit[2](x)\n",
        "\t\tx = self.conv2(x)\n",
        "\n",
        "\t\tx = self.pool(x).view(-1, x.shape[1])\n",
        "\t\tx = self.fc(x)\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "def mobilevit_xxs_v1():\n",
        "\tdims = [64, 80, 96]\n",
        "\tchannels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320]\n",
        "\treturn MobileVit_v1((256, 256), dims, channels, num_classes=1000, expansion=2)\n",
        "\n",
        "\n",
        "def mobilevit_xs_v1():\n",
        "\tdims = [96, 120, 144]\n",
        "\tchannels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384]\n",
        "\treturn MobileVit_v1((256, 256), dims, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def mobilevit_s_v1():\n",
        "\tdims = [144, 192, 240]\n",
        "\tchannels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640]\n",
        "\treturn MobileVit_v1((256, 256), dims, channels, num_classes=54)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cc63f01-78da-4881-a6b5-a8e3ee47cc67",
      "metadata": {
        "id": "3cc63f01-78da-4881-a6b5-a8e3ee47cc67"
      },
      "outputs": [],
      "source": [
        "# 实例化模型并且移动到GPU\n",
        "criterion_train = SoftTargetCrossEntropy()# 训练用的loss\n",
        "criterion_val = torch.nn.CrossEntropyLoss()# 验证用的loss\n",
        "model_ft_v1 = mobilevit_s_v1()# 定义模型，并设置预训练# 修改类别\n",
        "model_ft_v1.to(DEVICE)\n",
        "# 选择简单暴力的Adam优化器，学习率调低\n",
        "optimizer = optim.Adam(model_ft_v1.parameters(), lr=model_lr)\n",
        "cosine_schedule = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=20, eta_min=1e-6)# 使用余弦退火算法调整学习率\n",
        "if use_amp: #如果使用混合精度训练，则初始化amp。\n",
        "    model, optimizer = amp.initialize(model_ft_v1, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”\n",
        "if torch.cuda.device_count() > 1: #检测是否存在多张显卡，如果存在则使用DP的方式并行训练\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model_ft_v1 = torch.nn.DataParallel(model_ft_v1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d119f4bf-8fc6-4d6e-9275-3c1c9fd4ef55",
      "metadata": {
        "id": "d119f4bf-8fc6-4d6e-9275-3c1c9fd4ef55"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    sum_loss = 0\n",
        "    total_num = len(train_loader.dataset)\n",
        "    print(total_num, len(train_loader))\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if len(data) % 2 != 0:\n",
        "            if len(data) < 2:\n",
        "                continue\n",
        "            data = data[0:len(data) - 1]\n",
        "            target = target[0:len(target) - 1]\n",
        "            print(len(data))\n",
        "        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "        samples, targets = mixup_fn(data, target)\n",
        "        output = model(data)\n",
        "        loss = criterion_train(output, targets)\n",
        "        optimizer.zero_grad()\n",
        "        if use_amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), CLIP_GRAD)\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n",
        "        optimizer.step()\n",
        "        lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
        "        print_loss = loss.data.item()\n",
        "        sum_loss += print_loss\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR:{:.9f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item(), lr))\n",
        "    ave_loss = sum_loss / len(train_loader)\n",
        "    print('epoch:{},loss:{}'.format(epoch, ave_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b805dc-fd88-4e2b-a492-cda305cfc207",
      "metadata": {
        "id": "46b805dc-fd88-4e2b-a492-cda305cfc207"
      },
      "outputs": [],
      "source": [
        "ACC = 0\n",
        "# 验证过程\n",
        "def val(model, device, test_loader):\n",
        "    global ACC\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total_num = len(test_loader.dataset)\n",
        "    print(total_num, len(test_loader))\n",
        "    val_list = []\n",
        "    pred_list = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            for t in target:\n",
        "                val_list.append(t.data.item())\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion_val(output, target)\n",
        "            _, pred = torch.max(output.data, 1)\n",
        "            for p in pred:\n",
        "                pred_list.append(p.data.item())\n",
        "            correct += torch.sum(pred == target)\n",
        "            print_loss = loss.data.item()\n",
        "            test_loss += print_loss\n",
        "        correct = correct.data.item()\n",
        "        acc = correct / total_num\n",
        "        avgloss = test_loss / len(test_loader)\n",
        "        print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            avgloss, correct, len(test_loader.dataset), 100 * acc))\n",
        "        if acc > ACC:\n",
        "            if isinstance(model, torch.nn.DataParallel):\n",
        "                torch.save(model.module, 'modelv1_TSRD_m' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n",
        "            else:\n",
        "                torch.save(model, 'modelv1_TSRD_m' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n",
        "            ACC = acc\n",
        "    return val_list, pred_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "856194a3-47fb-4b7e-8ff7-1366d1aaa725",
      "metadata": {
        "id": "856194a3-47fb-4b7e-8ff7-1366d1aaa725"
      },
      "outputs": [],
      "source": [
        "# 训练\n",
        "torch.cuda.empty_cache()\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "is_set_lr = False\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    since = time.time()\n",
        "    train(model_ft_v1, DEVICE, train_loader, optimizer, epoch)\n",
        "    if epoch < 600:\n",
        "        cosine_schedule.step()\n",
        "    else:\n",
        "        if is_set_lr:\n",
        "            continue\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = 1e-6\n",
        "            is_set_lr = True\n",
        "    val_list, pred_list = val(model_ft_v1, DEVICE, test_loader)\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print(classification_report(val_list, pred_list, target_names=dataset_train.class_to_idx))\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add ELA"
      ],
      "metadata": {
        "id": "tH1btRstUfgJ"
      },
      "id": "tH1btRstUfgJ"
    },
    {
      "cell_type": "code",
      "source": [
        "#v1_5\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "\tif min_value is None:\n",
        "\t\tmin_value = divisor\n",
        "\tnew_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "\t# Make sure that round down does not go down by more than 10%.\n",
        "\tif new_v < 0.9 * v:\n",
        "\t\tnew_v += divisor\n",
        "\treturn new_v\n",
        "\n",
        "\n",
        "def Conv_BN_ReLU(inp, oup, kernel, stride=1):\n",
        "\treturn nn.Sequential(\n",
        "\t\tnn.Conv2d(inp, oup, kernel_size=kernel, stride=stride, padding=1, bias=False),\n",
        "\t\tnn.BatchNorm2d(oup),\n",
        "\t\tnn.ReLU6(inplace=True)\n",
        "\t)\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "\treturn nn.Sequential(\n",
        "\t\tnn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "\t\tnn.BatchNorm2d(oup),\n",
        "\t\tnn.ReLU6(inplace=True)\n",
        "\t)\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "\tdef __init__(self, dim, fn):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.norm = nn.LayerNorm(dim)\n",
        "\t\tself.fn = fn\n",
        "\n",
        "\tdef forward(self, x, **kwargs):\n",
        "\t\treturn self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\tdef __init__(self, dim, hidden_dim, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.ffn = nn.Sequential(\n",
        "\t\t\tnn.Linear(dim, hidden_dim),\n",
        "\t\t\tnn.SiLU(),\n",
        "\t\t\tnn.Dropout(dropout),\n",
        "\t\t\tnn.Linear(hidden_dim, dim),\n",
        "\t\t\tnn.Dropout(dropout)\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\treturn self.ffn(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\tdef __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tinner_dim = dim_head * heads\n",
        "\t\tproject_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "\t\tself.heads = heads\n",
        "\t\tself.scale = dim_head ** -0.5\n",
        "\n",
        "\t\tself.attend = nn.Softmax(dim=-1)\n",
        "\t\tself.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "\n",
        "\t\tself.to_out = nn.Sequential(\n",
        "\t\t\tnn.Linear(inner_dim, dim),\n",
        "\t\t\tnn.Dropout(dropout)\n",
        "\t\t) if project_out else nn.Identity()\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tqkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "\t\tq, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h=self.heads), qkv)\n",
        "\n",
        "\t\tdots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\t\tattn = self.attend(dots)\n",
        "\t\tout = torch.matmul(attn, v)\n",
        "\t\tout = rearrange(out, 'b p h n d -> b p n (h d)')\n",
        "\t\treturn self.to_out(out)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\tdef __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.layers = nn.ModuleList([])\n",
        "\t\tfor _ in range(depth):\n",
        "\t\t\tself.layers.append(nn.ModuleList([\n",
        "\t\t\t\tPreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "\t\t\t\tPreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
        "\t\t\t]))\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tfor attn, ff in self.layers:\n",
        "\t\t\tx = attn(x) + x\n",
        "\t\t\tx = ff(x) + x\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "class MV2Block(nn.Module):\n",
        "\tdef __init__(self, inp, oup, stride=1, expand_ratio=4):\n",
        "\t\tsuper(MV2Block, self).__init__()\n",
        "\t\tassert stride in [1, 2]\n",
        "\n",
        "\t\thidden_dim = round(inp * expand_ratio)\n",
        "\t\tself.identity = stride == 1 and inp == oup\n",
        "\n",
        "\t\tif expand_ratio == 1:\n",
        "\t\t\tself.conv = nn.Sequential(\n",
        "\t\t\t\t# dw\n",
        "\t\t\t\tnn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "\t\t\t\tnn.BatchNorm2d(hidden_dim),\n",
        "\t\t\t\t# nn.ReLU6(inplace=True),\n",
        "\t\t\t\tnn.SiLU(),\n",
        "\t\t\t\t# pw-linear\n",
        "\t\t\t\tnn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "\t\t\t\tnn.BatchNorm2d(oup),\n",
        "\t\t\t)\n",
        "\t\telse:\n",
        "\t\t\tself.conv = nn.Sequential(\n",
        "\t\t\t\t# pw\n",
        "\t\t\t\tnn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "\t\t\t\tnn.BatchNorm2d(hidden_dim),\n",
        "\t\t\t\t# nn.ReLU6(inplace=True),\n",
        "\t\t\t\tnn.SiLU(),\n",
        "\t\t\t\t# dw\n",
        "\t\t\t\tnn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "\t\t\t\tnn.BatchNorm2d(hidden_dim),\n",
        "\t\t\t\t# nn.ReLU6(inplace=True),\n",
        "\t\t\t\tnn.SiLU(),\n",
        "\t\t\t\t# pw-linear\n",
        "\t\t\t\tnn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "\t\t\t\tnn.BatchNorm2d(oup),\n",
        "\t\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tif self.identity:\n",
        "\t\t\treturn x + self.conv(x)\n",
        "\t\telse:\n",
        "\t\t\treturn self.conv(x)\n",
        "\n",
        "# 实现ELA注意力机制\n",
        "class ELAAttention(nn.Module):\n",
        "    def __init__(self, channel, kernel_size):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.pad = kernel_size // 2\n",
        "        self.conv = nn.Conv1d(channel, channel, kernel_size, padding=self.pad, groups=channel, bias=False)\n",
        "        self.gn = nn.GroupNorm(16, channel)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.size()\n",
        "        x_h = torch.mean(x, dim=3, keepdim=True).view(b, c, h)\n",
        "        x_w = torch.mean(x, dim=2, keepdim=True).view(b, c, w)\n",
        "\n",
        "        x_h = self.sigmoid(self.gn(self.conv(x_h))).view(b, c, h, 1)\n",
        "        x_w = self.sigmoid(self.gn(self.conv(x_w))).view(b, c, 1, w)\n",
        "        return x * x_h * x_w\n",
        "\n",
        "\n",
        "\n",
        "class MobileViTBlock(nn.Module):\n",
        "    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.ph, self.pw = patch_size\n",
        "\n",
        "        self.conv1 = Conv_BN_ReLU(channel, channel, kernel_size)\n",
        "        self.conv2 = conv_1x1_bn(channel, dim)\n",
        "\n",
        "        # Transformer module includes attention mechanism internally\n",
        "        self.transformer = Transformer(dim, depth, 1, 32, mlp_dim, dropout)\n",
        "\n",
        "        # ELA Attention module\n",
        "        self.ela_attention = ELAAttention(channel, kernel_size)\n",
        "\n",
        "        # We will apply Transformer and ELAAttention in parallel to the feature map\n",
        "        # Then, we fuse their outputs along with the original feature map from conv2\n",
        "\n",
        "        # Since we're fusing three feature maps: original, Transformer output, and ELA output,\n",
        "        # we'll have 3 * channel output channels before reducing it back to the original channel size.\n",
        "        self.conv3 = conv_1x1_bn(dim * 2 + channel, channel)  # Adjust for the tripled channel size\n",
        "        self.conv4 = Conv_BN_ReLU(channel, channel, kernel_size)  # Process fused feature map\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x.clone()\n",
        "\n",
        "        # Local representations\n",
        "        x_local = self.conv1(x)\n",
        "        x_local = self.conv2(x_local)\n",
        "\n",
        "        # 计算原始特征图的空间维度\n",
        "        _, _, h, w = x_local.shape\n",
        "\n",
        "        # Transformer representations\n",
        "        # 调整 x_local 的形状以适应 Transformer 的输入要求\n",
        "        x_transformed = rearrange(x_local, 'b d (h ph) (w pw) -> b (h w) (ph pw) d', ph=self.ph, pw=self.pw)\n",
        "        x_transformed = self.transformer(x_transformed)\n",
        "        # 将 Transformer 的输出重新排列回原始的空间维度\n",
        "        x_transformed = rearrange(x_transformed, 'b (h w) (ph pw) d -> b d (h ph) (w pw)', ph=self.ph, pw=self.pw, h=h // self.ph, w=w // self.pw)\n",
        "\n",
        "        # ELA Attention representations\n",
        "        x_ela = self.ela_attention(y)\n",
        "\n",
        "        # Fusion of features from both attentions and the original\n",
        "        # 确保x_ela, x_transformed, 和 x_local 在空间维度上一致\n",
        "        x_fused = torch.cat((x_local, x_ela, x_transformed), 1)\n",
        "\n",
        "        # Reduce channel size and process\n",
        "        x_reduced = self.conv3(x_fused)\n",
        "        x_final = self.conv4(x_reduced)\n",
        "\n",
        "        return x_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MobileVit_v1_5(nn.Module):\n",
        "\tdef __init__(self, image_size, dims, channels, num_classes, expansion=4, kernel_size=3, patch_size=(2, 2)):\n",
        "\t\tsuper().__init__()\n",
        "\t\tih, iw = image_size\n",
        "\t\tph, pw = patch_size\n",
        "\t\tassert ih % ph == 0 and iw % pw == 0\n",
        "\n",
        "\t\tL = [2, 4, 3]\n",
        "\n",
        "\t\tself.conv1 = Conv_BN_ReLU(3, channels[0], kernel=3, stride=2)\n",
        "\n",
        "\t\tself.mv2 = nn.ModuleList([])\n",
        "\t\tself.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))  # Repeat\n",
        "\t\tself.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
        "\n",
        "\t\tself.mvit = nn.ModuleList([])\n",
        "\t\tself.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0] * 2)))\n",
        "\t\tself.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1] * 4)))\n",
        "\t\tself.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2] * 4)))\n",
        "\n",
        "\t\tself.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
        "\n",
        "\t\tself.pool = nn.AvgPool2d(ih // 32, 1)\n",
        "\t\tself.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = self.mv2[0](x)\n",
        "\n",
        "\t\tx = self.mv2[1](x)\n",
        "\t\tx = self.mv2[2](x)\n",
        "\t\tx = self.mv2[3](x)  # Repeat\n",
        "\n",
        "\t\tx = self.mv2[4](x)\n",
        "\t\tx = self.mvit[0](x)\n",
        "\n",
        "\t\tx = self.mv2[5](x)\n",
        "\t\tx = self.mvit[1](x)\n",
        "\n",
        "\t\tx = self.mv2[6](x)\n",
        "\t\tx = self.mvit[2](x)\n",
        "\t\tx = self.conv2(x)\n",
        "\n",
        "\t\tx = self.pool(x).view(-1, x.shape[1])\n",
        "\t\tx = self.fc(x)\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "def mobilevit_xxs_v1_5():\n",
        "\tdims = [64, 80, 96]\n",
        "\tchannels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320]\n",
        "\treturn MobileVit_v1_5((256, 256), dims, channels, num_classes=1000, expansion=2)\n",
        "\n",
        "\n",
        "def mobilevit_xs_v1_5():\n",
        "\tdims = [96, 120, 144]\n",
        "\tchannels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384]\n",
        "\treturn MobileVit_v1_5((256, 256), dims, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def mobilevit_s_v1_5():\n",
        "\tdims = [144, 192, 240]\n",
        "\tchannels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640]\n",
        "\treturn MobileVit_v1_5((256, 256), dims, channels, num_classes=54)"
      ],
      "metadata": {
        "id": "g5MdmYYb_t0_"
      },
      "id": "g5MdmYYb_t0_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实例化模型并且移动到GPU\n",
        "criterion_train = SoftTargetCrossEntropy()# 训练用的loss\n",
        "criterion_val = torch.nn.CrossEntropyLoss()# 验证用的loss\n",
        "model_ft_v1_5 = mobilevit_s_v1_5()# 定义模型，并设置预训练# 修改类别\n",
        "model_ft_v1_5.to(DEVICE)\n",
        "# 选择简单暴力的Adam优化器，学习率调低\n",
        "optimizer = optim.Adam(model_ft_v1_5.parameters(), lr=model_lr)\n",
        "cosine_schedule = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=20, eta_min=1e-6)# 使用余弦退火算法调整学习率\n",
        "if use_amp: #如果使用混合精度训练，则初始化amp。\n",
        "    model, optimizer = amp.initialize(model_ft_v1_5, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”\n",
        "if torch.cuda.device_count() > 1: #检测是否存在多张显卡，如果存在则使用DP的方式并行训练\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model_ft = torch.nn.DataParallel(model_ft_v1_5)"
      ],
      "metadata": {
        "id": "d0wv0aIHARv2"
      },
      "id": "d0wv0aIHARv2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    sum_loss = 0\n",
        "    total_num = len(train_loader.dataset)\n",
        "    print(total_num, len(train_loader))\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if len(data) % 2 != 0:\n",
        "            if len(data) < 2:\n",
        "                continue\n",
        "            data = data[0:len(data) - 1]\n",
        "            target = target[0:len(target) - 1]\n",
        "            print(len(data))\n",
        "        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "        samples, targets = mixup_fn(data, target)\n",
        "        output = model(data)\n",
        "        loss = criterion_train(output, targets)\n",
        "        optimizer.zero_grad()\n",
        "        if use_amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), CLIP_GRAD)\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n",
        "        optimizer.step()\n",
        "        lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
        "        print_loss = loss.data.item()\n",
        "        sum_loss += print_loss\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR:{:.9f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item(), lr))\n",
        "    ave_loss = sum_loss / len(train_loader)\n",
        "    print('epoch:{},loss:{}'.format(epoch, ave_loss))"
      ],
      "metadata": {
        "id": "Zvkv3YONAR9Q"
      },
      "id": "Zvkv3YONAR9Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ACC = 0\n",
        "# 验证过程\n",
        "def val(model, device, test_loader):\n",
        "    global ACC\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total_num = len(test_loader.dataset)\n",
        "    print(total_num, len(test_loader))\n",
        "    val_list = []\n",
        "    pred_list = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            for t in target:\n",
        "                val_list.append(t.data.item())\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion_val(output, target)\n",
        "            _, pred = torch.max(output.data, 1)\n",
        "            for p in pred:\n",
        "                pred_list.append(p.data.item())\n",
        "            correct += torch.sum(pred == target)\n",
        "            print_loss = loss.data.item()\n",
        "            test_loss += print_loss\n",
        "        correct = correct.data.item()\n",
        "        acc = correct / total_num\n",
        "        avgloss = test_loss / len(test_loader)\n",
        "        print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            avgloss, correct, len(test_loader.dataset), 100 * acc))\n",
        "        if acc > ACC:\n",
        "            if isinstance(model, torch.nn.DataParallel):\n",
        "                torch.save(model.module, 'modelv15_TSRD_m' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n",
        "            else:\n",
        "                torch.save(model, 'modelv15_TSRD_m' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n",
        "            ACC = acc\n",
        "    return val_list, pred_list"
      ],
      "metadata": {
        "id": "JHni7SpUASFz"
      },
      "id": "JHni7SpUASFz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练\n",
        "torch.cuda.empty_cache()\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "is_set_lr = False\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    since = time.time()\n",
        "    train(model_ft_v1_5, DEVICE, train_loader, optimizer, epoch)\n",
        "    if epoch < 600:\n",
        "        cosine_schedule.step()\n",
        "    else:\n",
        "        if is_set_lr:\n",
        "            continue\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = 1e-6\n",
        "            is_set_lr = True\n",
        "    val_list, pred_list = val(model_ft_v1_5, DEVICE, test_loader)\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print(classification_report(val_list, pred_list, target_names=dataset_train.class_to_idx))\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "kPOLHCmsASUa"
      },
      "id": "kPOLHCmsASUa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add ELA And CBAM"
      ],
      "metadata": {
        "id": "KVIf6xvdUoHD"
      },
      "id": "KVIf6xvdUoHD"
    },
    {
      "cell_type": "code",
      "source": [
        "#E-MobileViT\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "\tif min_value is None:\n",
        "\t\tmin_value = divisor\n",
        "\tnew_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "\t# Make sure that round down does not go down by more than 10%.\n",
        "\tif new_v < 0.9 * v:\n",
        "\t\tnew_v += divisor\n",
        "\treturn new_v\n",
        "\n",
        "\n",
        "def Conv_BN_ReLU(inp, oup, kernel, stride=1):\n",
        "\treturn nn.Sequential(\n",
        "\t\tnn.Conv2d(inp, oup, kernel_size=kernel, stride=stride, padding=1, bias=False),\n",
        "\t\tnn.BatchNorm2d(oup),\n",
        "\t\tnn.ReLU6(inplace=True)\n",
        "\t)\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "\treturn nn.Sequential(\n",
        "\t\tnn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "\t\tnn.BatchNorm2d(oup),\n",
        "\t\tnn.ReLU6(inplace=True)\n",
        "\t)\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "\tdef __init__(self, dim, fn):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.norm = nn.LayerNorm(dim)\n",
        "\t\tself.fn = fn\n",
        "\n",
        "\tdef forward(self, x, **kwargs):\n",
        "\t\treturn self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\tdef __init__(self, dim, hidden_dim, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.ffn = nn.Sequential(\n",
        "\t\t\tnn.Linear(dim, hidden_dim),\n",
        "\t\t\tnn.SiLU(),\n",
        "\t\t\tnn.Dropout(dropout),\n",
        "\t\t\tnn.Linear(hidden_dim, dim),\n",
        "\t\t\tnn.Dropout(dropout)\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\treturn self.ffn(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\tdef __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tinner_dim = dim_head * heads\n",
        "\t\tproject_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "\t\tself.heads = heads\n",
        "\t\tself.scale = dim_head ** -0.5\n",
        "\n",
        "\t\tself.attend = nn.Softmax(dim=-1)\n",
        "\t\tself.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "\n",
        "\t\tself.to_out = nn.Sequential(\n",
        "\t\t\tnn.Linear(inner_dim, dim),\n",
        "\t\t\tnn.Dropout(dropout)\n",
        "\t\t) if project_out else nn.Identity()\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tqkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "\t\tq, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h=self.heads), qkv)\n",
        "\n",
        "\t\tdots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\t\tattn = self.attend(dots)\n",
        "\t\tout = torch.matmul(attn, v)\n",
        "\t\tout = rearrange(out, 'b p h n d -> b p n (h d)')\n",
        "\t\treturn self.to_out(out)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\tdef __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.layers = nn.ModuleList([])\n",
        "\t\tfor _ in range(depth):\n",
        "\t\t\tself.layers.append(nn.ModuleList([\n",
        "\t\t\t\tPreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "\t\t\t\tPreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
        "\t\t\t]))\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tfor attn, ff in self.layers:\n",
        "\t\t\tx = attn(x) + x\n",
        "\t\t\tx = ff(x) + x\n",
        "\t\treturn x\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
        "        padding = 3 if kernel_size == 7 else 1\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.ca = ChannelAttention(in_planes, ratio)\n",
        "        self.sa = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x * self.ca(x)\n",
        "        return out * self.sa(out)\n",
        "\n",
        "class MV2Block(nn.Module):\n",
        "    def __init__(self, inp, oup, stride=1, expand_ratio=4):\n",
        "        super(MV2Block, self).__init__()\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = round(inp * expand_ratio)\n",
        "        self.identity = stride == 1 and inp == oup\n",
        "\n",
        "        if expand_ratio == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                # nn.ReLU6(inplace=True),\n",
        "                nn.SiLU(),\n",
        "                # CBAM 注意力机制放置在 3x3 卷积层之后\n",
        "                CBAM(hidden_dim),  # 添加 CBAM 注意力机制\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                # nn.ReLU6(inplace=True),\n",
        "                nn.SiLU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                # nn.ReLU6(inplace=True),\n",
        "                nn.SiLU(),\n",
        "                # CBAM 注意力机制放置在 3x3 卷积层之后\n",
        "                CBAM(hidden_dim),  # 添加 CBAM 注意力机制\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.identity:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "# 实现ELA注意力机制\n",
        "class ELAAttention(nn.Module):\n",
        "    def __init__(self, channel, kernel_size):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.pad = kernel_size // 2\n",
        "        self.conv = nn.Conv1d(channel, channel, kernel_size, padding=self.pad, groups=channel, bias=False)\n",
        "        self.gn = nn.GroupNorm(16, channel)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.size()\n",
        "        x_h = torch.mean(x, dim=3, keepdim=True).view(b, c, h)\n",
        "        x_w = torch.mean(x, dim=2, keepdim=True).view(b, c, w)\n",
        "\n",
        "        x_h = self.sigmoid(self.gn(self.conv(x_h))).view(b, c, h, 1)\n",
        "        x_w = self.sigmoid(self.gn(self.conv(x_w))).view(b, c, 1, w)\n",
        "        return x * x_h * x_w\n",
        "\n",
        "\n",
        "\n",
        "class MobileViTBlock(nn.Module):\n",
        "    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.ph, self.pw = patch_size\n",
        "\n",
        "        self.conv1 = Conv_BN_ReLU(channel, channel, kernel_size)\n",
        "        self.conv2 = conv_1x1_bn(channel, dim)\n",
        "\n",
        "        # Transformer module includes attention mechanism internally\n",
        "        self.transformer = Transformer(dim, depth, 1, 32, mlp_dim, dropout)\n",
        "\n",
        "        # ELA Attention module\n",
        "        self.ela_attention = ELAAttention(channel, kernel_size)\n",
        "\n",
        "        # 1x1 convolution to adjust the channel size of y to dim\n",
        "        self.conv_y = conv_1x1_bn(channel, dim)\n",
        "\n",
        "        # We will apply Transformer and ELAAttention in parallel to the feature map\n",
        "        # Then, we fuse their outputs along with the original feature map from conv2\n",
        "\n",
        "        # Since we're fusing three feature maps: original, Transformer output, and ELA output,\n",
        "        # we'll have 3 * channel output channels before reducing it back to the original channel size.\n",
        "        self.conv3 = conv_1x1_bn(dim * 2 + channel, channel)  # Adjust for the tripled channel size\n",
        "        self.conv4 = Conv_BN_ReLU(channel, channel, kernel_size)  # Process fused feature map\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x.clone()\n",
        "\n",
        "        # Local representations\n",
        "        x_local = self.conv1(x)\n",
        "        x_local = self.conv2(x_local)\n",
        "\n",
        "        # 计算原始特征图的空间维度\n",
        "        _, _, h, w = x_local.shape\n",
        "\n",
        "        # Transformer representations\n",
        "        # 调整 x_local 的形状以适应 Transformer 的输入要求\n",
        "        x_transformed = rearrange(x_local, 'b d (h ph) (w pw) -> b (h w) (ph pw) d', ph=self.ph, pw=self.pw)\n",
        "        x_transformed = self.transformer(x_transformed)\n",
        "        # 将 Transformer 的输出重新排列回原始的空间维度\n",
        "        x_transformed = rearrange(x_transformed, 'b (h w) (ph pw) d -> b d (h ph) (w pw)', ph=self.ph, pw=self.pw, h=h // self.ph, w=w // self.pw)\n",
        "\n",
        "        # ELA Attention representations\n",
        "        x_ela = self.ela_attention(y)\n",
        "\n",
        "        y = self.conv_y(y)\n",
        "\n",
        "        # Fusion of features from both attentions and the original\n",
        "        # 确保x_ela, x_transformed, 和 x_local 在空间维度上一致\n",
        "        x_fused = torch.cat((y, x_ela, x_transformed), 1)\n",
        "\n",
        "        # Reduce channel size and process\n",
        "        x_reduced = self.conv3(x_fused)\n",
        "        x_final = self.conv4(x_reduced)\n",
        "\n",
        "        return x_final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MobileVit_v2(nn.Module):\n",
        "\tdef __init__(self, image_size, dims, channels, num_classes, expansion=4, kernel_size=3, patch_size=(2, 2)):\n",
        "\t\tsuper().__init__()\n",
        "\t\tih, iw = image_size\n",
        "\t\tph, pw = patch_size\n",
        "\t\tassert ih % ph == 0 and iw % pw == 0\n",
        "\n",
        "\t\tL = [2, 4, 3]\n",
        "\n",
        "\t\tself.conv1 = Conv_BN_ReLU(3, channels[0], kernel=3, stride=2)\n",
        "\n",
        "\t\tself.mv2 = nn.ModuleList([])\n",
        "\t\tself.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))  # Repeat\n",
        "\t\tself.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
        "\n",
        "\t\tself.mvit = nn.ModuleList([])\n",
        "\t\tself.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0] * 2)))\n",
        "\t\tself.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1] * 4)))\n",
        "\t\tself.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2] * 4)))\n",
        "\n",
        "\t\tself.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
        "\n",
        "\t\tself.pool = nn.AvgPool2d(ih // 32, 1)\n",
        "\t\tself.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = self.mv2[0](x)\n",
        "\n",
        "\t\tx = self.mv2[1](x)\n",
        "\t\tx = self.mv2[2](x)\n",
        "\t\tx = self.mv2[3](x)  # Repeat\n",
        "\n",
        "\t\tx = self.mv2[4](x)\n",
        "\t\tx = self.mvit[0](x)\n",
        "\n",
        "\t\tx = self.mv2[5](x)\n",
        "\t\tx = self.mvit[1](x)\n",
        "\n",
        "\t\tx = self.mv2[6](x)\n",
        "\t\tx = self.mvit[2](x)\n",
        "\t\tx = self.conv2(x)\n",
        "\n",
        "\t\tx = self.pool(x).view(-1, x.shape[1])\n",
        "\t\tx = self.fc(x)\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "def mobilevit_xxs_v2():\n",
        "\tdims = [64, 80, 96]\n",
        "\tchannels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320]\n",
        "\treturn MobileVit_v2((256, 256), dims, channels, num_classes=1000, expansion=2)\n",
        "\n",
        "\n",
        "def mobilevit_xs_v2():\n",
        "\tdims = [96, 120, 144]\n",
        "\tchannels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384]\n",
        "\treturn MobileVit_v2((256, 256), dims, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def mobilevit_s_v2():\n",
        "\tdims = [144, 192, 240]\n",
        "\tchannels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640]\n",
        "\treturn MobileVit_v2((256, 256), dims, channels, num_classes=43)"
      ],
      "metadata": {
        "id": "c5ze0HpbKwCZ"
      },
      "id": "c5ze0HpbKwCZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "model = mobilevit_s_v2()\n",
        "dummy_input = torch.randn(1, 3, 256, 256)\n",
        "output = model(dummy_input)\n",
        "print(f'Final output shape: {output.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXIq2L8Puir8",
        "outputId": "b88f9ab6-0f60-45f7-ae0d-ee6af85f06cb"
      },
      "id": "YXIq2L8Puir8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output shape: torch.Size([1, 43])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 实例化模型并且移动到GPU\n",
        "criterion_train = SoftTargetCrossEntropy()# 训练用的loss\n",
        "criterion_val = torch.nn.CrossEntropyLoss()# 验证用的loss\n",
        "\n",
        "model.to(DEVICE)\n",
        "# 选择简单暴力的Adam优化器，学习率调低\n",
        "optimizer = optim.Adam(model.parameters(), lr=model_lr)\n",
        "cosine_schedule = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=20, eta_min=1e-6)# 使用余弦退火算法调整学习率\n",
        "#if use_amp: #如果使用混合精度训练，则初始化amp。\n",
        "    #model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”\n",
        "if torch.cuda.device_count() > 1: #检测是否存在多张显卡，如果存在则使用DP的方式并行训练\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = torch.nn.DataParallel(model)"
      ],
      "metadata": {
        "id": "7aG0BUmNaSF4"
      },
      "id": "7aG0BUmNaSF4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    sum_loss = 0\n",
        "    total_num = len(train_loader.dataset)\n",
        "    print(total_num, len(train_loader))\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if len(data) % 2 != 0:\n",
        "            if len(data) < 2:\n",
        "                continue\n",
        "            data = data[0:len(data) - 1]\n",
        "            target = target[0:len(target) - 1]\n",
        "            print(len(data))\n",
        "        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "        samples, targets = mixup_fn(data, target)\n",
        "        output = model(data)\n",
        "        loss = criterion_train(output, targets)\n",
        "        optimizer.zero_grad()\n",
        "        if use_amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), CLIP_GRAD)\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n",
        "        optimizer.step()\n",
        "        lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
        "        print_loss = loss.data.item()\n",
        "        sum_loss += print_loss\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR:{:.9f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item(), lr))\n",
        "    ave_loss = sum_loss / len(train_loader)\n",
        "    print('epoch:{},loss:{}'.format(epoch, ave_loss))"
      ],
      "metadata": {
        "id": "iJPFyYI8aUX2"
      },
      "id": "iJPFyYI8aUX2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ACC = 0\n",
        "# 验证过程\n",
        "def val(model, device, test_loader):\n",
        "    global ACC\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total_num = len(test_loader.dataset)\n",
        "    print(total_num, len(test_loader))\n",
        "    val_list = []\n",
        "    pred_list = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            for t in target:\n",
        "                val_list.append(t.data.item())\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion_val(output, target)\n",
        "            _, pred = torch.max(output.data, 1)\n",
        "            for p in pred:\n",
        "                pred_list.append(p.data.item())\n",
        "            correct += torch.sum(pred == target)\n",
        "            print_loss = loss.data.item()\n",
        "            test_loss += print_loss\n",
        "        correct = correct.data.item()\n",
        "        acc = correct / total_num\n",
        "        avgloss = test_loss / len(test_loader)\n",
        "        print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            avgloss, correct, len(test_loader.dataset), 100 * acc))\n",
        "        if acc > ACC:\n",
        "            if isinstance(model, torch.nn.DataParallel):\n",
        "                torch.save(model.module, 'mv2_2_TSRD_m' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n",
        "            else:\n",
        "                torch.save(model, 'mv2_2_TSRD_m' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n",
        "            ACC = acc\n",
        "    return val_list, pred_list"
      ],
      "metadata": {
        "id": "b1R93-vaaVcm"
      },
      "id": "b1R93-vaaVcm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练\n",
        "torch.cuda.empty_cache()\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "is_set_lr = False\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    since = time.time()\n",
        "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
        "    if epoch < 600:\n",
        "        cosine_schedule.step()\n",
        "    else:\n",
        "        if is_set_lr:\n",
        "            continue\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = 1e-6\n",
        "            is_set_lr = True\n",
        "    val_list, pred_list = val(model, DEVICE, test_loader)\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print(classification_report(val_list, pred_list, target_names=dataset_train.class_to_idx))\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "3bEgEiKtaXyH"
      },
      "id": "3bEgEiKtaXyH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实例化模型并且移动到GPU\n",
        "criterion_train = SoftTargetCrossEntropy()# 训练用的loss\n",
        "criterion_val = torch.nn.CrossEntropyLoss()# 验证用的loss\n",
        "model_ft_v2 = mobilevit_s_v2()# 定义模型，并设置预训练# 修改类别\n",
        "model_ft_v2.to(DEVICE)\n",
        "# 选择简单暴力的Adam优化器，学习率调低\n",
        "optimizer = optim.Adam(model_ft_v2.parameters(), lr=model_lr)\n",
        "cosine_schedule = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=20, eta_min=1e-6)# 使用余弦退火算法调整学习率\n",
        "if use_amp: #如果使用混合精度训练，则初始化amp。\n",
        "    model, optimizer = amp.initialize(model_ft_v2, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”\n",
        "if torch.cuda.device_count() > 1: #检测是否存在多张显卡，如果存在则使用DP的方式并行训练\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model_ft = torch.nn.DataParallel(model_ft_v2)"
      ],
      "metadata": {
        "id": "K7e7jG_vOwii"
      },
      "id": "K7e7jG_vOwii",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    sum_loss = 0\n",
        "    total_num = len(train_loader.dataset)\n",
        "    print(total_num, len(train_loader))\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if len(data) % 2 != 0:\n",
        "            if len(data) < 2:\n",
        "                continue\n",
        "            data = data[0:len(data) - 1]\n",
        "            target = target[0:len(target) - 1]\n",
        "            print(len(data))\n",
        "        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "        samples, targets = mixup_fn(data, target)\n",
        "        output = model(data)\n",
        "        loss = criterion_train(output, targets)\n",
        "        optimizer.zero_grad()\n",
        "        if use_amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), CLIP_GRAD)\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n",
        "        optimizer.step()\n",
        "        lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
        "        print_loss = loss.data.item()\n",
        "        sum_loss += print_loss\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR:{:.9f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item(), lr))\n",
        "    ave_loss = sum_loss / len(train_loader)\n",
        "    print('epoch:{},loss:{}'.format(epoch, ave_loss))"
      ],
      "metadata": {
        "id": "K5GwEXRkO0KZ"
      },
      "id": "K5GwEXRkO0KZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ACC = 0"
      ],
      "metadata": {
        "id": "t4IvMeRyO7-0"
      },
      "id": "t4IvMeRyO7-0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ACC = 0\n",
        "# 验证过程\n",
        "def val(model, device, test_loader):\n",
        "    global ACC\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total_num = len(test_loader.dataset)\n",
        "    print(total_num, len(test_loader))\n",
        "    val_list = []\n",
        "    pred_list = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            for t in target:\n",
        "                val_list.append(t.data.item())\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion_val(output, target)\n",
        "            _, pred = torch.max(output.data, 1)\n",
        "            for p in pred:\n",
        "                pred_list.append(p.data.item())\n",
        "            correct += torch.sum(pred == target)\n",
        "            print_loss = loss.data.item()\n",
        "            test_loss += print_loss\n",
        "        correct = correct.data.item()\n",
        "        acc = correct / total_num\n",
        "        avgloss = test_loss / len(test_loader)\n",
        "        print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            avgloss, correct, len(test_loader.dataset), 100 * acc))\n",
        "        if acc > ACC:\n",
        "            if isinstance(model, torch.nn.DataParallel):\n",
        "                torch.save(model.module, 'modelv2_GT_m' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n",
        "            else:\n",
        "                torch.save(model, 'modelv2_GT_m' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n",
        "            ACC = acc\n",
        "    return val_list, pred_list"
      ],
      "metadata": {
        "id": "SQEDGukyO_jt"
      },
      "id": "SQEDGukyO_jt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练\n",
        "torch.cuda.empty_cache()\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "is_set_lr = False\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    since = time.time()\n",
        "    train(model_ft_v2, DEVICE, train_loader, optimizer, epoch)\n",
        "    if epoch < 600:\n",
        "        cosine_schedule.step()\n",
        "    else:\n",
        "        if is_set_lr:\n",
        "            continue\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = 1e-6\n",
        "            is_set_lr = True\n",
        "    val_list, pred_list = val(model_ft_v2, DEVICE, test_loader)\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print(classification_report(val_list, pred_list, target_names=dataset_train.class_to_idx))\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "cp5uFgLqPP4k"
      },
      "id": "cp5uFgLqPP4k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MobileViT"
      ],
      "metadata": {
        "id": "Hj5-7x3AV3Yr"
      },
      "id": "Hj5-7x3AV3Yr"
    },
    {
      "cell_type": "code",
      "source": [
        "#v0\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "\tif min_value is None:\n",
        "\t\tmin_value = divisor\n",
        "\tnew_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "\t# Make sure that round down does not go down by more than 10%.\n",
        "\tif new_v < 0.9 * v:\n",
        "\t\tnew_v += divisor\n",
        "\treturn new_v\n",
        "\n",
        "\n",
        "def Conv_BN_ReLU(inp, oup, kernel, stride=1):\n",
        "\treturn nn.Sequential(\n",
        "\t\tnn.Conv2d(inp, oup, kernel_size=kernel, stride=stride, padding=1, bias=False),\n",
        "\t\tnn.BatchNorm2d(oup),\n",
        "\t\tnn.ReLU6(inplace=True)\n",
        "\t)\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "\treturn nn.Sequential(\n",
        "\t\tnn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "\t\tnn.BatchNorm2d(oup),\n",
        "\t\tnn.ReLU6(inplace=True)\n",
        "\t)\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "\tdef __init__(self, dim, fn):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.norm = nn.LayerNorm(dim)\n",
        "\t\tself.fn = fn\n",
        "\n",
        "\tdef forward(self, x, **kwargs):\n",
        "\t\treturn self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\tdef __init__(self, dim, hidden_dim, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.ffn = nn.Sequential(\n",
        "\t\t\tnn.Linear(dim, hidden_dim),\n",
        "\t\t\tnn.SiLU(),\n",
        "\t\t\tnn.Dropout(dropout),\n",
        "\t\t\tnn.Linear(hidden_dim, dim),\n",
        "\t\t\tnn.Dropout(dropout)\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\treturn self.ffn(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\tdef __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tinner_dim = dim_head * heads\n",
        "\t\tproject_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "\t\tself.heads = heads\n",
        "\t\tself.scale = dim_head ** -0.5\n",
        "\n",
        "\t\tself.attend = nn.Softmax(dim=-1)\n",
        "\t\tself.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "\n",
        "\t\tself.to_out = nn.Sequential(\n",
        "\t\t\tnn.Linear(inner_dim, dim),\n",
        "\t\t\tnn.Dropout(dropout)\n",
        "\t\t) if project_out else nn.Identity()\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tqkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "\t\tq, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h=self.heads), qkv)\n",
        "\n",
        "\t\tdots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\t\tattn = self.attend(dots)\n",
        "\t\tout = torch.matmul(attn, v)\n",
        "\t\tout = rearrange(out, 'b p h n d -> b p n (h d)')\n",
        "\t\treturn self.to_out(out)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\tdef __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.layers = nn.ModuleList([])\n",
        "\t\tfor _ in range(depth):\n",
        "\t\t\tself.layers.append(nn.ModuleList([\n",
        "\t\t\t\tPreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "\t\t\t\tPreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
        "\t\t\t]))\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tfor attn, ff in self.layers:\n",
        "\t\t\tx = attn(x) + x\n",
        "\t\t\tx = ff(x) + x\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "class MV2Block(nn.Module):\n",
        "\tdef __init__(self, inp, oup, stride=1, expand_ratio=4):\n",
        "\t\tsuper(MV2Block, self).__init__()\n",
        "\t\tassert stride in [1, 2]\n",
        "\n",
        "\t\thidden_dim = round(inp * expand_ratio)\n",
        "\t\tself.identity = stride == 1 and inp == oup\n",
        "\n",
        "\t\tif expand_ratio == 1:\n",
        "\t\t\tself.conv = nn.Sequential(\n",
        "\t\t\t\t# dw\n",
        "\t\t\t\tnn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "\t\t\t\tnn.BatchNorm2d(hidden_dim),\n",
        "\t\t\t\t# nn.ReLU6(inplace=True),\n",
        "\t\t\t\tnn.SiLU(),\n",
        "\t\t\t\t# pw-linear\n",
        "\t\t\t\tnn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "\t\t\t\tnn.BatchNorm2d(oup),\n",
        "\t\t\t)\n",
        "\t\telse:\n",
        "\t\t\tself.conv = nn.Sequential(\n",
        "\t\t\t\t# pw\n",
        "\t\t\t\tnn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "\t\t\t\tnn.BatchNorm2d(hidden_dim),\n",
        "\t\t\t\t# nn.ReLU6(inplace=True),\n",
        "\t\t\t\tnn.SiLU(),\n",
        "\t\t\t\t# dw\n",
        "\t\t\t\tnn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "\t\t\t\tnn.BatchNorm2d(hidden_dim),\n",
        "\t\t\t\t# nn.ReLU6(inplace=True),\n",
        "\t\t\t\tnn.SiLU(),\n",
        "\t\t\t\t# pw-linear\n",
        "\t\t\t\tnn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "\t\t\t\tnn.BatchNorm2d(oup),\n",
        "\t\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tif self.identity:\n",
        "\t\t\treturn x + self.conv(x)\n",
        "\t\telse:\n",
        "\t\t\treturn self.conv(x)\n",
        "\n",
        "\n",
        "class MobileViTBlock(nn.Module):\n",
        "\tdef __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.ph, self.pw = patch_size\n",
        "\n",
        "\t\tself.conv1 = Conv_BN_ReLU(channel, channel, kernel_size)\n",
        "\t\tself.conv2 = conv_1x1_bn(channel, dim)\n",
        "\n",
        "\t\tself.transformer = Transformer(dim, depth, 1, 32, mlp_dim, dropout)\n",
        "\n",
        "\t\tself.conv3 = conv_1x1_bn(dim, channel)\n",
        "\t\tself.conv4 = Conv_BN_ReLU(2 * channel, channel, kernel_size)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\ty = x.clone()\n",
        "\n",
        "\t\t# Local representations\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = self.conv2(x)\n",
        "\n",
        "\t\t# Global representations\n",
        "\t\t_, _, h, w = x.shape\n",
        "\t\tx = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)\n",
        "\t\tx = self.transformer(x)\n",
        "\t\tx = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h // self.ph, w=w // self.pw, ph=self.ph,\n",
        "\t\t              pw=self.pw)\n",
        "\n",
        "\t\t# Fusion\n",
        "\t\tx = self.conv3(x)\n",
        "\t\tx = torch.cat((x, y), 1)\n",
        "\t\tx = self.conv4(x)\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "class MobileVit(nn.Module):\n",
        "\tdef __init__(self, image_size, dims, channels, num_classes, expansion=4, kernel_size=3, patch_size=(2, 2)):\n",
        "\t\tsuper().__init__()\n",
        "\t\tih, iw = image_size\n",
        "\t\tph, pw = patch_size\n",
        "\t\tassert ih % ph == 0 and iw % pw == 0\n",
        "\n",
        "\t\tL = [2, 4, 3]\n",
        "\n",
        "\t\tself.conv1 = Conv_BN_ReLU(3, channels[0], kernel=3, stride=2)\n",
        "\n",
        "\t\tself.mv2 = nn.ModuleList([])\n",
        "\t\tself.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))  # Repeat\n",
        "\t\tself.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
        "\t\tself.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
        "\n",
        "\t\tself.mvit = nn.ModuleList([])\n",
        "\t\tself.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0] * 2)))\n",
        "\t\tself.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1] * 4)))\n",
        "\t\tself.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2] * 4)))\n",
        "\n",
        "\t\tself.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
        "\n",
        "\t\tself.pool = nn.AvgPool2d(ih // 32, 1)\n",
        "\t\tself.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = self.mv2[0](x)\n",
        "\n",
        "\t\tx = self.mv2[1](x)\n",
        "\t\tx = self.mv2[2](x)\n",
        "\t\tx = self.mv2[3](x)  # Repeat\n",
        "\n",
        "\t\tx = self.mv2[4](x)\n",
        "\t\tx = self.mvit[0](x)\n",
        "\n",
        "\t\tx = self.mv2[5](x)\n",
        "\t\tx = self.mvit[1](x)\n",
        "\n",
        "\t\tx = self.mv2[6](x)\n",
        "\t\tx = self.mvit[2](x)\n",
        "\t\tx = self.conv2(x)\n",
        "\n",
        "\t\tx = self.pool(x).view(-1, x.shape[1])\n",
        "\t\tx = self.fc(x)\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "def mobilevit_xxs():\n",
        "\tdims = [64, 80, 96]\n",
        "\tchannels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320]\n",
        "\treturn MobileVit((256, 256), dims, channels, num_classes=1000, expansion=2)\n",
        "\n",
        "\n",
        "def mobilevit_xs():\n",
        "\tdims = [96, 120, 144]\n",
        "\tchannels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384]\n",
        "\treturn MobileVit((256, 256), dims, channels, num_classes=1000)\n",
        "\n",
        "\n",
        "def mobilevit_s():\n",
        "\tdims = [144, 192, 240]\n",
        "\tchannels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640]\n",
        "\treturn MobileVit((256, 256), dims, channels, num_classes=15)"
      ],
      "metadata": {
        "id": "TpW6gQjlvq7y"
      },
      "id": "TpW6gQjlvq7y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实例化模型并且移动到GPU\n",
        "criterion_train = SoftTargetCrossEntropy()# 训练用的loss\n",
        "criterion_val = torch.nn.CrossEntropyLoss()# 验证用的loss\n",
        "model_ft_v0 = mobilevit_s()# 定义模型，并设置预训练# 修改类别\n",
        "model_ft_v0.to(DEVICE)\n",
        "# 选择简单暴力的Adam优化器，学习率调低\n",
        "optimizer = optim.Adam(model_ft_v0.parameters(), lr=model_lr)\n",
        "cosine_schedule = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=20, eta_min=1e-6)# 使用余弦退火算法调整学习率\n",
        "if use_amp: #如果使用混合精度训练，则初始化amp。\n",
        "    model, optimizer = amp.initialize(model_ft_v0, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”\n",
        "if torch.cuda.device_count() > 1: #检测是否存在多张显卡，如果存在则使用DP的方式并行训练\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model_ft_v0 = torch.nn.DataParallel(model_ft_v0)"
      ],
      "metadata": {
        "id": "8-EydY8Hv81C"
      },
      "id": "8-EydY8Hv81C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    sum_loss = 0\n",
        "    total_num = len(train_loader.dataset)\n",
        "    print(total_num, len(train_loader))\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if len(data) % 2 != 0:\n",
        "            if len(data) < 2:\n",
        "                continue\n",
        "            data = data[0:len(data) - 1]\n",
        "            target = target[0:len(target) - 1]\n",
        "            print(len(data))\n",
        "        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "        samples, targets = mixup_fn(data, target)\n",
        "        output = model(data)\n",
        "        loss = criterion_train(output, targets)\n",
        "        optimizer.zero_grad()\n",
        "        if use_amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), CLIP_GRAD)\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n",
        "        optimizer.step()\n",
        "        lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
        "        print_loss = loss.data.item()\n",
        "        sum_loss += print_loss\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR:{:.9f}'.format(\n",
        "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
        "                       100. * (batch_idx + 1) / len(train_loader), loss.item(), lr))\n",
        "    ave_loss = sum_loss / len(train_loader)\n",
        "    print('epoch:{},loss:{}'.format(epoch, ave_loss))"
      ],
      "metadata": {
        "id": "YLLDlofBg5BB"
      },
      "id": "YLLDlofBg5BB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ACC = 0\n",
        "# 验证过程\n",
        "def val(model, device, test_loader):\n",
        "    global ACC\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total_num = len(test_loader.dataset)\n",
        "    print(total_num, len(test_loader))\n",
        "    val_list = []\n",
        "    pred_list = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            for t in target:\n",
        "                val_list.append(t.data.item())\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion_val(output, target)\n",
        "            _, pred = torch.max(output.data, 1)\n",
        "            for p in pred:\n",
        "                pred_list.append(p.data.item())\n",
        "            correct += torch.sum(pred == target)\n",
        "            print_loss = loss.data.item()\n",
        "            test_loss += print_loss\n",
        "        correct = correct.data.item()\n",
        "        acc = correct / total_num\n",
        "        avgloss = test_loss / len(test_loader)\n",
        "        print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            avgloss, correct, len(test_loader.dataset), 100 * acc))\n",
        "        if acc > ACC:\n",
        "            if isinstance(model, torch.nn.DataParallel):\n",
        "                torch.save(model.module, 'modelv0_TSSD_m' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n",
        "            else:\n",
        "                torch.save(model, 'modelv0_TSSD_m' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n",
        "            ACC = acc\n",
        "    return val_list, pred_list"
      ],
      "metadata": {
        "id": "oGBCdYO1hETj"
      },
      "id": "oGBCdYO1hETj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "is_set_lr = False\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    since = time.time()\n",
        "    train(model_ft_v0, DEVICE, train_loader, optimizer, epoch)\n",
        "    if epoch < 600:\n",
        "        cosine_schedule.step()\n",
        "    else:\n",
        "        if is_set_lr:\n",
        "            continue\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = 1e-6\n",
        "            is_set_lr = True\n",
        "    val_list, pred_list = val(model_ft_v0, DEVICE, test_loader)\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print(classification_report(val_list, pred_list, target_names=dataset_train.class_to_idx))\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXuxkmxnhLek",
        "outputId": "dac173f0-2629-4236-eb91-59a8acfb7554"
      },
      "id": "rXuxkmxnhLek",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3527 111\n",
            "Train Epoch: 1 [320/3527 (9%)]\tLoss: 2.668868\tLR:0.000100000\n",
            "Train Epoch: 1 [640/3527 (18%)]\tLoss: 2.540170\tLR:0.000100000\n",
            "Train Epoch: 1 [960/3527 (27%)]\tLoss: 2.559960\tLR:0.000100000\n",
            "Train Epoch: 1 [1280/3527 (36%)]\tLoss: 2.549647\tLR:0.000100000\n",
            "Train Epoch: 1 [1600/3527 (45%)]\tLoss: 2.410724\tLR:0.000100000\n",
            "Train Epoch: 1 [1920/3527 (54%)]\tLoss: 2.586918\tLR:0.000100000\n",
            "Train Epoch: 1 [2240/3527 (63%)]\tLoss: 2.566855\tLR:0.000100000\n",
            "Train Epoch: 1 [2560/3527 (72%)]\tLoss: 2.235375\tLR:0.000100000\n",
            "Train Epoch: 1 [2880/3527 (81%)]\tLoss: 2.431061\tLR:0.000100000\n",
            "Train Epoch: 1 [3200/3527 (90%)]\tLoss: 2.338374\tLR:0.000100000\n",
            "Train Epoch: 1 [3520/3527 (99%)]\tLoss: 2.344551\tLR:0.000100000\n",
            "6\n",
            "epoch:1,loss:2.514401596945685\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 2.3032, Accuracy: 138/637 (22%)\n",
            "\n",
            "Training complete in 0m 40s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.61      0.34        69\n",
            "           1       0.00      0.00      0.00        56\n",
            "          10       0.40      0.14      0.21        43\n",
            "          11       0.17      0.09      0.11        47\n",
            "          12       0.16      0.35      0.22        55\n",
            "          13       0.00      0.00      0.00        30\n",
            "          14       0.30      0.70      0.42        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.00      0.00      0.00        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.29      0.34      0.32        35\n",
            "           6       0.19      0.25      0.21        44\n",
            "           7       0.11      0.07      0.09        56\n",
            "           8       0.17      0.15      0.16        47\n",
            "           9       0.00      0.00      0.00        45\n",
            "\n",
            "    accuracy                           0.22       637\n",
            "   macro avg       0.14      0.18      0.14       637\n",
            "weighted avg       0.15      0.22      0.16       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 2 [320/3527 (9%)]\tLoss: 2.606095\tLR:0.000099391\n",
            "Train Epoch: 2 [640/3527 (18%)]\tLoss: 2.293482\tLR:0.000099391\n",
            "Train Epoch: 2 [960/3527 (27%)]\tLoss: 2.401312\tLR:0.000099391\n",
            "Train Epoch: 2 [1280/3527 (36%)]\tLoss: 2.559300\tLR:0.000099391\n",
            "Train Epoch: 2 [1600/3527 (45%)]\tLoss: 2.340470\tLR:0.000099391\n",
            "Train Epoch: 2 [1920/3527 (54%)]\tLoss: 2.549324\tLR:0.000099391\n",
            "Train Epoch: 2 [2240/3527 (63%)]\tLoss: 2.283673\tLR:0.000099391\n",
            "Train Epoch: 2 [2560/3527 (72%)]\tLoss: 2.416434\tLR:0.000099391\n",
            "Train Epoch: 2 [2880/3527 (81%)]\tLoss: 2.485334\tLR:0.000099391\n",
            "Train Epoch: 2 [3200/3527 (90%)]\tLoss: 2.202767\tLR:0.000099391\n",
            "Train Epoch: 2 [3520/3527 (99%)]\tLoss: 2.408051\tLR:0.000099391\n",
            "6\n",
            "epoch:2,loss:2.371083420676154\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 2.2043, Accuracy: 151/637 (24%)\n",
            "\n",
            "Training complete in 0m 40s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.07      0.12        69\n",
            "           1       0.18      0.34      0.24        56\n",
            "          10       0.00      0.00      0.00        43\n",
            "          11       0.10      0.15      0.12        47\n",
            "          12       0.14      0.25      0.18        55\n",
            "          13       0.00      0.00      0.00        30\n",
            "          14       0.79      0.72      0.76        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.10      0.07      0.08        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.26      0.37      0.31        35\n",
            "           6       0.36      0.80      0.49        44\n",
            "           7       0.17      0.29      0.21        56\n",
            "           8       0.13      0.09      0.10        47\n",
            "           9       0.33      0.02      0.04        45\n",
            "\n",
            "    accuracy                           0.24       637\n",
            "   macro avg       0.20      0.21      0.18       637\n",
            "weighted avg       0.23      0.24      0.20       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 3 [320/3527 (9%)]\tLoss: 2.265971\tLR:0.000097577\n",
            "Train Epoch: 3 [640/3527 (18%)]\tLoss: 2.562938\tLR:0.000097577\n",
            "Train Epoch: 3 [960/3527 (27%)]\tLoss: 2.207544\tLR:0.000097577\n",
            "Train Epoch: 3 [1280/3527 (36%)]\tLoss: 2.309329\tLR:0.000097577\n",
            "Train Epoch: 3 [1600/3527 (45%)]\tLoss: 2.149579\tLR:0.000097577\n",
            "Train Epoch: 3 [1920/3527 (54%)]\tLoss: 2.289984\tLR:0.000097577\n",
            "Train Epoch: 3 [2240/3527 (63%)]\tLoss: 2.137218\tLR:0.000097577\n",
            "Train Epoch: 3 [2560/3527 (72%)]\tLoss: 2.395682\tLR:0.000097577\n",
            "Train Epoch: 3 [2880/3527 (81%)]\tLoss: 2.237026\tLR:0.000097577\n",
            "Train Epoch: 3 [3200/3527 (90%)]\tLoss: 2.576375\tLR:0.000097577\n",
            "Train Epoch: 3 [3520/3527 (99%)]\tLoss: 2.096303\tLR:0.000097577\n",
            "6\n",
            "epoch:3,loss:2.300306206350928\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 2.0371, Accuracy: 197/637 (31%)\n",
            "\n",
            "Training complete in 0m 41s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.70      0.44        69\n",
            "           1       0.00      0.00      0.00        56\n",
            "          10       0.00      0.00      0.00        43\n",
            "          11       0.18      0.30      0.23        47\n",
            "          12       0.23      0.58      0.33        55\n",
            "          13       0.00      0.00      0.00        30\n",
            "          14       0.68      0.77      0.72        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       1.00      0.07      0.14        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.39      0.34      0.36        35\n",
            "           6       0.49      0.82      0.61        44\n",
            "           7       0.14      0.21      0.17        56\n",
            "           8       0.33      0.04      0.08        47\n",
            "           9       0.18      0.04      0.07        45\n",
            "\n",
            "    accuracy                           0.31       637\n",
            "   macro avg       0.26      0.26      0.21       637\n",
            "weighted avg       0.29      0.31      0.24       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 4 [320/3527 (9%)]\tLoss: 2.117610\tLR:0.000094605\n",
            "Train Epoch: 4 [640/3527 (18%)]\tLoss: 2.265424\tLR:0.000094605\n",
            "Train Epoch: 4 [960/3527 (27%)]\tLoss: 2.210012\tLR:0.000094605\n",
            "Train Epoch: 4 [1280/3527 (36%)]\tLoss: 2.234682\tLR:0.000094605\n",
            "Train Epoch: 4 [1600/3527 (45%)]\tLoss: 2.118060\tLR:0.000094605\n",
            "Train Epoch: 4 [1920/3527 (54%)]\tLoss: 2.256882\tLR:0.000094605\n",
            "Train Epoch: 4 [2240/3527 (63%)]\tLoss: 2.090492\tLR:0.000094605\n",
            "Train Epoch: 4 [2560/3527 (72%)]\tLoss: 2.087843\tLR:0.000094605\n",
            "Train Epoch: 4 [2880/3527 (81%)]\tLoss: 2.213074\tLR:0.000094605\n",
            "Train Epoch: 4 [3200/3527 (90%)]\tLoss: 2.249873\tLR:0.000094605\n",
            "Train Epoch: 4 [3520/3527 (99%)]\tLoss: 2.114511\tLR:0.000094605\n",
            "6\n",
            "epoch:4,loss:2.242903367893116\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 2.0325, Accuracy: 201/637 (32%)\n",
            "\n",
            "Training complete in 0m 40s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.41      0.32        69\n",
            "           1       0.42      0.30      0.35        56\n",
            "          10       0.28      0.26      0.27        43\n",
            "          11       0.30      0.06      0.11        47\n",
            "          12       0.22      0.62      0.33        55\n",
            "          13       0.00      0.00      0.00        30\n",
            "          14       0.71      0.77      0.73        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.00      0.00      0.00        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.29      0.31      0.30        35\n",
            "           6       0.68      0.77      0.72        44\n",
            "           7       0.15      0.29      0.20        56\n",
            "           8       0.18      0.06      0.09        47\n",
            "           9       0.32      0.18      0.23        45\n",
            "\n",
            "    accuracy                           0.32       637\n",
            "   macro avg       0.25      0.27      0.24       637\n",
            "weighted avg       0.29      0.32      0.28       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 5 [320/3527 (9%)]\tLoss: 2.097097\tLR:0.000090546\n",
            "Train Epoch: 5 [640/3527 (18%)]\tLoss: 2.212119\tLR:0.000090546\n",
            "Train Epoch: 5 [960/3527 (27%)]\tLoss: 2.206492\tLR:0.000090546\n",
            "Train Epoch: 5 [1280/3527 (36%)]\tLoss: 2.183180\tLR:0.000090546\n",
            "Train Epoch: 5 [1600/3527 (45%)]\tLoss: 2.696434\tLR:0.000090546\n",
            "Train Epoch: 5 [1920/3527 (54%)]\tLoss: 1.975523\tLR:0.000090546\n",
            "Train Epoch: 5 [2240/3527 (63%)]\tLoss: 2.301526\tLR:0.000090546\n",
            "Train Epoch: 5 [2560/3527 (72%)]\tLoss: 2.251323\tLR:0.000090546\n",
            "Train Epoch: 5 [2880/3527 (81%)]\tLoss: 2.032936\tLR:0.000090546\n",
            "Train Epoch: 5 [3200/3527 (90%)]\tLoss: 2.318645\tLR:0.000090546\n",
            "Train Epoch: 5 [3520/3527 (99%)]\tLoss: 2.052046\tLR:0.000090546\n",
            "6\n",
            "epoch:5,loss:2.2075104316075644\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 2.0009, Accuracy: 198/637 (31%)\n",
            "\n",
            "Training complete in 0m 40s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.67      0.43        69\n",
            "           1       0.36      0.09      0.14        56\n",
            "          10       0.33      0.14      0.20        43\n",
            "          11       0.20      0.34      0.25        47\n",
            "          12       0.24      0.40      0.30        55\n",
            "          13       0.12      0.03      0.05        30\n",
            "          14       0.77      0.77      0.77        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.33      0.07      0.12        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.22      0.37      0.28        35\n",
            "           6       0.86      0.55      0.67        44\n",
            "           7       0.16      0.32      0.21        56\n",
            "           8       0.36      0.17      0.23        47\n",
            "           9       0.00      0.00      0.00        45\n",
            "\n",
            "    accuracy                           0.31       637\n",
            "   macro avg       0.29      0.26      0.24       637\n",
            "weighted avg       0.32      0.31      0.28       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 6 [320/3527 (9%)]\tLoss: 2.039098\tLR:0.000085502\n",
            "Train Epoch: 6 [640/3527 (18%)]\tLoss: 2.456345\tLR:0.000085502\n",
            "Train Epoch: 6 [960/3527 (27%)]\tLoss: 1.943278\tLR:0.000085502\n",
            "Train Epoch: 6 [1280/3527 (36%)]\tLoss: 1.955535\tLR:0.000085502\n",
            "Train Epoch: 6 [1600/3527 (45%)]\tLoss: 2.504680\tLR:0.000085502\n",
            "Train Epoch: 6 [1920/3527 (54%)]\tLoss: 2.020895\tLR:0.000085502\n",
            "Train Epoch: 6 [2240/3527 (63%)]\tLoss: 2.042674\tLR:0.000085502\n",
            "Train Epoch: 6 [2560/3527 (72%)]\tLoss: 2.247123\tLR:0.000085502\n",
            "Train Epoch: 6 [2880/3527 (81%)]\tLoss: 2.072582\tLR:0.000085502\n",
            "Train Epoch: 6 [3200/3527 (90%)]\tLoss: 2.290818\tLR:0.000085502\n",
            "Train Epoch: 6 [3520/3527 (99%)]\tLoss: 2.167681\tLR:0.000085502\n",
            "6\n",
            "epoch:6,loss:2.1278253836674734\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.9099, Accuracy: 224/637 (35%)\n",
            "\n",
            "Training complete in 0m 40s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.46      0.44        69\n",
            "           1       0.24      0.34      0.28        56\n",
            "          10       0.31      0.26      0.28        43\n",
            "          11       0.26      0.21      0.23        47\n",
            "          12       0.35      0.47      0.40        55\n",
            "          13       0.00      0.00      0.00        30\n",
            "          14       0.62      0.79      0.69        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.25      0.22      0.23        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.48      0.29      0.36        35\n",
            "           6       0.67      0.75      0.71        44\n",
            "           7       0.18      0.38      0.24        56\n",
            "           8       0.47      0.17      0.25        47\n",
            "           9       0.33      0.18      0.23        45\n",
            "\n",
            "    accuracy                           0.35       637\n",
            "   macro avg       0.31      0.30      0.29       637\n",
            "weighted avg       0.34      0.35      0.33       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 7 [320/3527 (9%)]\tLoss: 2.130682\tLR:0.000079595\n",
            "Train Epoch: 7 [640/3527 (18%)]\tLoss: 2.258166\tLR:0.000079595\n",
            "Train Epoch: 7 [960/3527 (27%)]\tLoss: 2.231956\tLR:0.000079595\n",
            "Train Epoch: 7 [1280/3527 (36%)]\tLoss: 1.974184\tLR:0.000079595\n",
            "Train Epoch: 7 [1600/3527 (45%)]\tLoss: 2.159621\tLR:0.000079595\n",
            "Train Epoch: 7 [1920/3527 (54%)]\tLoss: 2.289210\tLR:0.000079595\n",
            "Train Epoch: 7 [2240/3527 (63%)]\tLoss: 2.005811\tLR:0.000079595\n",
            "Train Epoch: 7 [2560/3527 (72%)]\tLoss: 1.981105\tLR:0.000079595\n",
            "Train Epoch: 7 [2880/3527 (81%)]\tLoss: 2.368145\tLR:0.000079595\n",
            "Train Epoch: 7 [3200/3527 (90%)]\tLoss: 1.966725\tLR:0.000079595\n",
            "Train Epoch: 7 [3520/3527 (99%)]\tLoss: 1.934903\tLR:0.000079595\n",
            "6\n",
            "epoch:7,loss:2.0830207805375793\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.8454, Accuracy: 233/637 (37%)\n",
            "\n",
            "Training complete in 0m 40s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.64      0.48        69\n",
            "           1       0.38      0.29      0.33        56\n",
            "          10       0.33      0.16      0.22        43\n",
            "          11       0.22      0.11      0.14        47\n",
            "          12       0.35      0.35      0.35        55\n",
            "          13       0.50      0.07      0.12        30\n",
            "          14       0.66      0.81      0.72        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.32      0.20      0.24        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.35      0.34      0.35        35\n",
            "           6       0.58      0.86      0.69        44\n",
            "           7       0.15      0.38      0.22        56\n",
            "           8       0.42      0.32      0.36        47\n",
            "           9       0.35      0.18      0.24        45\n",
            "\n",
            "    accuracy                           0.37       637\n",
            "   macro avg       0.33      0.31      0.30       637\n",
            "weighted avg       0.36      0.37      0.34       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 8 [320/3527 (9%)]\tLoss: 1.972969\tLR:0.000072973\n",
            "Train Epoch: 8 [640/3527 (18%)]\tLoss: 1.957960\tLR:0.000072973\n",
            "Train Epoch: 8 [960/3527 (27%)]\tLoss: 2.000346\tLR:0.000072973\n",
            "Train Epoch: 8 [1280/3527 (36%)]\tLoss: 2.032868\tLR:0.000072973\n",
            "Train Epoch: 8 [1600/3527 (45%)]\tLoss: 1.977236\tLR:0.000072973\n",
            "Train Epoch: 8 [1920/3527 (54%)]\tLoss: 2.238222\tLR:0.000072973\n",
            "Train Epoch: 8 [2240/3527 (63%)]\tLoss: 2.125501\tLR:0.000072973\n",
            "Train Epoch: 8 [2560/3527 (72%)]\tLoss: 1.955956\tLR:0.000072973\n",
            "Train Epoch: 8 [2880/3527 (81%)]\tLoss: 2.012726\tLR:0.000072973\n",
            "Train Epoch: 8 [3200/3527 (90%)]\tLoss: 2.004367\tLR:0.000072973\n",
            "Train Epoch: 8 [3520/3527 (99%)]\tLoss: 2.240814\tLR:0.000072973\n",
            "6\n",
            "epoch:8,loss:2.0558850120853736\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.8207, Accuracy: 249/637 (39%)\n",
            "\n",
            "Training complete in 0m 40s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.48      0.49        69\n",
            "           1       0.32      0.38      0.34        56\n",
            "          10       0.37      0.30      0.33        43\n",
            "          11       0.43      0.06      0.11        47\n",
            "          12       0.31      0.38      0.34        55\n",
            "          13       0.30      0.20      0.24        30\n",
            "          14       0.93      0.79      0.85        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.28      0.39      0.32        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.32      0.37      0.34        35\n",
            "           6       0.74      0.80      0.77        44\n",
            "           7       0.25      0.48      0.33        56\n",
            "           8       0.25      0.34      0.29        47\n",
            "           9       0.42      0.18      0.25        45\n",
            "\n",
            "    accuracy                           0.39       637\n",
            "   macro avg       0.36      0.34      0.33       637\n",
            "weighted avg       0.41      0.39      0.38       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 9 [320/3527 (9%)]\tLoss: 1.898335\tLR:0.000065796\n",
            "Train Epoch: 9 [640/3527 (18%)]\tLoss: 2.110843\tLR:0.000065796\n",
            "Train Epoch: 9 [960/3527 (27%)]\tLoss: 2.067829\tLR:0.000065796\n",
            "Train Epoch: 9 [1280/3527 (36%)]\tLoss: 2.049239\tLR:0.000065796\n",
            "Train Epoch: 9 [1600/3527 (45%)]\tLoss: 1.989262\tLR:0.000065796\n",
            "Train Epoch: 9 [1920/3527 (54%)]\tLoss: 1.875606\tLR:0.000065796\n",
            "Train Epoch: 9 [2240/3527 (63%)]\tLoss: 2.050800\tLR:0.000065796\n",
            "Train Epoch: 9 [2560/3527 (72%)]\tLoss: 1.897401\tLR:0.000065796\n",
            "Train Epoch: 9 [2880/3527 (81%)]\tLoss: 1.969263\tLR:0.000065796\n",
            "Train Epoch: 9 [3200/3527 (90%)]\tLoss: 2.124100\tLR:0.000065796\n",
            "Train Epoch: 9 [3520/3527 (99%)]\tLoss: 1.773761\tLR:0.000065796\n",
            "6\n",
            "epoch:9,loss:1.995267034650923\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.7678, Accuracy: 244/637 (38%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.41      0.49        69\n",
            "           1       0.35      0.64      0.46        56\n",
            "          10       0.36      0.30      0.33        43\n",
            "          11       0.40      0.26      0.31        47\n",
            "          12       0.24      0.29      0.26        55\n",
            "          13       0.09      0.03      0.05        30\n",
            "          14       0.95      0.81      0.87        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.16      0.34      0.22        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.27      0.34      0.30        35\n",
            "           6       0.60      0.86      0.71        44\n",
            "           7       0.22      0.25      0.24        56\n",
            "           8       0.42      0.34      0.38        47\n",
            "           9       0.46      0.13      0.21        45\n",
            "\n",
            "    accuracy                           0.38       637\n",
            "   macro avg       0.34      0.33      0.32       637\n",
            "weighted avg       0.40      0.38      0.37       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 10 [320/3527 (9%)]\tLoss: 2.180008\tLR:0.000058244\n",
            "Train Epoch: 10 [640/3527 (18%)]\tLoss: 2.105084\tLR:0.000058244\n",
            "Train Epoch: 10 [960/3527 (27%)]\tLoss: 2.038180\tLR:0.000058244\n",
            "Train Epoch: 10 [1280/3527 (36%)]\tLoss: 1.931104\tLR:0.000058244\n",
            "Train Epoch: 10 [1600/3527 (45%)]\tLoss: 1.845153\tLR:0.000058244\n",
            "Train Epoch: 10 [1920/3527 (54%)]\tLoss: 2.002793\tLR:0.000058244\n",
            "Train Epoch: 10 [2240/3527 (63%)]\tLoss: 1.944045\tLR:0.000058244\n",
            "Train Epoch: 10 [2560/3527 (72%)]\tLoss: 1.929399\tLR:0.000058244\n",
            "Train Epoch: 10 [2880/3527 (81%)]\tLoss: 1.831927\tLR:0.000058244\n",
            "Train Epoch: 10 [3200/3527 (90%)]\tLoss: 2.489563\tLR:0.000058244\n",
            "Train Epoch: 10 [3520/3527 (99%)]\tLoss: 1.847379\tLR:0.000058244\n",
            "6\n",
            "epoch:10,loss:1.9703322326814807\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.6830, Accuracy: 256/637 (40%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.48      0.54        69\n",
            "           1       0.30      0.57      0.40        56\n",
            "          10       0.48      0.28      0.35        43\n",
            "          11       0.28      0.30      0.29        47\n",
            "          12       0.39      0.40      0.40        55\n",
            "          13       0.33      0.07      0.11        30\n",
            "          14       0.88      0.81      0.84        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.19      0.20      0.19        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.62      0.29      0.39        35\n",
            "           6       0.73      0.86      0.79        44\n",
            "           7       0.19      0.41      0.26        56\n",
            "           8       0.46      0.26      0.33        47\n",
            "           9       0.29      0.27      0.28        45\n",
            "\n",
            "    accuracy                           0.40       637\n",
            "   macro avg       0.39      0.35      0.34       637\n",
            "weighted avg       0.43      0.40      0.40       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 11 [320/3527 (9%)]\tLoss: 1.970171\tLR:0.000050500\n",
            "Train Epoch: 11 [640/3527 (18%)]\tLoss: 1.813621\tLR:0.000050500\n",
            "Train Epoch: 11 [960/3527 (27%)]\tLoss: 2.076365\tLR:0.000050500\n",
            "Train Epoch: 11 [1280/3527 (36%)]\tLoss: 1.650410\tLR:0.000050500\n",
            "Train Epoch: 11 [1600/3527 (45%)]\tLoss: 1.878504\tLR:0.000050500\n",
            "Train Epoch: 11 [1920/3527 (54%)]\tLoss: 2.521768\tLR:0.000050500\n",
            "Train Epoch: 11 [2240/3527 (63%)]\tLoss: 2.463466\tLR:0.000050500\n",
            "Train Epoch: 11 [2560/3527 (72%)]\tLoss: 1.960956\tLR:0.000050500\n",
            "Train Epoch: 11 [2880/3527 (81%)]\tLoss: 1.701789\tLR:0.000050500\n",
            "Train Epoch: 11 [3200/3527 (90%)]\tLoss: 2.100873\tLR:0.000050500\n",
            "Train Epoch: 11 [3520/3527 (99%)]\tLoss: 1.712813\tLR:0.000050500\n",
            "6\n",
            "epoch:11,loss:1.9330083099571433\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.6496, Accuracy: 260/637 (41%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.43      0.52        69\n",
            "           1       0.37      0.57      0.45        56\n",
            "          10       0.43      0.23      0.30        43\n",
            "          11       0.35      0.17      0.23        47\n",
            "          12       0.27      0.60      0.37        55\n",
            "          13       0.14      0.03      0.05        30\n",
            "          14       0.93      0.81      0.86        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.23      0.27      0.25        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.33      0.46      0.38        35\n",
            "           6       0.73      0.84      0.78        44\n",
            "           7       0.30      0.30      0.30        56\n",
            "           8       0.44      0.34      0.39        47\n",
            "           9       0.26      0.24      0.25        45\n",
            "\n",
            "    accuracy                           0.41       637\n",
            "   macro avg       0.36      0.35      0.34       637\n",
            "weighted avg       0.42      0.41      0.39       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 12 [320/3527 (9%)]\tLoss: 1.823729\tLR:0.000042756\n",
            "Train Epoch: 12 [640/3527 (18%)]\tLoss: 1.566963\tLR:0.000042756\n",
            "Train Epoch: 12 [960/3527 (27%)]\tLoss: 1.923065\tLR:0.000042756\n",
            "Train Epoch: 12 [1280/3527 (36%)]\tLoss: 1.992545\tLR:0.000042756\n",
            "Train Epoch: 12 [1600/3527 (45%)]\tLoss: 1.770946\tLR:0.000042756\n",
            "Train Epoch: 12 [1920/3527 (54%)]\tLoss: 1.673256\tLR:0.000042756\n",
            "Train Epoch: 12 [2240/3527 (63%)]\tLoss: 1.808594\tLR:0.000042756\n",
            "Train Epoch: 12 [2560/3527 (72%)]\tLoss: 1.987980\tLR:0.000042756\n",
            "Train Epoch: 12 [2880/3527 (81%)]\tLoss: 1.895356\tLR:0.000042756\n",
            "Train Epoch: 12 [3200/3527 (90%)]\tLoss: 1.858663\tLR:0.000042756\n",
            "Train Epoch: 12 [3520/3527 (99%)]\tLoss: 1.908526\tLR:0.000042756\n",
            "6\n",
            "epoch:12,loss:1.8874615269738275\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.6625, Accuracy: 268/637 (42%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.46      0.52        69\n",
            "           1       0.46      0.43      0.44        56\n",
            "          10       0.37      0.35      0.36        43\n",
            "          11       0.26      0.21      0.24        47\n",
            "          12       0.29      0.45      0.35        55\n",
            "          13       0.22      0.23      0.23        30\n",
            "          14       0.97      0.81      0.88        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.21      0.32      0.25        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.43      0.46      0.44        35\n",
            "           6       0.90      0.86      0.88        44\n",
            "           7       0.29      0.46      0.36        56\n",
            "           8       0.40      0.30      0.34        47\n",
            "           9       0.38      0.22      0.28        45\n",
            "\n",
            "    accuracy                           0.42       637\n",
            "   macro avg       0.39      0.37      0.37       637\n",
            "weighted avg       0.44      0.42      0.42       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 13 [320/3527 (9%)]\tLoss: 2.210341\tLR:0.000035204\n",
            "Train Epoch: 13 [640/3527 (18%)]\tLoss: 2.029229\tLR:0.000035204\n",
            "Train Epoch: 13 [960/3527 (27%)]\tLoss: 1.971137\tLR:0.000035204\n",
            "Train Epoch: 13 [1280/3527 (36%)]\tLoss: 1.825140\tLR:0.000035204\n",
            "Train Epoch: 13 [1600/3527 (45%)]\tLoss: 1.986413\tLR:0.000035204\n",
            "Train Epoch: 13 [1920/3527 (54%)]\tLoss: 1.969688\tLR:0.000035204\n",
            "Train Epoch: 13 [2240/3527 (63%)]\tLoss: 1.859559\tLR:0.000035204\n",
            "Train Epoch: 13 [2560/3527 (72%)]\tLoss: 2.110259\tLR:0.000035204\n",
            "Train Epoch: 13 [2880/3527 (81%)]\tLoss: 1.503837\tLR:0.000035204\n",
            "Train Epoch: 13 [3200/3527 (90%)]\tLoss: 1.761644\tLR:0.000035204\n",
            "Train Epoch: 13 [3520/3527 (99%)]\tLoss: 1.992645\tLR:0.000035204\n",
            "6\n",
            "epoch:13,loss:1.8633416440035846\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.5983, Accuracy: 287/637 (45%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.61      0.63        69\n",
            "           1       0.39      0.48      0.43        56\n",
            "          10       0.44      0.28      0.34        43\n",
            "          11       0.38      0.23      0.29        47\n",
            "          12       0.27      0.49      0.35        55\n",
            "          13       0.24      0.20      0.22        30\n",
            "          14       0.95      0.81      0.87        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.25      0.32      0.28        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.38      0.57      0.45        35\n",
            "           6       0.79      0.95      0.87        44\n",
            "           7       0.41      0.34      0.37        56\n",
            "           8       0.38      0.47      0.42        47\n",
            "           9       0.53      0.18      0.27        45\n",
            "\n",
            "    accuracy                           0.45       637\n",
            "   macro avg       0.40      0.40      0.39       637\n",
            "weighted avg       0.46      0.45      0.44       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 14 [320/3527 (9%)]\tLoss: 2.516426\tLR:0.000028027\n",
            "Train Epoch: 14 [640/3527 (18%)]\tLoss: 2.039136\tLR:0.000028027\n",
            "Train Epoch: 14 [960/3527 (27%)]\tLoss: 1.838734\tLR:0.000028027\n",
            "Train Epoch: 14 [1280/3527 (36%)]\tLoss: 1.724406\tLR:0.000028027\n",
            "Train Epoch: 14 [1600/3527 (45%)]\tLoss: 2.164236\tLR:0.000028027\n",
            "Train Epoch: 14 [1920/3527 (54%)]\tLoss: 1.674344\tLR:0.000028027\n",
            "Train Epoch: 14 [2240/3527 (63%)]\tLoss: 1.718239\tLR:0.000028027\n",
            "Train Epoch: 14 [2560/3527 (72%)]\tLoss: 1.711927\tLR:0.000028027\n",
            "Train Epoch: 14 [2880/3527 (81%)]\tLoss: 1.704904\tLR:0.000028027\n",
            "Train Epoch: 14 [3200/3527 (90%)]\tLoss: 1.859305\tLR:0.000028027\n",
            "Train Epoch: 14 [3520/3527 (99%)]\tLoss: 1.643756\tLR:0.000028027\n",
            "6\n",
            "epoch:14,loss:1.810183511124001\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.5579, Accuracy: 297/637 (47%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.67      0.66        69\n",
            "           1       0.43      0.54      0.48        56\n",
            "          10       0.47      0.37      0.42        43\n",
            "          11       0.48      0.32      0.38        47\n",
            "          12       0.33      0.51      0.40        55\n",
            "          13       0.33      0.07      0.11        30\n",
            "          14       1.00      0.79      0.88        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.22      0.41      0.29        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.42      0.46      0.44        35\n",
            "           6       0.78      0.82      0.80        44\n",
            "           7       0.38      0.43      0.40        56\n",
            "           8       0.36      0.43      0.39        47\n",
            "           9       0.56      0.22      0.32        45\n",
            "\n",
            "    accuracy                           0.47       637\n",
            "   macro avg       0.43      0.40      0.40       637\n",
            "weighted avg       0.48      0.47      0.46       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 15 [320/3527 (9%)]\tLoss: 2.112835\tLR:0.000021405\n",
            "Train Epoch: 15 [640/3527 (18%)]\tLoss: 1.550945\tLR:0.000021405\n",
            "Train Epoch: 15 [960/3527 (27%)]\tLoss: 1.981604\tLR:0.000021405\n",
            "Train Epoch: 15 [1280/3527 (36%)]\tLoss: 1.513766\tLR:0.000021405\n",
            "Train Epoch: 15 [1600/3527 (45%)]\tLoss: 1.698674\tLR:0.000021405\n",
            "Train Epoch: 15 [1920/3527 (54%)]\tLoss: 1.634629\tLR:0.000021405\n",
            "Train Epoch: 15 [2240/3527 (63%)]\tLoss: 1.729265\tLR:0.000021405\n",
            "Train Epoch: 15 [2560/3527 (72%)]\tLoss: 1.644386\tLR:0.000021405\n",
            "Train Epoch: 15 [2880/3527 (81%)]\tLoss: 2.017416\tLR:0.000021405\n",
            "Train Epoch: 15 [3200/3527 (90%)]\tLoss: 1.768516\tLR:0.000021405\n",
            "Train Epoch: 15 [3520/3527 (99%)]\tLoss: 1.894580\tLR:0.000021405\n",
            "6\n",
            "epoch:15,loss:1.7826277275343199\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.5326, Accuracy: 314/637 (49%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.64      0.66        69\n",
            "           1       0.57      0.55      0.56        56\n",
            "          10       0.48      0.37      0.42        43\n",
            "          11       0.61      0.30      0.40        47\n",
            "          12       0.35      0.47      0.40        55\n",
            "          13       0.32      0.20      0.24        30\n",
            "          14       0.97      0.79      0.87        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.24      0.51      0.32        41\n",
            "           4       0.40      0.20      0.27        20\n",
            "           5       0.39      0.46      0.42        35\n",
            "           6       0.81      0.95      0.88        44\n",
            "           7       0.47      0.48      0.47        56\n",
            "           8       0.37      0.45      0.40        47\n",
            "           9       0.39      0.20      0.26        45\n",
            "\n",
            "    accuracy                           0.49       637\n",
            "   macro avg       0.47      0.44      0.44       637\n",
            "weighted avg       0.52      0.49      0.49       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 16 [320/3527 (9%)]\tLoss: 1.590838\tLR:0.000015498\n",
            "Train Epoch: 16 [640/3527 (18%)]\tLoss: 1.854216\tLR:0.000015498\n",
            "Train Epoch: 16 [960/3527 (27%)]\tLoss: 1.427716\tLR:0.000015498\n",
            "Train Epoch: 16 [1280/3527 (36%)]\tLoss: 1.668620\tLR:0.000015498\n",
            "Train Epoch: 16 [1600/3527 (45%)]\tLoss: 1.566262\tLR:0.000015498\n",
            "Train Epoch: 16 [1920/3527 (54%)]\tLoss: 1.795329\tLR:0.000015498\n",
            "Train Epoch: 16 [2240/3527 (63%)]\tLoss: 1.808051\tLR:0.000015498\n",
            "Train Epoch: 16 [2560/3527 (72%)]\tLoss: 1.772300\tLR:0.000015498\n",
            "Train Epoch: 16 [2880/3527 (81%)]\tLoss: 1.632242\tLR:0.000015498\n",
            "Train Epoch: 16 [3200/3527 (90%)]\tLoss: 1.564652\tLR:0.000015498\n",
            "Train Epoch: 16 [3520/3527 (99%)]\tLoss: 2.089149\tLR:0.000015498\n",
            "6\n",
            "epoch:16,loss:1.7585569813444808\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.5239, Accuracy: 310/637 (49%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.55      0.62        69\n",
            "           1       0.56      0.54      0.55        56\n",
            "          10       0.51      0.44      0.48        43\n",
            "          11       0.53      0.38      0.44        47\n",
            "          12       0.33      0.47      0.39        55\n",
            "          13       0.29      0.13      0.18        30\n",
            "          14       0.93      0.83      0.88        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.27      0.44      0.33        41\n",
            "           4       0.20      0.20      0.20        20\n",
            "           5       0.41      0.51      0.46        35\n",
            "           6       0.77      0.93      0.85        44\n",
            "           7       0.39      0.48      0.43        56\n",
            "           8       0.40      0.40      0.40        47\n",
            "           9       0.41      0.20      0.27        45\n",
            "\n",
            "    accuracy                           0.49       637\n",
            "   macro avg       0.45      0.43      0.43       637\n",
            "weighted avg       0.50      0.49      0.48       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 17 [320/3527 (9%)]\tLoss: 1.519902\tLR:0.000010454\n",
            "Train Epoch: 17 [640/3527 (18%)]\tLoss: 1.974747\tLR:0.000010454\n",
            "Train Epoch: 17 [960/3527 (27%)]\tLoss: 1.704952\tLR:0.000010454\n",
            "Train Epoch: 17 [1280/3527 (36%)]\tLoss: 2.062957\tLR:0.000010454\n",
            "Train Epoch: 17 [1600/3527 (45%)]\tLoss: 1.544815\tLR:0.000010454\n",
            "Train Epoch: 17 [1920/3527 (54%)]\tLoss: 2.009615\tLR:0.000010454\n",
            "Train Epoch: 17 [2240/3527 (63%)]\tLoss: 1.680996\tLR:0.000010454\n",
            "Train Epoch: 17 [2560/3527 (72%)]\tLoss: 1.726450\tLR:0.000010454\n",
            "Train Epoch: 17 [2880/3527 (81%)]\tLoss: 1.562292\tLR:0.000010454\n",
            "Train Epoch: 17 [3200/3527 (90%)]\tLoss: 1.704838\tLR:0.000010454\n",
            "Train Epoch: 17 [3520/3527 (99%)]\tLoss: 2.518826\tLR:0.000010454\n",
            "6\n",
            "epoch:17,loss:1.7571245485597902\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.5244, Accuracy: 302/637 (47%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.54      0.60        69\n",
            "           1       0.57      0.57      0.57        56\n",
            "          10       0.48      0.35      0.41        43\n",
            "          11       0.52      0.26      0.34        47\n",
            "          12       0.30      0.51      0.38        55\n",
            "          13       0.19      0.13      0.16        30\n",
            "          14       0.95      0.81      0.87        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.25      0.44      0.32        41\n",
            "           4       0.33      0.20      0.25        20\n",
            "           5       0.32      0.54      0.40        35\n",
            "           6       0.78      0.89      0.83        44\n",
            "           7       0.46      0.52      0.49        56\n",
            "           8       0.43      0.40      0.42        47\n",
            "           9       0.38      0.18      0.24        45\n",
            "\n",
            "    accuracy                           0.47       637\n",
            "   macro avg       0.44      0.42      0.42       637\n",
            "weighted avg       0.50      0.47      0.47       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 18 [320/3527 (9%)]\tLoss: 1.581416\tLR:0.000006395\n",
            "Train Epoch: 18 [640/3527 (18%)]\tLoss: 1.722140\tLR:0.000006395\n",
            "Train Epoch: 18 [960/3527 (27%)]\tLoss: 1.912545\tLR:0.000006395\n",
            "Train Epoch: 18 [1280/3527 (36%)]\tLoss: 1.562331\tLR:0.000006395\n",
            "Train Epoch: 18 [1600/3527 (45%)]\tLoss: 1.623687\tLR:0.000006395\n",
            "Train Epoch: 18 [1920/3527 (54%)]\tLoss: 1.734754\tLR:0.000006395\n",
            "Train Epoch: 18 [2240/3527 (63%)]\tLoss: 1.713412\tLR:0.000006395\n",
            "Train Epoch: 18 [2560/3527 (72%)]\tLoss: 1.773096\tLR:0.000006395\n",
            "Train Epoch: 18 [2880/3527 (81%)]\tLoss: 1.704671\tLR:0.000006395\n",
            "Train Epoch: 18 [3200/3527 (90%)]\tLoss: 1.652630\tLR:0.000006395\n",
            "Train Epoch: 18 [3520/3527 (99%)]\tLoss: 1.655901\tLR:0.000006395\n",
            "6\n",
            "epoch:18,loss:1.7443690546998032\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4882, Accuracy: 316/637 (50%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.58      0.65        69\n",
            "           1       0.53      0.61      0.57        56\n",
            "          10       0.49      0.40      0.44        43\n",
            "          11       0.63      0.40      0.49        47\n",
            "          12       0.34      0.53      0.41        55\n",
            "          13       0.27      0.20      0.23        30\n",
            "          14       1.00      0.81      0.89        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.25      0.39      0.31        41\n",
            "           4       0.23      0.15      0.18        20\n",
            "           5       0.39      0.51      0.44        35\n",
            "           6       0.84      0.93      0.88        44\n",
            "           7       0.42      0.48      0.45        56\n",
            "           8       0.36      0.40      0.38        47\n",
            "           9       0.45      0.20      0.28        45\n",
            "\n",
            "    accuracy                           0.50       637\n",
            "   macro avg       0.46      0.44      0.44       637\n",
            "weighted avg       0.52      0.50      0.50       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 19 [320/3527 (9%)]\tLoss: 1.664697\tLR:0.000003423\n",
            "Train Epoch: 19 [640/3527 (18%)]\tLoss: 1.834425\tLR:0.000003423\n",
            "Train Epoch: 19 [960/3527 (27%)]\tLoss: 1.471987\tLR:0.000003423\n",
            "Train Epoch: 19 [1280/3527 (36%)]\tLoss: 1.736457\tLR:0.000003423\n",
            "Train Epoch: 19 [1600/3527 (45%)]\tLoss: 1.531343\tLR:0.000003423\n",
            "Train Epoch: 19 [1920/3527 (54%)]\tLoss: 1.729573\tLR:0.000003423\n",
            "Train Epoch: 19 [2240/3527 (63%)]\tLoss: 1.733572\tLR:0.000003423\n",
            "Train Epoch: 19 [2560/3527 (72%)]\tLoss: 1.659585\tLR:0.000003423\n",
            "Train Epoch: 19 [2880/3527 (81%)]\tLoss: 1.592633\tLR:0.000003423\n",
            "Train Epoch: 19 [3200/3527 (90%)]\tLoss: 1.952976\tLR:0.000003423\n",
            "Train Epoch: 19 [3520/3527 (99%)]\tLoss: 1.632054\tLR:0.000003423\n",
            "6\n",
            "epoch:19,loss:1.7190635580200333\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4713, Accuracy: 322/637 (51%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.62      0.68        69\n",
            "           1       0.58      0.59      0.58        56\n",
            "          10       0.59      0.40      0.47        43\n",
            "          11       0.54      0.28      0.37        47\n",
            "          12       0.34      0.49      0.40        55\n",
            "          13       0.43      0.30      0.35        30\n",
            "          14       0.97      0.83      0.90        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.25      0.41      0.31        41\n",
            "           4       0.27      0.15      0.19        20\n",
            "           5       0.39      0.51      0.44        35\n",
            "           6       0.72      0.95      0.82        44\n",
            "           7       0.48      0.50      0.49        56\n",
            "           8       0.37      0.47      0.41        47\n",
            "           9       0.41      0.24      0.31        45\n",
            "\n",
            "    accuracy                           0.51       637\n",
            "   macro avg       0.47      0.45      0.45       637\n",
            "weighted avg       0.53      0.51      0.50       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 20 [320/3527 (9%)]\tLoss: 1.714913\tLR:0.000001609\n",
            "Train Epoch: 20 [640/3527 (18%)]\tLoss: 1.811513\tLR:0.000001609\n",
            "Train Epoch: 20 [960/3527 (27%)]\tLoss: 1.464303\tLR:0.000001609\n",
            "Train Epoch: 20 [1280/3527 (36%)]\tLoss: 1.587404\tLR:0.000001609\n",
            "Train Epoch: 20 [1600/3527 (45%)]\tLoss: 1.427083\tLR:0.000001609\n",
            "Train Epoch: 20 [1920/3527 (54%)]\tLoss: 1.768279\tLR:0.000001609\n",
            "Train Epoch: 20 [2240/3527 (63%)]\tLoss: 1.883289\tLR:0.000001609\n",
            "Train Epoch: 20 [2560/3527 (72%)]\tLoss: 2.198389\tLR:0.000001609\n",
            "Train Epoch: 20 [2880/3527 (81%)]\tLoss: 1.837172\tLR:0.000001609\n",
            "Train Epoch: 20 [3200/3527 (90%)]\tLoss: 1.655677\tLR:0.000001609\n",
            "Train Epoch: 20 [3520/3527 (99%)]\tLoss: 1.649501\tLR:0.000001609\n",
            "6\n",
            "epoch:20,loss:1.6943154667948817\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4700, Accuracy: 320/637 (50%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.65      0.68        69\n",
            "           1       0.52      0.59      0.55        56\n",
            "          10       0.52      0.37      0.43        43\n",
            "          11       0.62      0.34      0.44        47\n",
            "          12       0.35      0.45      0.40        55\n",
            "          13       0.36      0.27      0.31        30\n",
            "          14       1.00      0.81      0.89        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.26      0.41      0.32        41\n",
            "           4       0.33      0.30      0.32        20\n",
            "           5       0.36      0.51      0.42        35\n",
            "           6       0.75      0.93      0.83        44\n",
            "           7       0.46      0.50      0.48        56\n",
            "           8       0.39      0.40      0.40        47\n",
            "           9       0.43      0.22      0.29        45\n",
            "\n",
            "    accuracy                           0.50       637\n",
            "   macro avg       0.47      0.45      0.45       637\n",
            "weighted avg       0.52      0.50      0.50       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 21 [320/3527 (9%)]\tLoss: 1.786410\tLR:0.000001000\n",
            "Train Epoch: 21 [640/3527 (18%)]\tLoss: 1.651401\tLR:0.000001000\n",
            "Train Epoch: 21 [960/3527 (27%)]\tLoss: 1.551356\tLR:0.000001000\n",
            "Train Epoch: 21 [1280/3527 (36%)]\tLoss: 2.300992\tLR:0.000001000\n",
            "Train Epoch: 21 [1600/3527 (45%)]\tLoss: 1.278488\tLR:0.000001000\n",
            "Train Epoch: 21 [1920/3527 (54%)]\tLoss: 1.697983\tLR:0.000001000\n",
            "Train Epoch: 21 [2240/3527 (63%)]\tLoss: 1.472721\tLR:0.000001000\n",
            "Train Epoch: 21 [2560/3527 (72%)]\tLoss: 1.577517\tLR:0.000001000\n",
            "Train Epoch: 21 [2880/3527 (81%)]\tLoss: 1.555987\tLR:0.000001000\n",
            "Train Epoch: 21 [3200/3527 (90%)]\tLoss: 1.552164\tLR:0.000001000\n",
            "Train Epoch: 21 [3520/3527 (99%)]\tLoss: 1.582570\tLR:0.000001000\n",
            "6\n",
            "epoch:21,loss:1.6740172156342514\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4687, Accuracy: 325/637 (51%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.62      0.67        69\n",
            "           1       0.61      0.55      0.58        56\n",
            "          10       0.52      0.37      0.43        43\n",
            "          11       0.65      0.36      0.47        47\n",
            "          12       0.33      0.49      0.39        55\n",
            "          13       0.38      0.30      0.33        30\n",
            "          14       0.95      0.83      0.89        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.28      0.41      0.34        41\n",
            "           4       0.27      0.20      0.23        20\n",
            "           5       0.40      0.49      0.44        35\n",
            "           6       0.78      0.95      0.86        44\n",
            "           7       0.48      0.55      0.51        56\n",
            "           8       0.36      0.45      0.40        47\n",
            "           9       0.41      0.24      0.31        45\n",
            "\n",
            "    accuracy                           0.51       637\n",
            "   macro avg       0.48      0.46      0.46       637\n",
            "weighted avg       0.53      0.51      0.51       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 22 [320/3527 (9%)]\tLoss: 1.637681\tLR:0.000001609\n",
            "Train Epoch: 22 [640/3527 (18%)]\tLoss: 1.653730\tLR:0.000001609\n",
            "Train Epoch: 22 [960/3527 (27%)]\tLoss: 1.324879\tLR:0.000001609\n",
            "Train Epoch: 22 [1280/3527 (36%)]\tLoss: 1.531159\tLR:0.000001609\n",
            "Train Epoch: 22 [1600/3527 (45%)]\tLoss: 1.554066\tLR:0.000001609\n",
            "Train Epoch: 22 [1920/3527 (54%)]\tLoss: 1.543156\tLR:0.000001609\n",
            "Train Epoch: 22 [2240/3527 (63%)]\tLoss: 1.548031\tLR:0.000001609\n",
            "Train Epoch: 22 [2560/3527 (72%)]\tLoss: 1.713905\tLR:0.000001609\n",
            "Train Epoch: 22 [2880/3527 (81%)]\tLoss: 2.116632\tLR:0.000001609\n",
            "Train Epoch: 22 [3200/3527 (90%)]\tLoss: 2.392478\tLR:0.000001609\n",
            "Train Epoch: 22 [3520/3527 (99%)]\tLoss: 1.623533\tLR:0.000001609\n",
            "6\n",
            "epoch:22,loss:1.6971551448375255\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4620, Accuracy: 320/637 (50%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.68      0.69        69\n",
            "           1       0.56      0.59      0.57        56\n",
            "          10       0.53      0.37      0.44        43\n",
            "          11       0.74      0.36      0.49        47\n",
            "          12       0.33      0.49      0.40        55\n",
            "          13       0.29      0.27      0.28        30\n",
            "          14       0.97      0.81      0.88        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.26      0.41      0.32        41\n",
            "           4       0.31      0.20      0.24        20\n",
            "           5       0.36      0.51      0.42        35\n",
            "           6       0.72      0.93      0.81        44\n",
            "           7       0.46      0.46      0.46        56\n",
            "           8       0.41      0.38      0.40        47\n",
            "           9       0.42      0.22      0.29        45\n",
            "\n",
            "    accuracy                           0.50       637\n",
            "   macro avg       0.47      0.45      0.45       637\n",
            "weighted avg       0.53      0.50      0.50       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 23 [320/3527 (9%)]\tLoss: 1.708881\tLR:0.000003423\n",
            "Train Epoch: 23 [640/3527 (18%)]\tLoss: 1.659998\tLR:0.000003423\n",
            "Train Epoch: 23 [960/3527 (27%)]\tLoss: 1.715537\tLR:0.000003423\n",
            "Train Epoch: 23 [1280/3527 (36%)]\tLoss: 1.403975\tLR:0.000003423\n",
            "Train Epoch: 23 [1600/3527 (45%)]\tLoss: 2.635300\tLR:0.000003423\n",
            "Train Epoch: 23 [1920/3527 (54%)]\tLoss: 2.460926\tLR:0.000003423\n",
            "Train Epoch: 23 [2240/3527 (63%)]\tLoss: 1.760661\tLR:0.000003423\n",
            "Train Epoch: 23 [2560/3527 (72%)]\tLoss: 1.543189\tLR:0.000003423\n",
            "Train Epoch: 23 [2880/3527 (81%)]\tLoss: 1.726512\tLR:0.000003423\n",
            "Train Epoch: 23 [3200/3527 (90%)]\tLoss: 1.551275\tLR:0.000003423\n",
            "Train Epoch: 23 [3520/3527 (99%)]\tLoss: 2.330479\tLR:0.000003423\n",
            "6\n",
            "epoch:23,loss:1.726487455067334\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4513, Accuracy: 325/637 (51%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.65      0.69        69\n",
            "           1       0.57      0.57      0.57        56\n",
            "          10       0.53      0.37      0.44        43\n",
            "          11       0.65      0.32      0.43        47\n",
            "          12       0.34      0.49      0.40        55\n",
            "          13       0.35      0.27      0.30        30\n",
            "          14       0.97      0.81      0.88        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.28      0.46      0.35        41\n",
            "           4       0.29      0.20      0.24        20\n",
            "           5       0.41      0.51      0.46        35\n",
            "           6       0.78      0.95      0.86        44\n",
            "           7       0.51      0.55      0.53        56\n",
            "           8       0.34      0.43      0.38        47\n",
            "           9       0.40      0.22      0.29        45\n",
            "\n",
            "    accuracy                           0.51       637\n",
            "   macro avg       0.48      0.45      0.45       637\n",
            "weighted avg       0.53      0.51      0.51       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 24 [320/3527 (9%)]\tLoss: 1.810045\tLR:0.000006395\n",
            "Train Epoch: 24 [640/3527 (18%)]\tLoss: 2.615326\tLR:0.000006395\n",
            "Train Epoch: 24 [960/3527 (27%)]\tLoss: 1.895365\tLR:0.000006395\n",
            "Train Epoch: 24 [1280/3527 (36%)]\tLoss: 1.804845\tLR:0.000006395\n",
            "Train Epoch: 24 [1600/3527 (45%)]\tLoss: 1.671603\tLR:0.000006395\n",
            "Train Epoch: 24 [1920/3527 (54%)]\tLoss: 1.694005\tLR:0.000006395\n",
            "Train Epoch: 24 [2240/3527 (63%)]\tLoss: 1.459834\tLR:0.000006395\n",
            "Train Epoch: 24 [2560/3527 (72%)]\tLoss: 1.466318\tLR:0.000006395\n",
            "Train Epoch: 24 [2880/3527 (81%)]\tLoss: 1.503719\tLR:0.000006395\n",
            "Train Epoch: 24 [3200/3527 (90%)]\tLoss: 1.554618\tLR:0.000006395\n",
            "Train Epoch: 24 [3520/3527 (99%)]\tLoss: 1.520077\tLR:0.000006395\n",
            "6\n",
            "epoch:24,loss:1.6798328623041376\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4468, Accuracy: 324/637 (51%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.62      0.67        69\n",
            "           1       0.58      0.57      0.58        56\n",
            "          10       0.61      0.40      0.48        43\n",
            "          11       0.60      0.38      0.47        47\n",
            "          12       0.34      0.51      0.41        55\n",
            "          13       0.29      0.30      0.30        30\n",
            "          14       0.95      0.83      0.89        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.33      0.44      0.37        41\n",
            "           4       0.00      0.00      0.00        20\n",
            "           5       0.35      0.49      0.40        35\n",
            "           6       0.75      0.95      0.84        44\n",
            "           7       0.46      0.55      0.50        56\n",
            "           8       0.38      0.45      0.41        47\n",
            "           9       0.43      0.20      0.27        45\n",
            "\n",
            "    accuracy                           0.51       637\n",
            "   macro avg       0.45      0.45      0.44       637\n",
            "weighted avg       0.52      0.51      0.50       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 25 [320/3527 (9%)]\tLoss: 1.736797\tLR:0.000010454\n",
            "Train Epoch: 25 [640/3527 (18%)]\tLoss: 1.676369\tLR:0.000010454\n",
            "Train Epoch: 25 [960/3527 (27%)]\tLoss: 1.385225\tLR:0.000010454\n",
            "Train Epoch: 25 [1280/3527 (36%)]\tLoss: 1.419595\tLR:0.000010454\n",
            "Train Epoch: 25 [1600/3527 (45%)]\tLoss: 2.547368\tLR:0.000010454\n",
            "Train Epoch: 25 [1920/3527 (54%)]\tLoss: 2.394325\tLR:0.000010454\n",
            "Train Epoch: 25 [2240/3527 (63%)]\tLoss: 1.723379\tLR:0.000010454\n",
            "Train Epoch: 25 [2560/3527 (72%)]\tLoss: 1.821398\tLR:0.000010454\n",
            "Train Epoch: 25 [2880/3527 (81%)]\tLoss: 1.445318\tLR:0.000010454\n",
            "Train Epoch: 25 [3200/3527 (90%)]\tLoss: 1.430140\tLR:0.000010454\n",
            "Train Epoch: 25 [3520/3527 (99%)]\tLoss: 1.620528\tLR:0.000010454\n",
            "6\n",
            "epoch:25,loss:1.685047233426893\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4424, Accuracy: 330/637 (52%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.58      0.64        69\n",
            "           1       0.53      0.61      0.57        56\n",
            "          10       0.47      0.35      0.40        43\n",
            "          11       0.76      0.34      0.47        47\n",
            "          12       0.36      0.49      0.42        55\n",
            "          13       0.36      0.30      0.33        30\n",
            "          14       0.97      0.83      0.90        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.26      0.46      0.33        41\n",
            "           4       0.45      0.25      0.32        20\n",
            "           5       0.42      0.51      0.46        35\n",
            "           6       0.79      0.95      0.87        44\n",
            "           7       0.52      0.57      0.54        56\n",
            "           8       0.40      0.49      0.44        47\n",
            "           9       0.44      0.24      0.31        45\n",
            "\n",
            "    accuracy                           0.52       637\n",
            "   macro avg       0.50      0.47      0.47       637\n",
            "weighted avg       0.55      0.52      0.52       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 26 [320/3527 (9%)]\tLoss: 1.340994\tLR:0.000015498\n",
            "Train Epoch: 26 [640/3527 (18%)]\tLoss: 1.554908\tLR:0.000015498\n",
            "Train Epoch: 26 [960/3527 (27%)]\tLoss: 1.530886\tLR:0.000015498\n",
            "Train Epoch: 26 [1280/3527 (36%)]\tLoss: 1.517578\tLR:0.000015498\n",
            "Train Epoch: 26 [1600/3527 (45%)]\tLoss: 1.647052\tLR:0.000015498\n",
            "Train Epoch: 26 [1920/3527 (54%)]\tLoss: 1.517859\tLR:0.000015498\n",
            "Train Epoch: 26 [2240/3527 (63%)]\tLoss: 1.909639\tLR:0.000015498\n",
            "Train Epoch: 26 [2560/3527 (72%)]\tLoss: 1.633224\tLR:0.000015498\n",
            "Train Epoch: 26 [2880/3527 (81%)]\tLoss: 1.535024\tLR:0.000015498\n",
            "Train Epoch: 26 [3200/3527 (90%)]\tLoss: 1.309681\tLR:0.000015498\n",
            "Train Epoch: 26 [3520/3527 (99%)]\tLoss: 1.563680\tLR:0.000015498\n",
            "6\n",
            "epoch:26,loss:1.6609964574779477\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4456, Accuracy: 324/637 (51%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.67      0.68        69\n",
            "           1       0.62      0.55      0.58        56\n",
            "          10       0.60      0.35      0.44        43\n",
            "          11       0.63      0.40      0.49        47\n",
            "          12       0.32      0.51      0.39        55\n",
            "          13       0.45      0.17      0.24        30\n",
            "          14       0.97      0.83      0.90        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.24      0.41      0.30        41\n",
            "           4       0.28      0.25      0.26        20\n",
            "           5       0.42      0.49      0.45        35\n",
            "           6       0.81      0.98      0.89        44\n",
            "           7       0.45      0.48      0.47        56\n",
            "           8       0.36      0.47      0.41        47\n",
            "           9       0.42      0.22      0.29        45\n",
            "\n",
            "    accuracy                           0.51       637\n",
            "   macro avg       0.49      0.45      0.45       637\n",
            "weighted avg       0.54      0.51      0.51       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 27 [320/3527 (9%)]\tLoss: 1.873685\tLR:0.000021405\n",
            "Train Epoch: 27 [640/3527 (18%)]\tLoss: 1.608990\tLR:0.000021405\n",
            "Train Epoch: 27 [960/3527 (27%)]\tLoss: 1.567056\tLR:0.000021405\n",
            "Train Epoch: 27 [1280/3527 (36%)]\tLoss: 1.876716\tLR:0.000021405\n",
            "Train Epoch: 27 [1600/3527 (45%)]\tLoss: 1.478287\tLR:0.000021405\n",
            "Train Epoch: 27 [1920/3527 (54%)]\tLoss: 1.535622\tLR:0.000021405\n",
            "Train Epoch: 27 [2240/3527 (63%)]\tLoss: 1.663101\tLR:0.000021405\n",
            "Train Epoch: 27 [2560/3527 (72%)]\tLoss: 1.772735\tLR:0.000021405\n",
            "Train Epoch: 27 [2880/3527 (81%)]\tLoss: 1.507557\tLR:0.000021405\n",
            "Train Epoch: 27 [3200/3527 (90%)]\tLoss: 1.724919\tLR:0.000021405\n",
            "Train Epoch: 27 [3520/3527 (99%)]\tLoss: 1.634813\tLR:0.000021405\n",
            "6\n",
            "epoch:27,loss:1.7407967013281744\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4308, Accuracy: 336/637 (53%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.59      0.67        69\n",
            "           1       0.55      0.59      0.57        56\n",
            "          10       0.62      0.42      0.50        43\n",
            "          11       0.70      0.49      0.57        47\n",
            "          12       0.34      0.45      0.39        55\n",
            "          13       0.41      0.30      0.35        30\n",
            "          14       0.97      0.83      0.90        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.26      0.44      0.33        41\n",
            "           4       0.39      0.45      0.42        20\n",
            "           5       0.44      0.51      0.47        35\n",
            "           6       0.79      0.95      0.87        44\n",
            "           7       0.45      0.59      0.51        56\n",
            "           8       0.37      0.40      0.39        47\n",
            "           9       0.60      0.20      0.30        45\n",
            "\n",
            "    accuracy                           0.53       637\n",
            "   macro avg       0.51      0.48      0.48       637\n",
            "weighted avg       0.56      0.53      0.53       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 28 [320/3527 (9%)]\tLoss: 1.607331\tLR:0.000028027\n",
            "Train Epoch: 28 [640/3527 (18%)]\tLoss: 1.518016\tLR:0.000028027\n",
            "Train Epoch: 28 [960/3527 (27%)]\tLoss: 2.188952\tLR:0.000028027\n",
            "Train Epoch: 28 [1280/3527 (36%)]\tLoss: 1.581849\tLR:0.000028027\n",
            "Train Epoch: 28 [1600/3527 (45%)]\tLoss: 1.455629\tLR:0.000028027\n",
            "Train Epoch: 28 [1920/3527 (54%)]\tLoss: 1.696895\tLR:0.000028027\n",
            "Train Epoch: 28 [2240/3527 (63%)]\tLoss: 1.844928\tLR:0.000028027\n",
            "Train Epoch: 28 [2560/3527 (72%)]\tLoss: 1.517934\tLR:0.000028027\n",
            "Train Epoch: 28 [2880/3527 (81%)]\tLoss: 1.677660\tLR:0.000028027\n",
            "Train Epoch: 28 [3200/3527 (90%)]\tLoss: 1.853434\tLR:0.000028027\n",
            "Train Epoch: 28 [3520/3527 (99%)]\tLoss: 1.609727\tLR:0.000028027\n",
            "6\n",
            "epoch:28,loss:1.692795723408192\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4329, Accuracy: 323/637 (51%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.51      0.58        69\n",
            "           1       0.46      0.66      0.54        56\n",
            "          10       0.58      0.35      0.43        43\n",
            "          11       0.69      0.38      0.49        47\n",
            "          12       0.36      0.51      0.42        55\n",
            "          13       0.44      0.27      0.33        30\n",
            "          14       0.98      0.85      0.91        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.29      0.34      0.31        41\n",
            "           4       0.35      0.40      0.37        20\n",
            "           5       0.34      0.51      0.41        35\n",
            "           6       0.78      0.91      0.84        44\n",
            "           7       0.44      0.59      0.50        56\n",
            "           8       0.45      0.43      0.44        47\n",
            "           9       0.41      0.20      0.27        45\n",
            "\n",
            "    accuracy                           0.51       637\n",
            "   macro avg       0.48      0.46      0.46       637\n",
            "weighted avg       0.53      0.51      0.50       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 29 [320/3527 (9%)]\tLoss: 1.437792\tLR:0.000035204\n",
            "Train Epoch: 29 [640/3527 (18%)]\tLoss: 1.475265\tLR:0.000035204\n",
            "Train Epoch: 29 [960/3527 (27%)]\tLoss: 1.660538\tLR:0.000035204\n",
            "Train Epoch: 29 [1280/3527 (36%)]\tLoss: 1.576285\tLR:0.000035204\n",
            "Train Epoch: 29 [1600/3527 (45%)]\tLoss: 1.997589\tLR:0.000035204\n",
            "Train Epoch: 29 [1920/3527 (54%)]\tLoss: 1.441335\tLR:0.000035204\n",
            "Train Epoch: 29 [2240/3527 (63%)]\tLoss: 1.659498\tLR:0.000035204\n",
            "Train Epoch: 29 [2560/3527 (72%)]\tLoss: 1.793714\tLR:0.000035204\n",
            "Train Epoch: 29 [2880/3527 (81%)]\tLoss: 1.864793\tLR:0.000035204\n",
            "Train Epoch: 29 [3200/3527 (90%)]\tLoss: 1.610876\tLR:0.000035204\n",
            "Train Epoch: 29 [3520/3527 (99%)]\tLoss: 1.706904\tLR:0.000035204\n",
            "6\n",
            "epoch:29,loss:1.679554109100823\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4007, Accuracy: 334/637 (52%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.65      0.67        69\n",
            "           1       0.61      0.62      0.62        56\n",
            "          10       0.55      0.37      0.44        43\n",
            "          11       0.70      0.40      0.51        47\n",
            "          12       0.40      0.53      0.45        55\n",
            "          13       0.33      0.43      0.37        30\n",
            "          14       0.93      0.85      0.89        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.32      0.34      0.33        41\n",
            "           4       0.33      0.25      0.29        20\n",
            "           5       0.38      0.54      0.45        35\n",
            "           6       0.75      0.95      0.84        44\n",
            "           7       0.41      0.52      0.46        56\n",
            "           8       0.42      0.40      0.41        47\n",
            "           9       0.43      0.20      0.27        45\n",
            "\n",
            "    accuracy                           0.52       637\n",
            "   macro avg       0.48      0.47      0.47       637\n",
            "weighted avg       0.54      0.52      0.52       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 30 [320/3527 (9%)]\tLoss: 1.717716\tLR:0.000042756\n",
            "Train Epoch: 30 [640/3527 (18%)]\tLoss: 1.610698\tLR:0.000042756\n",
            "Train Epoch: 30 [960/3527 (27%)]\tLoss: 1.571851\tLR:0.000042756\n",
            "Train Epoch: 30 [1280/3527 (36%)]\tLoss: 1.419915\tLR:0.000042756\n",
            "Train Epoch: 30 [1600/3527 (45%)]\tLoss: 1.556629\tLR:0.000042756\n",
            "Train Epoch: 30 [1920/3527 (54%)]\tLoss: 1.724636\tLR:0.000042756\n",
            "Train Epoch: 30 [2240/3527 (63%)]\tLoss: 1.645232\tLR:0.000042756\n",
            "Train Epoch: 30 [2560/3527 (72%)]\tLoss: 1.556299\tLR:0.000042756\n",
            "Train Epoch: 30 [2880/3527 (81%)]\tLoss: 1.936296\tLR:0.000042756\n",
            "Train Epoch: 30 [3200/3527 (90%)]\tLoss: 1.587169\tLR:0.000042756\n",
            "Train Epoch: 30 [3520/3527 (99%)]\tLoss: 1.653422\tLR:0.000042756\n",
            "6\n",
            "epoch:30,loss:1.666624837093525\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4047, Accuracy: 324/637 (51%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.51      0.60        69\n",
            "           1       0.56      0.62      0.59        56\n",
            "          10       0.67      0.33      0.44        43\n",
            "          11       0.49      0.55      0.52        47\n",
            "          12       0.32      0.42      0.36        55\n",
            "          13       0.47      0.23      0.31        30\n",
            "          14       0.98      0.85      0.91        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.33      0.34      0.33        41\n",
            "           4       0.50      0.45      0.47        20\n",
            "           5       0.39      0.46      0.42        35\n",
            "           6       0.70      0.95      0.81        44\n",
            "           7       0.39      0.52      0.44        56\n",
            "           8       0.36      0.43      0.39        47\n",
            "           9       0.47      0.31      0.37        45\n",
            "\n",
            "    accuracy                           0.51       637\n",
            "   macro avg       0.49      0.46      0.46       637\n",
            "weighted avg       0.53      0.51      0.51       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 31 [320/3527 (9%)]\tLoss: 1.286215\tLR:0.000050500\n",
            "Train Epoch: 31 [640/3527 (18%)]\tLoss: 1.371148\tLR:0.000050500\n",
            "Train Epoch: 31 [960/3527 (27%)]\tLoss: 1.686916\tLR:0.000050500\n",
            "Train Epoch: 31 [1280/3527 (36%)]\tLoss: 1.591310\tLR:0.000050500\n",
            "Train Epoch: 31 [1600/3527 (45%)]\tLoss: 1.838591\tLR:0.000050500\n",
            "Train Epoch: 31 [1920/3527 (54%)]\tLoss: 1.566031\tLR:0.000050500\n",
            "Train Epoch: 31 [2240/3527 (63%)]\tLoss: 1.441353\tLR:0.000050500\n",
            "Train Epoch: 31 [2560/3527 (72%)]\tLoss: 1.735293\tLR:0.000050500\n",
            "Train Epoch: 31 [2880/3527 (81%)]\tLoss: 1.553406\tLR:0.000050500\n",
            "Train Epoch: 31 [3200/3527 (90%)]\tLoss: 1.370854\tLR:0.000050500\n",
            "Train Epoch: 31 [3520/3527 (99%)]\tLoss: 1.326010\tLR:0.000050500\n",
            "6\n",
            "epoch:31,loss:1.6588044660585421\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4145, Accuracy: 336/637 (53%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.62      0.68        69\n",
            "           1       0.67      0.62      0.65        56\n",
            "          10       0.83      0.35      0.49        43\n",
            "          11       0.71      0.51      0.59        47\n",
            "          12       0.33      0.47      0.39        55\n",
            "          13       0.42      0.17      0.24        30\n",
            "          14       0.98      0.85      0.91        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.21      0.49      0.29        41\n",
            "           4       0.50      0.20      0.29        20\n",
            "           5       0.40      0.49      0.44        35\n",
            "           6       0.75      0.89      0.81        44\n",
            "           7       0.48      0.50      0.49        56\n",
            "           8       0.47      0.47      0.47        47\n",
            "           9       0.43      0.40      0.41        45\n",
            "\n",
            "    accuracy                           0.53       637\n",
            "   macro avg       0.53      0.47      0.48       637\n",
            "weighted avg       0.58      0.53      0.53       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 32 [320/3527 (9%)]\tLoss: 1.595783\tLR:0.000058244\n",
            "Train Epoch: 32 [640/3527 (18%)]\tLoss: 1.523727\tLR:0.000058244\n",
            "Train Epoch: 32 [960/3527 (27%)]\tLoss: 1.718698\tLR:0.000058244\n",
            "Train Epoch: 32 [1280/3527 (36%)]\tLoss: 1.696776\tLR:0.000058244\n",
            "Train Epoch: 32 [1600/3527 (45%)]\tLoss: 1.617818\tLR:0.000058244\n",
            "Train Epoch: 32 [1920/3527 (54%)]\tLoss: 1.514770\tLR:0.000058244\n",
            "Train Epoch: 32 [2240/3527 (63%)]\tLoss: 1.270182\tLR:0.000058244\n",
            "Train Epoch: 32 [2560/3527 (72%)]\tLoss: 1.621422\tLR:0.000058244\n",
            "Train Epoch: 32 [2880/3527 (81%)]\tLoss: 1.728184\tLR:0.000058244\n",
            "Train Epoch: 32 [3200/3527 (90%)]\tLoss: 1.620696\tLR:0.000058244\n",
            "Train Epoch: 32 [3520/3527 (99%)]\tLoss: 1.819147\tLR:0.000058244\n",
            "6\n",
            "epoch:32,loss:1.638801897968258\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4057, Accuracy: 340/637 (53%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.67      0.68        69\n",
            "           1       0.62      0.52      0.56        56\n",
            "          10       0.64      0.33      0.43        43\n",
            "          11       0.79      0.47      0.59        47\n",
            "          12       0.32      0.58      0.41        55\n",
            "          13       0.39      0.23      0.29        30\n",
            "          14       0.91      0.89      0.90        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.30      0.56      0.39        41\n",
            "           4       0.29      0.25      0.27        20\n",
            "           5       0.62      0.51      0.56        35\n",
            "           6       0.76      0.86      0.81        44\n",
            "           7       0.47      0.59      0.52        56\n",
            "           8       0.57      0.34      0.43        47\n",
            "           9       0.39      0.33      0.36        45\n",
            "\n",
            "    accuracy                           0.53       637\n",
            "   macro avg       0.52      0.48      0.48       637\n",
            "weighted avg       0.57      0.53      0.53       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 33 [320/3527 (9%)]\tLoss: 1.442280\tLR:0.000065796\n",
            "Train Epoch: 33 [640/3527 (18%)]\tLoss: 1.598174\tLR:0.000065796\n",
            "Train Epoch: 33 [960/3527 (27%)]\tLoss: 1.626692\tLR:0.000065796\n",
            "Train Epoch: 33 [1280/3527 (36%)]\tLoss: 1.381702\tLR:0.000065796\n",
            "Train Epoch: 33 [1600/3527 (45%)]\tLoss: 1.739810\tLR:0.000065796\n",
            "Train Epoch: 33 [1920/3527 (54%)]\tLoss: 1.699377\tLR:0.000065796\n",
            "Train Epoch: 33 [2240/3527 (63%)]\tLoss: 2.016320\tLR:0.000065796\n",
            "Train Epoch: 33 [2560/3527 (72%)]\tLoss: 1.890710\tLR:0.000065796\n",
            "Train Epoch: 33 [2880/3527 (81%)]\tLoss: 1.483232\tLR:0.000065796\n",
            "Train Epoch: 33 [3200/3527 (90%)]\tLoss: 1.605466\tLR:0.000065796\n",
            "Train Epoch: 33 [3520/3527 (99%)]\tLoss: 1.605841\tLR:0.000065796\n",
            "6\n",
            "epoch:33,loss:1.5916175960420489\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.3374, Accuracy: 355/637 (56%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.52      0.62        69\n",
            "           1       0.51      0.71      0.60        56\n",
            "          10       0.62      0.49      0.55        43\n",
            "          11       0.62      0.60      0.61        47\n",
            "          12       0.35      0.67      0.46        55\n",
            "          13       0.75      0.10      0.18        30\n",
            "          14       0.95      0.87      0.91        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.38      0.39      0.39        41\n",
            "           4       0.38      0.65      0.48        20\n",
            "           5       0.53      0.46      0.49        35\n",
            "           6       0.78      0.91      0.84        44\n",
            "           7       0.58      0.38      0.46        56\n",
            "           8       0.46      0.57      0.51        47\n",
            "           9       0.59      0.36      0.44        45\n",
            "\n",
            "    accuracy                           0.56       637\n",
            "   macro avg       0.55      0.51      0.50       637\n",
            "weighted avg       0.60      0.56      0.55       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 34 [320/3527 (9%)]\tLoss: 1.610374\tLR:0.000072973\n",
            "Train Epoch: 34 [640/3527 (18%)]\tLoss: 1.774531\tLR:0.000072973\n",
            "Train Epoch: 34 [960/3527 (27%)]\tLoss: 1.675908\tLR:0.000072973\n",
            "Train Epoch: 34 [1280/3527 (36%)]\tLoss: 1.731207\tLR:0.000072973\n",
            "Train Epoch: 34 [1600/3527 (45%)]\tLoss: 1.638821\tLR:0.000072973\n",
            "Train Epoch: 34 [1920/3527 (54%)]\tLoss: 1.430654\tLR:0.000072973\n",
            "Train Epoch: 34 [2240/3527 (63%)]\tLoss: 1.561415\tLR:0.000072973\n",
            "Train Epoch: 34 [2560/3527 (72%)]\tLoss: 1.591911\tLR:0.000072973\n",
            "Train Epoch: 34 [2880/3527 (81%)]\tLoss: 1.887070\tLR:0.000072973\n",
            "Train Epoch: 34 [3200/3527 (90%)]\tLoss: 1.544074\tLR:0.000072973\n",
            "Train Epoch: 34 [3520/3527 (99%)]\tLoss: 1.318099\tLR:0.000072973\n",
            "6\n",
            "epoch:34,loss:1.604616312293319\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.3200, Accuracy: 357/637 (56%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.75      0.67        69\n",
            "           1       0.61      0.39      0.48        56\n",
            "          10       0.63      0.44      0.52        43\n",
            "          11       0.80      0.60      0.68        47\n",
            "          12       0.35      0.42      0.38        55\n",
            "          13       0.38      0.50      0.43        30\n",
            "          14       0.93      0.91      0.92        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.32      0.59      0.41        41\n",
            "           4       0.25      0.05      0.08        20\n",
            "           5       0.40      0.57      0.47        35\n",
            "           6       0.95      0.86      0.90        44\n",
            "           7       0.70      0.57      0.63        56\n",
            "           8       0.47      0.53      0.50        47\n",
            "           9       0.52      0.33      0.41        45\n",
            "\n",
            "    accuracy                           0.56       637\n",
            "   macro avg       0.53      0.50      0.50       637\n",
            "weighted avg       0.59      0.56      0.56       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 35 [320/3527 (9%)]\tLoss: 1.467353\tLR:0.000079595\n",
            "Train Epoch: 35 [640/3527 (18%)]\tLoss: 1.443962\tLR:0.000079595\n",
            "Train Epoch: 35 [960/3527 (27%)]\tLoss: 1.928535\tLR:0.000079595\n",
            "Train Epoch: 35 [1280/3527 (36%)]\tLoss: 1.406263\tLR:0.000079595\n",
            "Train Epoch: 35 [1600/3527 (45%)]\tLoss: 1.356053\tLR:0.000079595\n",
            "Train Epoch: 35 [1920/3527 (54%)]\tLoss: 1.455670\tLR:0.000079595\n",
            "Train Epoch: 35 [2240/3527 (63%)]\tLoss: 2.050100\tLR:0.000079595\n",
            "Train Epoch: 35 [2560/3527 (72%)]\tLoss: 1.447604\tLR:0.000079595\n",
            "Train Epoch: 35 [2880/3527 (81%)]\tLoss: 1.504095\tLR:0.000079595\n",
            "Train Epoch: 35 [3200/3527 (90%)]\tLoss: 1.374746\tLR:0.000079595\n",
            "Train Epoch: 35 [3520/3527 (99%)]\tLoss: 2.120970\tLR:0.000079595\n",
            "6\n",
            "epoch:35,loss:1.5732889486862733\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.4274, Accuracy: 343/637 (54%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.51      0.60        69\n",
            "           1       0.60      0.52      0.56        56\n",
            "          10       0.62      0.53      0.57        43\n",
            "          11       0.61      0.64      0.62        47\n",
            "          12       0.32      0.45      0.37        55\n",
            "          13       0.50      0.30      0.37        30\n",
            "          14       0.93      0.87      0.90        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.31      0.54      0.39        41\n",
            "           4       0.29      0.30      0.29        20\n",
            "           5       0.51      0.51      0.51        35\n",
            "           6       0.83      0.86      0.84        44\n",
            "           7       0.54      0.50      0.52        56\n",
            "           8       0.39      0.62      0.48        47\n",
            "           9       0.67      0.22      0.33        45\n",
            "\n",
            "    accuracy                           0.54       637\n",
            "   macro avg       0.52      0.49      0.49       637\n",
            "weighted avg       0.58      0.54      0.54       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 36 [320/3527 (9%)]\tLoss: 1.804822\tLR:0.000085502\n",
            "Train Epoch: 36 [640/3527 (18%)]\tLoss: 1.433293\tLR:0.000085502\n",
            "Train Epoch: 36 [960/3527 (27%)]\tLoss: 1.621202\tLR:0.000085502\n",
            "Train Epoch: 36 [1280/3527 (36%)]\tLoss: 1.611930\tLR:0.000085502\n",
            "Train Epoch: 36 [1600/3527 (45%)]\tLoss: 1.570068\tLR:0.000085502\n",
            "Train Epoch: 36 [1920/3527 (54%)]\tLoss: 1.516882\tLR:0.000085502\n",
            "Train Epoch: 36 [2240/3527 (63%)]\tLoss: 1.471545\tLR:0.000085502\n",
            "Train Epoch: 36 [2560/3527 (72%)]\tLoss: 1.490360\tLR:0.000085502\n",
            "Train Epoch: 36 [2880/3527 (81%)]\tLoss: 1.519106\tLR:0.000085502\n",
            "Train Epoch: 36 [3200/3527 (90%)]\tLoss: 1.368677\tLR:0.000085502\n",
            "Train Epoch: 36 [3520/3527 (99%)]\tLoss: 1.529253\tLR:0.000085502\n",
            "6\n",
            "epoch:36,loss:1.6038817167282104\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.3187, Accuracy: 354/637 (56%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.55      0.64        69\n",
            "           1       0.59      0.64      0.62        56\n",
            "          10       0.72      0.42      0.53        43\n",
            "          11       0.62      0.66      0.64        47\n",
            "          12       0.39      0.49      0.43        55\n",
            "          13       0.50      0.37      0.42        30\n",
            "          14       0.90      0.91      0.91        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.35      0.54      0.42        41\n",
            "           4       0.44      0.35      0.39        20\n",
            "           5       0.39      0.31      0.35        35\n",
            "           6       0.84      0.86      0.85        44\n",
            "           7       0.56      0.43      0.48        56\n",
            "           8       0.48      0.47      0.47        47\n",
            "           9       0.37      0.58      0.45        45\n",
            "\n",
            "    accuracy                           0.56       637\n",
            "   macro avg       0.53      0.51      0.51       637\n",
            "weighted avg       0.58      0.56      0.56       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 37 [320/3527 (9%)]\tLoss: 1.375055\tLR:0.000090546\n",
            "Train Epoch: 37 [640/3527 (18%)]\tLoss: 1.587939\tLR:0.000090546\n",
            "Train Epoch: 37 [960/3527 (27%)]\tLoss: 1.801696\tLR:0.000090546\n",
            "Train Epoch: 37 [1280/3527 (36%)]\tLoss: 1.734243\tLR:0.000090546\n",
            "Train Epoch: 37 [1600/3527 (45%)]\tLoss: 1.683873\tLR:0.000090546\n",
            "Train Epoch: 37 [1920/3527 (54%)]\tLoss: 1.342604\tLR:0.000090546\n",
            "Train Epoch: 37 [2240/3527 (63%)]\tLoss: 1.516119\tLR:0.000090546\n",
            "Train Epoch: 37 [2560/3527 (72%)]\tLoss: 1.549772\tLR:0.000090546\n",
            "Train Epoch: 37 [2880/3527 (81%)]\tLoss: 1.366019\tLR:0.000090546\n",
            "Train Epoch: 37 [3200/3527 (90%)]\tLoss: 1.398800\tLR:0.000090546\n",
            "Train Epoch: 37 [3520/3527 (99%)]\tLoss: 1.466902\tLR:0.000090546\n",
            "6\n",
            "epoch:37,loss:1.530320194390443\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.3177, Accuracy: 363/637 (57%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.51      0.58        69\n",
            "           1       0.56      0.50      0.53        56\n",
            "          10       0.64      0.58      0.61        43\n",
            "          11       0.77      0.64      0.70        47\n",
            "          12       0.36      0.47      0.41        55\n",
            "          13       0.27      0.33      0.30        30\n",
            "          14       1.00      0.85      0.92        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.41      0.51      0.46        41\n",
            "           4       0.40      0.60      0.48        20\n",
            "           5       0.56      0.40      0.47        35\n",
            "           6       0.95      0.89      0.92        44\n",
            "           7       0.48      0.75      0.59        56\n",
            "           8       0.46      0.49      0.47        47\n",
            "           9       0.72      0.40      0.51        45\n",
            "\n",
            "    accuracy                           0.57       637\n",
            "   macro avg       0.55      0.53      0.53       637\n",
            "weighted avg       0.60      0.57      0.58       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 38 [320/3527 (9%)]\tLoss: 1.710318\tLR:0.000094605\n",
            "Train Epoch: 38 [640/3527 (18%)]\tLoss: 1.614706\tLR:0.000094605\n",
            "Train Epoch: 38 [960/3527 (27%)]\tLoss: 1.601189\tLR:0.000094605\n",
            "Train Epoch: 38 [1280/3527 (36%)]\tLoss: 1.414276\tLR:0.000094605\n",
            "Train Epoch: 38 [1600/3527 (45%)]\tLoss: 1.337513\tLR:0.000094605\n",
            "Train Epoch: 38 [1920/3527 (54%)]\tLoss: 1.510893\tLR:0.000094605\n",
            "Train Epoch: 38 [2240/3527 (63%)]\tLoss: 1.549686\tLR:0.000094605\n",
            "Train Epoch: 38 [2560/3527 (72%)]\tLoss: 1.464486\tLR:0.000094605\n",
            "Train Epoch: 38 [2880/3527 (81%)]\tLoss: 1.514339\tLR:0.000094605\n",
            "Train Epoch: 38 [3200/3527 (90%)]\tLoss: 1.238605\tLR:0.000094605\n",
            "Train Epoch: 38 [3520/3527 (99%)]\tLoss: 1.362870\tLR:0.000094605\n",
            "6\n",
            "epoch:38,loss:1.5190532497457556\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.2641, Accuracy: 384/637 (60%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.67      0.67        69\n",
            "           1       0.59      0.62      0.61        56\n",
            "          10       0.75      0.35      0.48        43\n",
            "          11       0.61      0.70      0.65        47\n",
            "          12       0.44      0.75      0.55        55\n",
            "          13       0.58      0.47      0.52        30\n",
            "          14       0.96      0.91      0.93        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.40      0.66      0.50        41\n",
            "           4       0.73      0.40      0.52        20\n",
            "           5       0.50      0.40      0.44        35\n",
            "           6       0.92      0.77      0.84        44\n",
            "           7       0.55      0.70      0.61        56\n",
            "           8       0.53      0.36      0.43        47\n",
            "           9       0.67      0.40      0.50        45\n",
            "\n",
            "    accuracy                           0.60       637\n",
            "   macro avg       0.59      0.54      0.55       637\n",
            "weighted avg       0.63      0.60      0.60       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 39 [320/3527 (9%)]\tLoss: 1.544861\tLR:0.000097577\n",
            "Train Epoch: 39 [640/3527 (18%)]\tLoss: 1.368023\tLR:0.000097577\n",
            "Train Epoch: 39 [960/3527 (27%)]\tLoss: 1.457023\tLR:0.000097577\n",
            "Train Epoch: 39 [1280/3527 (36%)]\tLoss: 1.627587\tLR:0.000097577\n",
            "Train Epoch: 39 [1600/3527 (45%)]\tLoss: 1.282438\tLR:0.000097577\n",
            "Train Epoch: 39 [1920/3527 (54%)]\tLoss: 1.285873\tLR:0.000097577\n",
            "Train Epoch: 39 [2240/3527 (63%)]\tLoss: 1.505359\tLR:0.000097577\n",
            "Train Epoch: 39 [2560/3527 (72%)]\tLoss: 1.208158\tLR:0.000097577\n",
            "Train Epoch: 39 [2880/3527 (81%)]\tLoss: 1.297012\tLR:0.000097577\n",
            "Train Epoch: 39 [3200/3527 (90%)]\tLoss: 1.359599\tLR:0.000097577\n",
            "Train Epoch: 39 [3520/3527 (99%)]\tLoss: 1.533711\tLR:0.000097577\n",
            "6\n",
            "epoch:39,loss:1.4509455491830636\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.3050, Accuracy: 368/637 (58%)\n",
            "\n",
            "Training complete in 0m 40s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.64      0.68        69\n",
            "           1       0.67      0.61      0.64        56\n",
            "          10       0.43      0.51      0.47        43\n",
            "          11       0.69      0.66      0.67        47\n",
            "          12       0.42      0.51      0.46        55\n",
            "          13       0.50      0.33      0.40        30\n",
            "          14       0.91      0.91      0.91        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.39      0.59      0.47        41\n",
            "           4       0.62      0.25      0.36        20\n",
            "           5       0.40      0.54      0.46        35\n",
            "           6       0.81      0.86      0.84        44\n",
            "           7       0.49      0.66      0.56        56\n",
            "           8       0.44      0.36      0.40        47\n",
            "           9       0.89      0.36      0.51        45\n",
            "\n",
            "    accuracy                           0.58       637\n",
            "   macro avg       0.56      0.52      0.52       637\n",
            "weighted avg       0.61      0.58      0.58       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 40 [320/3527 (9%)]\tLoss: 1.653122\tLR:0.000099391\n",
            "Train Epoch: 40 [640/3527 (18%)]\tLoss: 1.289855\tLR:0.000099391\n",
            "Train Epoch: 40 [960/3527 (27%)]\tLoss: 1.230956\tLR:0.000099391\n",
            "Train Epoch: 40 [1280/3527 (36%)]\tLoss: 1.488938\tLR:0.000099391\n",
            "Train Epoch: 40 [1600/3527 (45%)]\tLoss: 3.066729\tLR:0.000099391\n",
            "Train Epoch: 40 [1920/3527 (54%)]\tLoss: 1.243315\tLR:0.000099391\n",
            "Train Epoch: 40 [2240/3527 (63%)]\tLoss: 1.418969\tLR:0.000099391\n",
            "Train Epoch: 40 [2560/3527 (72%)]\tLoss: 1.444593\tLR:0.000099391\n",
            "Train Epoch: 40 [2880/3527 (81%)]\tLoss: 1.418409\tLR:0.000099391\n",
            "Train Epoch: 40 [3200/3527 (90%)]\tLoss: 1.298561\tLR:0.000099391\n",
            "Train Epoch: 40 [3520/3527 (99%)]\tLoss: 1.266369\tLR:0.000099391\n",
            "6\n",
            "epoch:40,loss:1.4443977007994782\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.2330, Accuracy: 384/637 (60%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.81      0.70        69\n",
            "           1       0.84      0.38      0.52        56\n",
            "          10       0.92      0.51      0.66        43\n",
            "          11       0.66      0.79      0.72        47\n",
            "          12       0.44      0.73      0.55        55\n",
            "          13       0.48      0.40      0.44        30\n",
            "          14       0.88      0.89      0.88        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.93      0.34      0.50        41\n",
            "           4       0.42      0.65      0.51        20\n",
            "           5       0.48      0.71      0.57        35\n",
            "           6       0.73      0.93      0.82        44\n",
            "           7       0.43      0.52      0.47        56\n",
            "           8       0.51      0.45      0.48        47\n",
            "           9       0.65      0.24      0.35        45\n",
            "\n",
            "    accuracy                           0.60       637\n",
            "   macro avg       0.60      0.56      0.55       637\n",
            "weighted avg       0.65      0.60      0.59       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 41 [320/3527 (9%)]\tLoss: 1.497215\tLR:0.000100000\n",
            "Train Epoch: 41 [640/3527 (18%)]\tLoss: 1.811898\tLR:0.000100000\n",
            "Train Epoch: 41 [960/3527 (27%)]\tLoss: 1.604814\tLR:0.000100000\n",
            "Train Epoch: 41 [1280/3527 (36%)]\tLoss: 1.321774\tLR:0.000100000\n",
            "Train Epoch: 41 [1600/3527 (45%)]\tLoss: 1.292788\tLR:0.000100000\n",
            "Train Epoch: 41 [1920/3527 (54%)]\tLoss: 1.334126\tLR:0.000100000\n",
            "Train Epoch: 41 [2240/3527 (63%)]\tLoss: 1.178598\tLR:0.000100000\n",
            "Train Epoch: 41 [2560/3527 (72%)]\tLoss: 1.355888\tLR:0.000100000\n",
            "Train Epoch: 41 [2880/3527 (81%)]\tLoss: 1.650478\tLR:0.000100000\n",
            "Train Epoch: 41 [3200/3527 (90%)]\tLoss: 1.352409\tLR:0.000100000\n",
            "Train Epoch: 41 [3520/3527 (99%)]\tLoss: 1.299195\tLR:0.000100000\n",
            "6\n",
            "epoch:41,loss:1.414373019794086\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.2187, Accuracy: 378/637 (59%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.59      0.65        69\n",
            "           1       0.64      0.57      0.60        56\n",
            "          10       0.75      0.49      0.59        43\n",
            "          11       0.68      0.64      0.66        47\n",
            "          12       0.38      0.67      0.49        55\n",
            "          13       0.46      0.20      0.28        30\n",
            "          14       0.96      0.91      0.93        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.36      0.54      0.43        41\n",
            "           4       0.42      0.65      0.51        20\n",
            "           5       0.50      0.54      0.52        35\n",
            "           6       0.87      0.91      0.89        44\n",
            "           7       0.67      0.59      0.63        56\n",
            "           8       0.45      0.43      0.44        47\n",
            "           9       0.62      0.47      0.53        45\n",
            "\n",
            "    accuracy                           0.59       637\n",
            "   macro avg       0.57      0.55      0.54       637\n",
            "weighted avg       0.62      0.59      0.60       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 42 [320/3527 (9%)]\tLoss: 1.225749\tLR:0.000099391\n",
            "Train Epoch: 42 [640/3527 (18%)]\tLoss: 1.421149\tLR:0.000099391\n",
            "Train Epoch: 42 [960/3527 (27%)]\tLoss: 1.722894\tLR:0.000099391\n",
            "Train Epoch: 42 [1280/3527 (36%)]\tLoss: 1.421402\tLR:0.000099391\n",
            "Train Epoch: 42 [1600/3527 (45%)]\tLoss: 1.286860\tLR:0.000099391\n",
            "Train Epoch: 42 [1920/3527 (54%)]\tLoss: 1.396928\tLR:0.000099391\n",
            "Train Epoch: 42 [2240/3527 (63%)]\tLoss: 2.444128\tLR:0.000099391\n",
            "Train Epoch: 42 [2560/3527 (72%)]\tLoss: 1.259722\tLR:0.000099391\n",
            "Train Epoch: 42 [2880/3527 (81%)]\tLoss: 1.192181\tLR:0.000099391\n",
            "Train Epoch: 42 [3200/3527 (90%)]\tLoss: 2.196665\tLR:0.000099391\n",
            "Train Epoch: 42 [3520/3527 (99%)]\tLoss: 1.350008\tLR:0.000099391\n",
            "6\n",
            "epoch:42,loss:1.383155178379368\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.1605, Accuracy: 403/637 (63%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.59      0.68        69\n",
            "           1       0.64      0.64      0.64        56\n",
            "          10       0.65      0.51      0.57        43\n",
            "          11       0.76      0.66      0.70        47\n",
            "          12       0.45      0.67      0.54        55\n",
            "          13       0.41      0.53      0.46        30\n",
            "          14       0.88      0.96      0.92        47\n",
            "           2       1.00      0.50      0.67         2\n",
            "           3       0.54      0.63      0.58        41\n",
            "           4       0.65      0.65      0.65        20\n",
            "           5       0.59      0.46      0.52        35\n",
            "           6       0.89      0.93      0.91        44\n",
            "           7       0.69      0.59      0.63        56\n",
            "           8       0.45      0.43      0.44        47\n",
            "           9       0.52      0.56      0.54        45\n",
            "\n",
            "    accuracy                           0.63       637\n",
            "   macro avg       0.66      0.62      0.63       637\n",
            "weighted avg       0.65      0.63      0.63       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 43 [320/3527 (9%)]\tLoss: 1.303337\tLR:0.000097577\n",
            "Train Epoch: 43 [640/3527 (18%)]\tLoss: 1.213326\tLR:0.000097577\n",
            "Train Epoch: 43 [960/3527 (27%)]\tLoss: 1.276046\tLR:0.000097577\n",
            "Train Epoch: 43 [1280/3527 (36%)]\tLoss: 1.267376\tLR:0.000097577\n",
            "Train Epoch: 43 [1600/3527 (45%)]\tLoss: 1.275931\tLR:0.000097577\n",
            "Train Epoch: 43 [1920/3527 (54%)]\tLoss: 1.019177\tLR:0.000097577\n",
            "Train Epoch: 43 [2240/3527 (63%)]\tLoss: 1.845424\tLR:0.000097577\n",
            "Train Epoch: 43 [2560/3527 (72%)]\tLoss: 1.040902\tLR:0.000097577\n",
            "Train Epoch: 43 [2880/3527 (81%)]\tLoss: 1.257602\tLR:0.000097577\n",
            "Train Epoch: 43 [3200/3527 (90%)]\tLoss: 1.677387\tLR:0.000097577\n",
            "Train Epoch: 43 [3520/3527 (99%)]\tLoss: 1.527802\tLR:0.000097577\n",
            "6\n",
            "epoch:43,loss:1.3342747392955128\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.1242, Accuracy: 409/637 (64%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.54      0.66        69\n",
            "           1       0.56      0.70      0.62        56\n",
            "          10       0.69      0.63      0.66        43\n",
            "          11       0.63      0.79      0.70        47\n",
            "          12       0.53      0.58      0.56        55\n",
            "          13       0.47      0.27      0.34        30\n",
            "          14       0.86      0.94      0.90        47\n",
            "           2       1.00      0.50      0.67         2\n",
            "           3       0.44      0.54      0.48        41\n",
            "           4       0.48      0.70      0.57        20\n",
            "           5       0.55      0.46      0.50        35\n",
            "           6       0.98      0.91      0.94        44\n",
            "           7       0.67      0.82      0.74        56\n",
            "           8       0.49      0.43      0.45        47\n",
            "           9       0.68      0.58      0.63        45\n",
            "\n",
            "    accuracy                           0.64       637\n",
            "   macro avg       0.66      0.62      0.63       637\n",
            "weighted avg       0.65      0.64      0.64       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 44 [320/3527 (9%)]\tLoss: 1.367626\tLR:0.000094605\n",
            "Train Epoch: 44 [640/3527 (18%)]\tLoss: 1.717546\tLR:0.000094605\n",
            "Train Epoch: 44 [960/3527 (27%)]\tLoss: 1.165657\tLR:0.000094605\n",
            "Train Epoch: 44 [1280/3527 (36%)]\tLoss: 1.177308\tLR:0.000094605\n",
            "Train Epoch: 44 [1600/3527 (45%)]\tLoss: 0.996308\tLR:0.000094605\n",
            "Train Epoch: 44 [1920/3527 (54%)]\tLoss: 1.337062\tLR:0.000094605\n",
            "Train Epoch: 44 [2240/3527 (63%)]\tLoss: 1.251161\tLR:0.000094605\n",
            "Train Epoch: 44 [2560/3527 (72%)]\tLoss: 1.114839\tLR:0.000094605\n",
            "Train Epoch: 44 [2880/3527 (81%)]\tLoss: 1.339497\tLR:0.000094605\n",
            "Train Epoch: 44 [3200/3527 (90%)]\tLoss: 1.246869\tLR:0.000094605\n",
            "Train Epoch: 44 [3520/3527 (99%)]\tLoss: 1.058414\tLR:0.000094605\n",
            "6\n",
            "epoch:44,loss:1.3009819195077226\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.1085, Accuracy: 417/637 (65%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.59      0.65        69\n",
            "           1       0.61      0.55      0.58        56\n",
            "          10       0.80      0.56      0.66        43\n",
            "          11       0.76      0.72      0.74        47\n",
            "          12       0.44      0.69      0.54        55\n",
            "          13       0.50      0.37      0.42        30\n",
            "          14       0.90      0.94      0.92        47\n",
            "           2       1.00      0.50      0.67         2\n",
            "           3       0.53      0.68      0.60        41\n",
            "           4       0.47      0.70      0.56        20\n",
            "           5       0.70      0.54      0.61        35\n",
            "           6       0.98      0.91      0.94        44\n",
            "           7       0.76      0.70      0.73        56\n",
            "           8       0.48      0.51      0.49        47\n",
            "           9       0.67      0.64      0.66        45\n",
            "\n",
            "    accuracy                           0.65       637\n",
            "   macro avg       0.69      0.64      0.65       637\n",
            "weighted avg       0.68      0.65      0.66       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 45 [320/3527 (9%)]\tLoss: 1.145559\tLR:0.000090546\n",
            "Train Epoch: 45 [640/3527 (18%)]\tLoss: 1.147282\tLR:0.000090546\n",
            "Train Epoch: 45 [960/3527 (27%)]\tLoss: 1.226273\tLR:0.000090546\n",
            "Train Epoch: 45 [1280/3527 (36%)]\tLoss: 1.230566\tLR:0.000090546\n",
            "Train Epoch: 45 [1600/3527 (45%)]\tLoss: 1.037514\tLR:0.000090546\n",
            "Train Epoch: 45 [1920/3527 (54%)]\tLoss: 1.333603\tLR:0.000090546\n",
            "Train Epoch: 45 [2240/3527 (63%)]\tLoss: 1.058196\tLR:0.000090546\n",
            "Train Epoch: 45 [2560/3527 (72%)]\tLoss: 1.209460\tLR:0.000090546\n",
            "Train Epoch: 45 [2880/3527 (81%)]\tLoss: 1.184379\tLR:0.000090546\n",
            "Train Epoch: 45 [3200/3527 (90%)]\tLoss: 1.329005\tLR:0.000090546\n",
            "Train Epoch: 45 [3520/3527 (99%)]\tLoss: 1.305981\tLR:0.000090546\n",
            "6\n",
            "epoch:45,loss:1.3036015237773861\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.0804, Accuracy: 429/637 (67%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.70      0.72        69\n",
            "           1       0.68      0.57      0.62        56\n",
            "          10       0.79      0.70      0.74        43\n",
            "          11       0.93      0.60      0.73        47\n",
            "          12       0.55      0.60      0.57        55\n",
            "          13       0.53      0.53      0.53        30\n",
            "          14       0.86      0.94      0.90        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.65      0.59      0.62        41\n",
            "           4       0.47      0.75      0.58        20\n",
            "           5       0.62      0.66      0.64        35\n",
            "           6       0.93      0.98      0.96        44\n",
            "           7       0.65      0.77      0.70        56\n",
            "           8       0.54      0.55      0.55        47\n",
            "           9       0.48      0.53      0.51        45\n",
            "\n",
            "    accuracy                           0.67       637\n",
            "   macro avg       0.63      0.63      0.62       637\n",
            "weighted avg       0.68      0.67      0.67       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 46 [320/3527 (9%)]\tLoss: 1.213000\tLR:0.000085502\n",
            "Train Epoch: 46 [640/3527 (18%)]\tLoss: 0.942998\tLR:0.000085502\n",
            "Train Epoch: 46 [960/3527 (27%)]\tLoss: 1.258999\tLR:0.000085502\n",
            "Train Epoch: 46 [1280/3527 (36%)]\tLoss: 1.214216\tLR:0.000085502\n",
            "Train Epoch: 46 [1600/3527 (45%)]\tLoss: 1.096197\tLR:0.000085502\n",
            "Train Epoch: 46 [1920/3527 (54%)]\tLoss: 1.067447\tLR:0.000085502\n",
            "Train Epoch: 46 [2240/3527 (63%)]\tLoss: 1.096533\tLR:0.000085502\n",
            "Train Epoch: 46 [2560/3527 (72%)]\tLoss: 1.185540\tLR:0.000085502\n",
            "Train Epoch: 46 [2880/3527 (81%)]\tLoss: 1.111274\tLR:0.000085502\n",
            "Train Epoch: 46 [3200/3527 (90%)]\tLoss: 0.949091\tLR:0.000085502\n",
            "Train Epoch: 46 [3520/3527 (99%)]\tLoss: 1.050913\tLR:0.000085502\n",
            "6\n",
            "epoch:46,loss:1.2182076852600854\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.0353, Accuracy: 429/637 (67%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.80      0.75        69\n",
            "           1       0.80      0.62      0.70        56\n",
            "          10       0.74      0.60      0.67        43\n",
            "          11       0.76      0.72      0.74        47\n",
            "          12       0.47      0.75      0.58        55\n",
            "          13       0.54      0.23      0.33        30\n",
            "          14       0.98      0.89      0.93        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.59      0.56      0.57        41\n",
            "           4       0.61      0.55      0.58        20\n",
            "           5       0.65      0.57      0.61        35\n",
            "           6       0.98      0.91      0.94        44\n",
            "           7       0.65      0.79      0.71        56\n",
            "           8       0.42      0.53      0.47        47\n",
            "           9       0.71      0.53      0.61        45\n",
            "\n",
            "    accuracy                           0.67       637\n",
            "   macro avg       0.71      0.67      0.68       637\n",
            "weighted avg       0.69      0.67      0.67       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 47 [320/3527 (9%)]\tLoss: 2.010774\tLR:0.000079595\n",
            "Train Epoch: 47 [640/3527 (18%)]\tLoss: 1.033666\tLR:0.000079595\n",
            "Train Epoch: 47 [960/3527 (27%)]\tLoss: 0.964523\tLR:0.000079595\n",
            "Train Epoch: 47 [1280/3527 (36%)]\tLoss: 1.097388\tLR:0.000079595\n",
            "Train Epoch: 47 [1600/3527 (45%)]\tLoss: 1.002083\tLR:0.000079595\n",
            "Train Epoch: 47 [1920/3527 (54%)]\tLoss: 1.113937\tLR:0.000079595\n",
            "Train Epoch: 47 [2240/3527 (63%)]\tLoss: 1.279570\tLR:0.000079595\n",
            "Train Epoch: 47 [2560/3527 (72%)]\tLoss: 1.000861\tLR:0.000079595\n",
            "Train Epoch: 47 [2880/3527 (81%)]\tLoss: 2.313220\tLR:0.000079595\n",
            "Train Epoch: 47 [3200/3527 (90%)]\tLoss: 1.073175\tLR:0.000079595\n",
            "Train Epoch: 47 [3520/3527 (99%)]\tLoss: 1.012623\tLR:0.000079595\n",
            "6\n",
            "epoch:47,loss:1.2412053507727545\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.0744, Accuracy: 416/637 (65%)\n",
            "\n",
            "Training complete in 0m 40s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.78      0.72        69\n",
            "           1       0.76      0.46      0.58        56\n",
            "          10       0.85      0.67      0.75        43\n",
            "          11       0.78      0.62      0.69        47\n",
            "          12       0.50      0.65      0.57        55\n",
            "          13       0.28      0.50      0.36        30\n",
            "          14       0.79      0.96      0.87        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.58      0.71      0.64        41\n",
            "           4       0.69      0.45      0.55        20\n",
            "           5       0.64      0.60      0.62        35\n",
            "           6       0.91      0.91      0.91        44\n",
            "           7       0.62      0.64      0.63        56\n",
            "           8       0.65      0.43      0.51        47\n",
            "           9       0.68      0.56      0.61        45\n",
            "\n",
            "    accuracy                           0.65       637\n",
            "   macro avg       0.69      0.66      0.67       637\n",
            "weighted avg       0.68      0.65      0.65       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 48 [320/3527 (9%)]\tLoss: 1.016979\tLR:0.000072973\n",
            "Train Epoch: 48 [640/3527 (18%)]\tLoss: 0.951084\tLR:0.000072973\n",
            "Train Epoch: 48 [960/3527 (27%)]\tLoss: 1.082409\tLR:0.000072973\n",
            "Train Epoch: 48 [1280/3527 (36%)]\tLoss: 1.566997\tLR:0.000072973\n",
            "Train Epoch: 48 [1600/3527 (45%)]\tLoss: 1.182444\tLR:0.000072973\n",
            "Train Epoch: 48 [1920/3527 (54%)]\tLoss: 1.566193\tLR:0.000072973\n",
            "Train Epoch: 48 [2240/3527 (63%)]\tLoss: 1.242418\tLR:0.000072973\n",
            "Train Epoch: 48 [2560/3527 (72%)]\tLoss: 0.979668\tLR:0.000072973\n",
            "Train Epoch: 48 [2880/3527 (81%)]\tLoss: 0.985411\tLR:0.000072973\n",
            "Train Epoch: 48 [3200/3527 (90%)]\tLoss: 2.115761\tLR:0.000072973\n",
            "Train Epoch: 48 [3520/3527 (99%)]\tLoss: 0.864483\tLR:0.000072973\n",
            "6\n",
            "epoch:48,loss:1.2331883585130847\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 1.0439, Accuracy: 437/637 (69%)\n",
            "\n",
            "Training complete in 0m 40s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.75      0.70        69\n",
            "           1       0.73      0.48      0.58        56\n",
            "          10       0.68      0.70      0.69        43\n",
            "          11       0.71      0.79      0.75        47\n",
            "          12       0.61      0.73      0.66        55\n",
            "          13       0.55      0.53      0.54        30\n",
            "          14       0.84      0.89      0.87        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.70      0.63      0.67        41\n",
            "           4       0.45      0.70      0.55        20\n",
            "           5       0.74      0.71      0.72        35\n",
            "           6       0.98      0.91      0.94        44\n",
            "           7       0.74      0.71      0.73        56\n",
            "           8       0.51      0.49      0.50        47\n",
            "           9       0.64      0.51      0.57        45\n",
            "\n",
            "    accuracy                           0.69       637\n",
            "   macro avg       0.70      0.70      0.70       637\n",
            "weighted avg       0.69      0.69      0.68       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 49 [320/3527 (9%)]\tLoss: 1.222512\tLR:0.000065796\n",
            "Train Epoch: 49 [640/3527 (18%)]\tLoss: 1.191228\tLR:0.000065796\n",
            "Train Epoch: 49 [960/3527 (27%)]\tLoss: 1.014503\tLR:0.000065796\n",
            "Train Epoch: 49 [1280/3527 (36%)]\tLoss: 1.056648\tLR:0.000065796\n",
            "Train Epoch: 49 [1600/3527 (45%)]\tLoss: 1.218365\tLR:0.000065796\n",
            "Train Epoch: 49 [1920/3527 (54%)]\tLoss: 1.315044\tLR:0.000065796\n",
            "Train Epoch: 49 [2240/3527 (63%)]\tLoss: 0.976246\tLR:0.000065796\n",
            "Train Epoch: 49 [2560/3527 (72%)]\tLoss: 0.991734\tLR:0.000065796\n",
            "Train Epoch: 49 [2880/3527 (81%)]\tLoss: 1.121338\tLR:0.000065796\n",
            "Train Epoch: 49 [3200/3527 (90%)]\tLoss: 1.057802\tLR:0.000065796\n",
            "Train Epoch: 49 [3520/3527 (99%)]\tLoss: 2.062043\tLR:0.000065796\n",
            "6\n",
            "epoch:49,loss:1.141882910921767\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9555, Accuracy: 449/637 (70%)\n",
            "\n",
            "Training complete in 0m 40s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.68      0.74        69\n",
            "           1       0.72      0.70      0.71        56\n",
            "          10       0.67      0.65      0.66        43\n",
            "          11       0.67      0.79      0.73        47\n",
            "          12       0.55      0.69      0.61        55\n",
            "          13       0.56      0.47      0.51        30\n",
            "          14       0.90      0.91      0.91        47\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.77      0.66      0.71        41\n",
            "           4       0.80      0.60      0.69        20\n",
            "           5       0.63      0.77      0.69        35\n",
            "           6       0.91      0.95      0.93        44\n",
            "           7       0.68      0.84      0.75        56\n",
            "           8       0.51      0.51      0.51        47\n",
            "           9       0.77      0.53      0.63        45\n",
            "\n",
            "    accuracy                           0.70       637\n",
            "   macro avg       0.66      0.65      0.65       637\n",
            "weighted avg       0.71      0.70      0.70       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 50 [320/3527 (9%)]\tLoss: 0.992251\tLR:0.000058244\n",
            "Train Epoch: 50 [640/3527 (18%)]\tLoss: 1.055007\tLR:0.000058244\n",
            "Train Epoch: 50 [960/3527 (27%)]\tLoss: 1.029237\tLR:0.000058244\n",
            "Train Epoch: 50 [1280/3527 (36%)]\tLoss: 1.196438\tLR:0.000058244\n",
            "Train Epoch: 50 [1600/3527 (45%)]\tLoss: 2.221808\tLR:0.000058244\n",
            "Train Epoch: 50 [1920/3527 (54%)]\tLoss: 0.979612\tLR:0.000058244\n",
            "Train Epoch: 50 [2240/3527 (63%)]\tLoss: 1.553440\tLR:0.000058244\n",
            "Train Epoch: 50 [2560/3527 (72%)]\tLoss: 1.054775\tLR:0.000058244\n",
            "Train Epoch: 50 [2880/3527 (81%)]\tLoss: 1.192153\tLR:0.000058244\n",
            "Train Epoch: 50 [3200/3527 (90%)]\tLoss: 1.009984\tLR:0.000058244\n",
            "Train Epoch: 50 [3520/3527 (99%)]\tLoss: 0.907267\tLR:0.000058244\n",
            "6\n",
            "epoch:50,loss:1.2256733152243469\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9822, Accuracy: 456/637 (72%)\n",
            "\n",
            "Training complete in 0m 40s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.77      0.75        69\n",
            "           1       0.77      0.54      0.63        56\n",
            "          10       0.76      0.72      0.74        43\n",
            "          11       0.70      0.85      0.77        47\n",
            "          12       0.57      0.87      0.69        55\n",
            "          13       0.59      0.33      0.43        30\n",
            "          14       0.87      0.96      0.91        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.76      0.61      0.68        41\n",
            "           4       0.60      0.75      0.67        20\n",
            "           5       0.74      0.66      0.70        35\n",
            "           6       0.95      0.95      0.95        44\n",
            "           7       0.66      0.80      0.73        56\n",
            "           8       0.59      0.47      0.52        47\n",
            "           9       0.74      0.56      0.63        45\n",
            "\n",
            "    accuracy                           0.72       637\n",
            "   macro avg       0.73      0.72      0.72       637\n",
            "weighted avg       0.72      0.72      0.71       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 51 [320/3527 (9%)]\tLoss: 0.847265\tLR:0.000050500\n",
            "Train Epoch: 51 [640/3527 (18%)]\tLoss: 0.993502\tLR:0.000050500\n",
            "Train Epoch: 51 [960/3527 (27%)]\tLoss: 1.082129\tLR:0.000050500\n",
            "Train Epoch: 51 [1280/3527 (36%)]\tLoss: 1.038952\tLR:0.000050500\n",
            "Train Epoch: 51 [1600/3527 (45%)]\tLoss: 1.101435\tLR:0.000050500\n",
            "Train Epoch: 51 [1920/3527 (54%)]\tLoss: 0.996208\tLR:0.000050500\n",
            "Train Epoch: 51 [2240/3527 (63%)]\tLoss: 0.944141\tLR:0.000050500\n",
            "Train Epoch: 51 [2560/3527 (72%)]\tLoss: 1.133675\tLR:0.000050500\n",
            "Train Epoch: 51 [2880/3527 (81%)]\tLoss: 0.848967\tLR:0.000050500\n",
            "Train Epoch: 51 [3200/3527 (90%)]\tLoss: 0.921586\tLR:0.000050500\n",
            "Train Epoch: 51 [3520/3527 (99%)]\tLoss: 0.999296\tLR:0.000050500\n",
            "6\n",
            "epoch:51,loss:1.0541283874898344\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9385, Accuracy: 457/637 (72%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.78      0.75        69\n",
            "           1       0.85      0.52      0.64        56\n",
            "          10       0.53      0.70      0.60        43\n",
            "          11       0.86      0.79      0.82        47\n",
            "          12       0.61      0.85      0.71        55\n",
            "          13       0.53      0.30      0.38        30\n",
            "          14       0.94      0.94      0.94        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.78      0.68      0.73        41\n",
            "           4       0.65      0.65      0.65        20\n",
            "           5       0.79      0.66      0.72        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.70      0.80      0.75        56\n",
            "           8       0.51      0.60      0.55        47\n",
            "           9       0.68      0.58      0.63        45\n",
            "\n",
            "    accuracy                           0.72       637\n",
            "   macro avg       0.74      0.72      0.72       637\n",
            "weighted avg       0.73      0.72      0.71       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 52 [320/3527 (9%)]\tLoss: 0.976633\tLR:0.000042756\n",
            "Train Epoch: 52 [640/3527 (18%)]\tLoss: 0.877344\tLR:0.000042756\n",
            "Train Epoch: 52 [960/3527 (27%)]\tLoss: 1.100174\tLR:0.000042756\n",
            "Train Epoch: 52 [1280/3527 (36%)]\tLoss: 1.085260\tLR:0.000042756\n",
            "Train Epoch: 52 [1600/3527 (45%)]\tLoss: 1.113187\tLR:0.000042756\n",
            "Train Epoch: 52 [1920/3527 (54%)]\tLoss: 0.817511\tLR:0.000042756\n",
            "Train Epoch: 52 [2240/3527 (63%)]\tLoss: 1.011926\tLR:0.000042756\n",
            "Train Epoch: 52 [2560/3527 (72%)]\tLoss: 1.679319\tLR:0.000042756\n",
            "Train Epoch: 52 [2880/3527 (81%)]\tLoss: 0.840437\tLR:0.000042756\n",
            "Train Epoch: 52 [3200/3527 (90%)]\tLoss: 1.319230\tLR:0.000042756\n",
            "Train Epoch: 52 [3520/3527 (99%)]\tLoss: 1.772476\tLR:0.000042756\n",
            "6\n",
            "epoch:52,loss:1.0535068404567134\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9344, Accuracy: 457/637 (72%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.65      0.75        69\n",
            "           1       0.69      0.80      0.74        56\n",
            "          10       0.86      0.70      0.77        43\n",
            "          11       0.82      0.77      0.79        47\n",
            "          12       0.59      0.75      0.66        55\n",
            "          13       0.50      0.47      0.48        30\n",
            "          14       0.88      0.96      0.92        47\n",
            "           2       1.00      0.50      0.67         2\n",
            "           3       0.63      0.71      0.67        41\n",
            "           4       0.48      0.60      0.53        20\n",
            "           5       0.62      0.74      0.68        35\n",
            "           6       0.95      0.95      0.95        44\n",
            "           7       0.71      0.79      0.75        56\n",
            "           8       0.58      0.47      0.52        47\n",
            "           9       0.71      0.56      0.63        45\n",
            "\n",
            "    accuracy                           0.72       637\n",
            "   macro avg       0.73      0.69      0.70       637\n",
            "weighted avg       0.73      0.72      0.72       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 53 [320/3527 (9%)]\tLoss: 0.939137\tLR:0.000035204\n",
            "Train Epoch: 53 [640/3527 (18%)]\tLoss: 0.777953\tLR:0.000035204\n",
            "Train Epoch: 53 [960/3527 (27%)]\tLoss: 0.945549\tLR:0.000035204\n",
            "Train Epoch: 53 [1280/3527 (36%)]\tLoss: 0.995257\tLR:0.000035204\n",
            "Train Epoch: 53 [1600/3527 (45%)]\tLoss: 0.808012\tLR:0.000035204\n",
            "Train Epoch: 53 [1920/3527 (54%)]\tLoss: 1.072701\tLR:0.000035204\n",
            "Train Epoch: 53 [2240/3527 (63%)]\tLoss: 0.867527\tLR:0.000035204\n",
            "Train Epoch: 53 [2560/3527 (72%)]\tLoss: 0.766049\tLR:0.000035204\n",
            "Train Epoch: 53 [2880/3527 (81%)]\tLoss: 0.939829\tLR:0.000035204\n",
            "Train Epoch: 53 [3200/3527 (90%)]\tLoss: 0.805314\tLR:0.000035204\n",
            "Train Epoch: 53 [3520/3527 (99%)]\tLoss: 0.906184\tLR:0.000035204\n",
            "6\n",
            "epoch:53,loss:1.0365247694221702\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9123, Accuracy: 474/637 (74%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.70      0.79        69\n",
            "           1       0.78      0.84      0.81        56\n",
            "          10       0.69      0.72      0.70        43\n",
            "          11       0.88      0.77      0.82        47\n",
            "          12       0.53      0.85      0.66        55\n",
            "          13       0.65      0.37      0.47        30\n",
            "          14       0.93      0.89      0.91        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.70      0.78      0.74        41\n",
            "           4       0.89      0.40      0.55        20\n",
            "           5       0.68      0.77      0.72        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.79      0.88      0.83        56\n",
            "           8       0.51      0.60      0.55        47\n",
            "           9       0.77      0.53      0.63        45\n",
            "\n",
            "    accuracy                           0.74       637\n",
            "   macro avg       0.78      0.74      0.74       637\n",
            "weighted avg       0.77      0.74      0.74       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 54 [320/3527 (9%)]\tLoss: 0.901853\tLR:0.000028027\n",
            "Train Epoch: 54 [640/3527 (18%)]\tLoss: 0.793801\tLR:0.000028027\n",
            "Train Epoch: 54 [960/3527 (27%)]\tLoss: 2.130722\tLR:0.000028027\n",
            "Train Epoch: 54 [1280/3527 (36%)]\tLoss: 0.963774\tLR:0.000028027\n",
            "Train Epoch: 54 [1600/3527 (45%)]\tLoss: 0.885533\tLR:0.000028027\n",
            "Train Epoch: 54 [1920/3527 (54%)]\tLoss: 0.936327\tLR:0.000028027\n",
            "Train Epoch: 54 [2240/3527 (63%)]\tLoss: 1.016347\tLR:0.000028027\n",
            "Train Epoch: 54 [2560/3527 (72%)]\tLoss: 2.522026\tLR:0.000028027\n",
            "Train Epoch: 54 [2880/3527 (81%)]\tLoss: 0.895814\tLR:0.000028027\n",
            "Train Epoch: 54 [3200/3527 (90%)]\tLoss: 2.515269\tLR:0.000028027\n",
            "Train Epoch: 54 [3520/3527 (99%)]\tLoss: 0.835056\tLR:0.000028027\n",
            "6\n",
            "epoch:54,loss:1.022362695083962\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9074, Accuracy: 472/637 (74%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.70      0.76        69\n",
            "           1       0.80      0.77      0.78        56\n",
            "          10       0.74      0.67      0.71        43\n",
            "          11       0.85      0.74      0.80        47\n",
            "          12       0.57      0.78      0.66        55\n",
            "          13       0.67      0.47      0.55        30\n",
            "          14       0.92      0.96      0.94        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.73      0.66      0.69        41\n",
            "           4       0.59      0.65      0.62        20\n",
            "           5       0.64      0.83      0.73        35\n",
            "           6       0.95      0.95      0.95        44\n",
            "           7       0.77      0.82      0.79        56\n",
            "           8       0.54      0.57      0.56        47\n",
            "           9       0.72      0.64      0.68        45\n",
            "\n",
            "    accuracy                           0.74       637\n",
            "   macro avg       0.76      0.75      0.75       637\n",
            "weighted avg       0.75      0.74      0.74       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 55 [320/3527 (9%)]\tLoss: 0.810462\tLR:0.000021405\n",
            "Train Epoch: 55 [640/3527 (18%)]\tLoss: 0.782279\tLR:0.000021405\n",
            "Train Epoch: 55 [960/3527 (27%)]\tLoss: 2.475523\tLR:0.000021405\n",
            "Train Epoch: 55 [1280/3527 (36%)]\tLoss: 0.924643\tLR:0.000021405\n",
            "Train Epoch: 55 [1600/3527 (45%)]\tLoss: 1.011861\tLR:0.000021405\n",
            "Train Epoch: 55 [1920/3527 (54%)]\tLoss: 0.839329\tLR:0.000021405\n",
            "Train Epoch: 55 [2240/3527 (63%)]\tLoss: 0.853525\tLR:0.000021405\n",
            "Train Epoch: 55 [2560/3527 (72%)]\tLoss: 0.828369\tLR:0.000021405\n",
            "Train Epoch: 55 [2880/3527 (81%)]\tLoss: 0.995891\tLR:0.000021405\n",
            "Train Epoch: 55 [3200/3527 (90%)]\tLoss: 0.863220\tLR:0.000021405\n",
            "Train Epoch: 55 [3520/3527 (99%)]\tLoss: 1.050922\tLR:0.000021405\n",
            "6\n",
            "epoch:55,loss:0.9970628044626735\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9192, Accuracy: 469/637 (74%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.72      0.76        69\n",
            "           1       0.82      0.71      0.76        56\n",
            "          10       0.74      0.74      0.74        43\n",
            "          11       0.86      0.77      0.81        47\n",
            "          12       0.58      0.80      0.67        55\n",
            "          13       0.71      0.40      0.51        30\n",
            "          14       0.92      0.94      0.93        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.65      0.76      0.70        41\n",
            "           4       0.70      0.35      0.47        20\n",
            "           5       0.67      0.74      0.70        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.69      0.84      0.76        56\n",
            "           8       0.59      0.64      0.61        47\n",
            "           9       0.67      0.58      0.62        45\n",
            "\n",
            "    accuracy                           0.74       637\n",
            "   macro avg       0.76      0.73      0.73       637\n",
            "weighted avg       0.75      0.74      0.73       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 56 [320/3527 (9%)]\tLoss: 0.861053\tLR:0.000015498\n",
            "Train Epoch: 56 [640/3527 (18%)]\tLoss: 0.826322\tLR:0.000015498\n",
            "Train Epoch: 56 [960/3527 (27%)]\tLoss: 0.950780\tLR:0.000015498\n",
            "Train Epoch: 56 [1280/3527 (36%)]\tLoss: 1.006552\tLR:0.000015498\n",
            "Train Epoch: 56 [1600/3527 (45%)]\tLoss: 1.077320\tLR:0.000015498\n",
            "Train Epoch: 56 [1920/3527 (54%)]\tLoss: 0.874483\tLR:0.000015498\n",
            "Train Epoch: 56 [2240/3527 (63%)]\tLoss: 0.879593\tLR:0.000015498\n",
            "Train Epoch: 56 [2560/3527 (72%)]\tLoss: 0.817396\tLR:0.000015498\n",
            "Train Epoch: 56 [2880/3527 (81%)]\tLoss: 1.510835\tLR:0.000015498\n",
            "Train Epoch: 56 [3200/3527 (90%)]\tLoss: 0.893218\tLR:0.000015498\n",
            "Train Epoch: 56 [3520/3527 (99%)]\tLoss: 0.884799\tLR:0.000015498\n",
            "6\n",
            "epoch:56,loss:0.9552376221966099\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9028, Accuracy: 466/637 (73%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.74      0.79        69\n",
            "           1       0.83      0.70      0.76        56\n",
            "          10       0.65      0.70      0.67        43\n",
            "          11       0.88      0.77      0.82        47\n",
            "          12       0.57      0.75      0.65        55\n",
            "          13       0.57      0.40      0.47        30\n",
            "          14       0.92      0.96      0.94        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.68      0.73      0.71        41\n",
            "           4       0.59      0.50      0.54        20\n",
            "           5       0.72      0.66      0.69        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.74      0.86      0.79        56\n",
            "           8       0.53      0.66      0.59        47\n",
            "           9       0.65      0.58      0.61        45\n",
            "\n",
            "    accuracy                           0.73       637\n",
            "   macro avg       0.74      0.73      0.73       637\n",
            "weighted avg       0.74      0.73      0.73       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 57 [320/3527 (9%)]\tLoss: 0.817436\tLR:0.000010454\n",
            "Train Epoch: 57 [640/3527 (18%)]\tLoss: 0.770316\tLR:0.000010454\n",
            "Train Epoch: 57 [960/3527 (27%)]\tLoss: 0.943885\tLR:0.000010454\n",
            "Train Epoch: 57 [1280/3527 (36%)]\tLoss: 0.781438\tLR:0.000010454\n",
            "Train Epoch: 57 [1600/3527 (45%)]\tLoss: 0.915013\tLR:0.000010454\n",
            "Train Epoch: 57 [1920/3527 (54%)]\tLoss: 0.708775\tLR:0.000010454\n",
            "Train Epoch: 57 [2240/3527 (63%)]\tLoss: 0.840111\tLR:0.000010454\n",
            "Train Epoch: 57 [2560/3527 (72%)]\tLoss: 0.866324\tLR:0.000010454\n",
            "Train Epoch: 57 [2880/3527 (81%)]\tLoss: 0.844306\tLR:0.000010454\n",
            "Train Epoch: 57 [3200/3527 (90%)]\tLoss: 0.971158\tLR:0.000010454\n",
            "Train Epoch: 57 [3520/3527 (99%)]\tLoss: 0.847528\tLR:0.000010454\n",
            "6\n",
            "epoch:57,loss:0.9449229621672416\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8769, Accuracy: 477/637 (75%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.75      0.78        69\n",
            "           1       0.80      0.70      0.74        56\n",
            "          10       0.74      0.72      0.73        43\n",
            "          11       0.88      0.81      0.84        47\n",
            "          12       0.59      0.85      0.70        55\n",
            "          13       0.67      0.40      0.50        30\n",
            "          14       0.94      0.94      0.94        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.73      0.73      0.73        41\n",
            "           4       0.71      0.50      0.59        20\n",
            "           5       0.70      0.74      0.72        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.69      0.89      0.78        56\n",
            "           8       0.60      0.64      0.62        47\n",
            "           9       0.71      0.53      0.61        45\n",
            "\n",
            "    accuracy                           0.75       637\n",
            "   macro avg       0.77      0.74      0.75       637\n",
            "weighted avg       0.76      0.75      0.75       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 58 [320/3527 (9%)]\tLoss: 0.904619\tLR:0.000006395\n",
            "Train Epoch: 58 [640/3527 (18%)]\tLoss: 0.874468\tLR:0.000006395\n",
            "Train Epoch: 58 [960/3527 (27%)]\tLoss: 0.764764\tLR:0.000006395\n",
            "Train Epoch: 58 [1280/3527 (36%)]\tLoss: 0.862726\tLR:0.000006395\n",
            "Train Epoch: 58 [1600/3527 (45%)]\tLoss: 0.955099\tLR:0.000006395\n",
            "Train Epoch: 58 [1920/3527 (54%)]\tLoss: 1.116355\tLR:0.000006395\n",
            "Train Epoch: 58 [2240/3527 (63%)]\tLoss: 0.840737\tLR:0.000006395\n",
            "Train Epoch: 58 [2560/3527 (72%)]\tLoss: 1.006080\tLR:0.000006395\n",
            "Train Epoch: 58 [2880/3527 (81%)]\tLoss: 0.809708\tLR:0.000006395\n",
            "Train Epoch: 58 [3200/3527 (90%)]\tLoss: 0.945516\tLR:0.000006395\n",
            "Train Epoch: 58 [3520/3527 (99%)]\tLoss: 0.842530\tLR:0.000006395\n",
            "6\n",
            "epoch:58,loss:0.9239282688579044\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8687, Accuracy: 482/637 (76%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.77      0.80        69\n",
            "           1       0.80      0.71      0.75        56\n",
            "          10       0.72      0.72      0.72        43\n",
            "          11       0.82      0.79      0.80        47\n",
            "          12       0.62      0.87      0.73        55\n",
            "          13       0.70      0.47      0.56        30\n",
            "          14       0.92      0.96      0.94        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.76      0.76      0.76        41\n",
            "           4       0.69      0.55      0.61        20\n",
            "           5       0.74      0.74      0.74        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.74      0.86      0.79        56\n",
            "           8       0.58      0.64      0.61        47\n",
            "           9       0.69      0.53      0.60        45\n",
            "\n",
            "    accuracy                           0.76       637\n",
            "   macro avg       0.77      0.75      0.76       637\n",
            "weighted avg       0.76      0.76      0.75       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 59 [320/3527 (9%)]\tLoss: 0.697779\tLR:0.000003423\n",
            "Train Epoch: 59 [640/3527 (18%)]\tLoss: 0.743562\tLR:0.000003423\n",
            "Train Epoch: 59 [960/3527 (27%)]\tLoss: 0.794471\tLR:0.000003423\n",
            "Train Epoch: 59 [1280/3527 (36%)]\tLoss: 0.849907\tLR:0.000003423\n",
            "Train Epoch: 59 [1600/3527 (45%)]\tLoss: 0.869929\tLR:0.000003423\n",
            "Train Epoch: 59 [1920/3527 (54%)]\tLoss: 0.869880\tLR:0.000003423\n",
            "Train Epoch: 59 [2240/3527 (63%)]\tLoss: 0.855392\tLR:0.000003423\n",
            "Train Epoch: 59 [2560/3527 (72%)]\tLoss: 0.870534\tLR:0.000003423\n",
            "Train Epoch: 59 [2880/3527 (81%)]\tLoss: 0.811915\tLR:0.000003423\n",
            "Train Epoch: 59 [3200/3527 (90%)]\tLoss: 0.910148\tLR:0.000003423\n",
            "Train Epoch: 59 [3520/3527 (99%)]\tLoss: 0.892333\tLR:0.000003423\n",
            "6\n",
            "epoch:59,loss:0.9219880581976058\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8731, Accuracy: 482/637 (76%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.77      0.79        69\n",
            "           1       0.81      0.77      0.79        56\n",
            "          10       0.79      0.70      0.74        43\n",
            "          11       0.84      0.79      0.81        47\n",
            "          12       0.62      0.87      0.72        55\n",
            "          13       0.68      0.43      0.53        30\n",
            "          14       0.94      0.94      0.94        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.69      0.76      0.72        41\n",
            "           4       0.69      0.55      0.61        20\n",
            "           5       0.73      0.69      0.71        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.74      0.86      0.79        56\n",
            "           8       0.56      0.66      0.61        47\n",
            "           9       0.74      0.56      0.63        45\n",
            "\n",
            "    accuracy                           0.76       637\n",
            "   macro avg       0.77      0.75      0.76       637\n",
            "weighted avg       0.76      0.76      0.75       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 60 [320/3527 (9%)]\tLoss: 2.392424\tLR:0.000001609\n",
            "Train Epoch: 60 [640/3527 (18%)]\tLoss: 0.840354\tLR:0.000001609\n",
            "Train Epoch: 60 [960/3527 (27%)]\tLoss: 0.931405\tLR:0.000001609\n",
            "Train Epoch: 60 [1280/3527 (36%)]\tLoss: 0.921025\tLR:0.000001609\n",
            "Train Epoch: 60 [1600/3527 (45%)]\tLoss: 0.715375\tLR:0.000001609\n",
            "Train Epoch: 60 [1920/3527 (54%)]\tLoss: 0.970031\tLR:0.000001609\n",
            "Train Epoch: 60 [2240/3527 (63%)]\tLoss: 0.782009\tLR:0.000001609\n",
            "Train Epoch: 60 [2560/3527 (72%)]\tLoss: 0.817515\tLR:0.000001609\n",
            "Train Epoch: 60 [2880/3527 (81%)]\tLoss: 0.695309\tLR:0.000001609\n",
            "Train Epoch: 60 [3200/3527 (90%)]\tLoss: 0.783551\tLR:0.000001609\n",
            "Train Epoch: 60 [3520/3527 (99%)]\tLoss: 2.272494\tLR:0.000001609\n",
            "6\n",
            "epoch:60,loss:1.002834003787857\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8877, Accuracy: 475/637 (75%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78        69\n",
            "           1       0.79      0.66      0.72        56\n",
            "          10       0.65      0.72      0.68        43\n",
            "          11       0.90      0.77      0.83        47\n",
            "          12       0.61      0.85      0.71        55\n",
            "          13       0.63      0.40      0.49        30\n",
            "          14       0.90      0.96      0.93        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.66      0.76      0.70        41\n",
            "           4       0.71      0.50      0.59        20\n",
            "           5       0.74      0.71      0.72        35\n",
            "           6       0.95      0.95      0.95        44\n",
            "           7       0.79      0.88      0.83        56\n",
            "           8       0.58      0.62      0.60        47\n",
            "           9       0.69      0.60      0.64        45\n",
            "\n",
            "    accuracy                           0.75       637\n",
            "   macro avg       0.76      0.74      0.75       637\n",
            "weighted avg       0.75      0.75      0.74       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 61 [320/3527 (9%)]\tLoss: 0.688026\tLR:0.000001000\n",
            "Train Epoch: 61 [640/3527 (18%)]\tLoss: 0.693292\tLR:0.000001000\n",
            "Train Epoch: 61 [960/3527 (27%)]\tLoss: 0.764846\tLR:0.000001000\n",
            "Train Epoch: 61 [1280/3527 (36%)]\tLoss: 0.864974\tLR:0.000001000\n",
            "Train Epoch: 61 [1600/3527 (45%)]\tLoss: 0.814264\tLR:0.000001000\n",
            "Train Epoch: 61 [1920/3527 (54%)]\tLoss: 0.845378\tLR:0.000001000\n",
            "Train Epoch: 61 [2240/3527 (63%)]\tLoss: 0.802572\tLR:0.000001000\n",
            "Train Epoch: 61 [2560/3527 (72%)]\tLoss: 0.820376\tLR:0.000001000\n",
            "Train Epoch: 61 [2880/3527 (81%)]\tLoss: 0.778645\tLR:0.000001000\n",
            "Train Epoch: 61 [3200/3527 (90%)]\tLoss: 0.921716\tLR:0.000001000\n",
            "Train Epoch: 61 [3520/3527 (99%)]\tLoss: 0.765607\tLR:0.000001000\n",
            "6\n",
            "epoch:61,loss:0.9018276092168447\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8894, Accuracy: 481/637 (76%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.77      0.80        69\n",
            "           1       0.82      0.73      0.77        56\n",
            "          10       0.72      0.72      0.72        43\n",
            "          11       0.86      0.77      0.81        47\n",
            "          12       0.61      0.87      0.72        55\n",
            "          13       0.71      0.40      0.51        30\n",
            "          14       0.92      0.96      0.94        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.72      0.76      0.74        41\n",
            "           4       0.73      0.55      0.63        20\n",
            "           5       0.76      0.71      0.74        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.69      0.89      0.78        56\n",
            "           8       0.56      0.64      0.59        47\n",
            "           9       0.77      0.53      0.63        45\n",
            "\n",
            "    accuracy                           0.76       637\n",
            "   macro avg       0.78      0.75      0.76       637\n",
            "weighted avg       0.77      0.76      0.75       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 62 [320/3527 (9%)]\tLoss: 0.907646\tLR:0.000001609\n",
            "Train Epoch: 62 [640/3527 (18%)]\tLoss: 0.890381\tLR:0.000001609\n",
            "Train Epoch: 62 [960/3527 (27%)]\tLoss: 1.980431\tLR:0.000001609\n",
            "Train Epoch: 62 [1280/3527 (36%)]\tLoss: 0.932974\tLR:0.000001609\n",
            "Train Epoch: 62 [1600/3527 (45%)]\tLoss: 0.719276\tLR:0.000001609\n",
            "Train Epoch: 62 [1920/3527 (54%)]\tLoss: 0.838012\tLR:0.000001609\n",
            "Train Epoch: 62 [2240/3527 (63%)]\tLoss: 0.892981\tLR:0.000001609\n",
            "Train Epoch: 62 [2560/3527 (72%)]\tLoss: 0.789202\tLR:0.000001609\n",
            "Train Epoch: 62 [2880/3527 (81%)]\tLoss: 1.882818\tLR:0.000001609\n",
            "Train Epoch: 62 [3200/3527 (90%)]\tLoss: 0.962151\tLR:0.000001609\n",
            "Train Epoch: 62 [3520/3527 (99%)]\tLoss: 0.889083\tLR:0.000001609\n",
            "6\n",
            "epoch:62,loss:0.9235292313335178\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8717, Accuracy: 481/637 (76%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.77      0.82        69\n",
            "           1       0.81      0.77      0.79        56\n",
            "          10       0.72      0.72      0.72        43\n",
            "          11       0.80      0.77      0.78        47\n",
            "          12       0.61      0.87      0.72        55\n",
            "          13       0.72      0.43      0.54        30\n",
            "          14       0.90      0.96      0.93        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.75      0.73      0.74        41\n",
            "           4       0.65      0.55      0.59        20\n",
            "           5       0.72      0.74      0.73        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.72      0.82      0.77        56\n",
            "           8       0.58      0.60      0.59        47\n",
            "           9       0.71      0.60      0.65        45\n",
            "\n",
            "    accuracy                           0.76       637\n",
            "   macro avg       0.77      0.75      0.76       637\n",
            "weighted avg       0.76      0.76      0.75       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 63 [320/3527 (9%)]\tLoss: 0.860832\tLR:0.000003423\n",
            "Train Epoch: 63 [640/3527 (18%)]\tLoss: 0.849240\tLR:0.000003423\n",
            "Train Epoch: 63 [960/3527 (27%)]\tLoss: 0.907303\tLR:0.000003423\n",
            "Train Epoch: 63 [1280/3527 (36%)]\tLoss: 0.763585\tLR:0.000003423\n",
            "Train Epoch: 63 [1600/3527 (45%)]\tLoss: 0.889593\tLR:0.000003423\n",
            "Train Epoch: 63 [1920/3527 (54%)]\tLoss: 0.802959\tLR:0.000003423\n",
            "Train Epoch: 63 [2240/3527 (63%)]\tLoss: 0.866562\tLR:0.000003423\n",
            "Train Epoch: 63 [2560/3527 (72%)]\tLoss: 0.725299\tLR:0.000003423\n",
            "Train Epoch: 63 [2880/3527 (81%)]\tLoss: 0.870983\tLR:0.000003423\n",
            "Train Epoch: 63 [3200/3527 (90%)]\tLoss: 1.528532\tLR:0.000003423\n",
            "Train Epoch: 63 [3520/3527 (99%)]\tLoss: 0.905719\tLR:0.000003423\n",
            "6\n",
            "epoch:63,loss:1.0090137510686308\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8754, Accuracy: 480/637 (75%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.74      0.79        69\n",
            "           1       0.79      0.79      0.79        56\n",
            "          10       0.78      0.72      0.75        43\n",
            "          11       0.79      0.79      0.79        47\n",
            "          12       0.63      0.85      0.72        55\n",
            "          13       0.63      0.40      0.49        30\n",
            "          14       0.94      0.96      0.95        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.75      0.73      0.74        41\n",
            "           4       0.69      0.55      0.61        20\n",
            "           5       0.76      0.74      0.75        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.68      0.84      0.75        56\n",
            "           8       0.57      0.66      0.61        47\n",
            "           9       0.71      0.53      0.61        45\n",
            "\n",
            "    accuracy                           0.75       637\n",
            "   macro avg       0.77      0.75      0.75       637\n",
            "weighted avg       0.76      0.75      0.75       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 64 [320/3527 (9%)]\tLoss: 0.783096\tLR:0.000006395\n",
            "Train Epoch: 64 [640/3527 (18%)]\tLoss: 0.980430\tLR:0.000006395\n",
            "Train Epoch: 64 [960/3527 (27%)]\tLoss: 1.105436\tLR:0.000006395\n",
            "Train Epoch: 64 [1280/3527 (36%)]\tLoss: 0.831802\tLR:0.000006395\n",
            "Train Epoch: 64 [1600/3527 (45%)]\tLoss: 0.761834\tLR:0.000006395\n",
            "Train Epoch: 64 [1920/3527 (54%)]\tLoss: 0.820754\tLR:0.000006395\n",
            "Train Epoch: 64 [2240/3527 (63%)]\tLoss: 0.920241\tLR:0.000006395\n",
            "Train Epoch: 64 [2560/3527 (72%)]\tLoss: 0.830345\tLR:0.000006395\n",
            "Train Epoch: 64 [2880/3527 (81%)]\tLoss: 0.782045\tLR:0.000006395\n",
            "Train Epoch: 64 [3200/3527 (90%)]\tLoss: 0.827544\tLR:0.000006395\n",
            "Train Epoch: 64 [3520/3527 (99%)]\tLoss: 0.682168\tLR:0.000006395\n",
            "6\n",
            "epoch:64,loss:0.9436497156684464\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8752, Accuracy: 477/637 (75%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.77      0.78        69\n",
            "           1       0.77      0.73      0.75        56\n",
            "          10       0.73      0.70      0.71        43\n",
            "          11       0.76      0.81      0.78        47\n",
            "          12       0.64      0.85      0.73        55\n",
            "          13       0.67      0.40      0.50        30\n",
            "          14       0.92      0.94      0.93        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.71      0.71      0.71        41\n",
            "           4       0.67      0.60      0.63        20\n",
            "           5       0.75      0.77      0.76        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.70      0.84      0.76        56\n",
            "           8       0.61      0.57      0.59        47\n",
            "           9       0.74      0.58      0.65        45\n",
            "\n",
            "    accuracy                           0.75       637\n",
            "   macro avg       0.76      0.75      0.75       637\n",
            "weighted avg       0.75      0.75      0.75       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 65 [320/3527 (9%)]\tLoss: 2.318143\tLR:0.000010454\n",
            "Train Epoch: 65 [640/3527 (18%)]\tLoss: 0.850509\tLR:0.000010454\n",
            "Train Epoch: 65 [960/3527 (27%)]\tLoss: 0.739258\tLR:0.000010454\n",
            "Train Epoch: 65 [1280/3527 (36%)]\tLoss: 0.872123\tLR:0.000010454\n",
            "Train Epoch: 65 [1600/3527 (45%)]\tLoss: 0.704098\tLR:0.000010454\n",
            "Train Epoch: 65 [1920/3527 (54%)]\tLoss: 0.844206\tLR:0.000010454\n",
            "Train Epoch: 65 [2240/3527 (63%)]\tLoss: 0.862525\tLR:0.000010454\n",
            "Train Epoch: 65 [2560/3527 (72%)]\tLoss: 0.781546\tLR:0.000010454\n",
            "Train Epoch: 65 [2880/3527 (81%)]\tLoss: 0.908140\tLR:0.000010454\n",
            "Train Epoch: 65 [3200/3527 (90%)]\tLoss: 0.881357\tLR:0.000010454\n",
            "Train Epoch: 65 [3520/3527 (99%)]\tLoss: 0.791221\tLR:0.000010454\n",
            "6\n",
            "epoch:65,loss:0.9569789450447839\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9038, Accuracy: 468/637 (73%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.72      0.79        69\n",
            "           1       0.82      0.73      0.77        56\n",
            "          10       0.73      0.77      0.75        43\n",
            "          11       0.73      0.79      0.76        47\n",
            "          12       0.63      0.82      0.71        55\n",
            "          13       0.62      0.43      0.51        30\n",
            "          14       0.88      0.96      0.92        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.70      0.73      0.71        41\n",
            "           4       0.65      0.55      0.59        20\n",
            "           5       0.72      0.74      0.73        35\n",
            "           6       0.95      0.95      0.95        44\n",
            "           7       0.61      0.79      0.69        56\n",
            "           8       0.52      0.47      0.49        47\n",
            "           9       0.79      0.60      0.68        45\n",
            "\n",
            "    accuracy                           0.73       637\n",
            "   macro avg       0.75      0.74      0.74       637\n",
            "weighted avg       0.74      0.73      0.73       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 66 [320/3527 (9%)]\tLoss: 0.878640\tLR:0.000015498\n",
            "Train Epoch: 66 [640/3527 (18%)]\tLoss: 0.908344\tLR:0.000015498\n",
            "Train Epoch: 66 [960/3527 (27%)]\tLoss: 0.763273\tLR:0.000015498\n",
            "Train Epoch: 66 [1280/3527 (36%)]\tLoss: 0.773997\tLR:0.000015498\n",
            "Train Epoch: 66 [1600/3527 (45%)]\tLoss: 0.869785\tLR:0.000015498\n",
            "Train Epoch: 66 [1920/3527 (54%)]\tLoss: 0.857576\tLR:0.000015498\n",
            "Train Epoch: 66 [2240/3527 (63%)]\tLoss: 1.010705\tLR:0.000015498\n",
            "Train Epoch: 66 [2560/3527 (72%)]\tLoss: 0.713527\tLR:0.000015498\n",
            "Train Epoch: 66 [2880/3527 (81%)]\tLoss: 0.833331\tLR:0.000015498\n",
            "Train Epoch: 66 [3200/3527 (90%)]\tLoss: 2.504714\tLR:0.000015498\n",
            "Train Epoch: 66 [3520/3527 (99%)]\tLoss: 0.908492\tLR:0.000015498\n",
            "6\n",
            "epoch:66,loss:0.9915415064708607\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8823, Accuracy: 475/637 (75%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.75      0.76        69\n",
            "           1       0.75      0.71      0.73        56\n",
            "          10       0.75      0.70      0.72        43\n",
            "          11       0.74      0.83      0.78        47\n",
            "          12       0.63      0.84      0.72        55\n",
            "          13       0.67      0.40      0.50        30\n",
            "          14       0.90      0.96      0.93        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.71      0.71      0.71        41\n",
            "           4       0.68      0.65      0.67        20\n",
            "           5       0.75      0.77      0.76        35\n",
            "           6       0.95      0.95      0.95        44\n",
            "           7       0.72      0.84      0.78        56\n",
            "           8       0.62      0.53      0.57        47\n",
            "           9       0.72      0.58      0.64        45\n",
            "\n",
            "    accuracy                           0.75       637\n",
            "   macro avg       0.76      0.75      0.75       637\n",
            "weighted avg       0.75      0.75      0.74       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 67 [320/3527 (9%)]\tLoss: 1.027777\tLR:0.000021405\n",
            "Train Epoch: 67 [640/3527 (18%)]\tLoss: 0.811874\tLR:0.000021405\n",
            "Train Epoch: 67 [960/3527 (27%)]\tLoss: 0.843050\tLR:0.000021405\n",
            "Train Epoch: 67 [1280/3527 (36%)]\tLoss: 0.865519\tLR:0.000021405\n",
            "Train Epoch: 67 [1600/3527 (45%)]\tLoss: 1.176334\tLR:0.000021405\n",
            "Train Epoch: 67 [1920/3527 (54%)]\tLoss: 0.797239\tLR:0.000021405\n",
            "Train Epoch: 67 [2240/3527 (63%)]\tLoss: 0.839752\tLR:0.000021405\n",
            "Train Epoch: 67 [2560/3527 (72%)]\tLoss: 0.878339\tLR:0.000021405\n",
            "Train Epoch: 67 [2880/3527 (81%)]\tLoss: 0.697519\tLR:0.000021405\n",
            "Train Epoch: 67 [3200/3527 (90%)]\tLoss: 0.810221\tLR:0.000021405\n",
            "Train Epoch: 67 [3520/3527 (99%)]\tLoss: 1.088675\tLR:0.000021405\n",
            "6\n",
            "epoch:67,loss:0.999533060971681\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9078, Accuracy: 474/637 (74%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.72      0.76        69\n",
            "           1       0.73      0.80      0.76        56\n",
            "          10       0.77      0.70      0.73        43\n",
            "          11       0.86      0.81      0.84        47\n",
            "          12       0.60      0.87      0.71        55\n",
            "          13       0.61      0.37      0.46        30\n",
            "          14       0.92      0.96      0.94        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.67      0.71      0.69        41\n",
            "           4       0.85      0.55      0.67        20\n",
            "           5       0.68      0.71      0.69        35\n",
            "           6       0.98      0.93      0.95        44\n",
            "           7       0.71      0.89      0.79        56\n",
            "           8       0.57      0.53      0.55        47\n",
            "           9       0.75      0.53      0.62        45\n",
            "\n",
            "    accuracy                           0.74       637\n",
            "   macro avg       0.77      0.74      0.74       637\n",
            "weighted avg       0.75      0.74      0.74       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 68 [320/3527 (9%)]\tLoss: 1.007417\tLR:0.000028027\n",
            "Train Epoch: 68 [640/3527 (18%)]\tLoss: 0.872684\tLR:0.000028027\n",
            "Train Epoch: 68 [960/3527 (27%)]\tLoss: 0.819340\tLR:0.000028027\n",
            "Train Epoch: 68 [1280/3527 (36%)]\tLoss: 0.782236\tLR:0.000028027\n",
            "Train Epoch: 68 [1600/3527 (45%)]\tLoss: 0.883406\tLR:0.000028027\n",
            "Train Epoch: 68 [1920/3527 (54%)]\tLoss: 1.788098\tLR:0.000028027\n",
            "Train Epoch: 68 [2240/3527 (63%)]\tLoss: 0.912067\tLR:0.000028027\n",
            "Train Epoch: 68 [2560/3527 (72%)]\tLoss: 0.745605\tLR:0.000028027\n",
            "Train Epoch: 68 [2880/3527 (81%)]\tLoss: 0.944138\tLR:0.000028027\n",
            "Train Epoch: 68 [3200/3527 (90%)]\tLoss: 0.930364\tLR:0.000028027\n",
            "Train Epoch: 68 [3520/3527 (99%)]\tLoss: 0.812858\tLR:0.000028027\n",
            "6\n",
            "epoch:68,loss:0.9258123856407028\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9214, Accuracy: 469/637 (74%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.80      0.75        69\n",
            "           1       0.78      0.64      0.71        56\n",
            "          10       0.79      0.70      0.74        43\n",
            "          11       0.83      0.74      0.79        47\n",
            "          12       0.60      0.85      0.71        55\n",
            "          13       0.60      0.40      0.48        30\n",
            "          14       0.85      0.94      0.89        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.79      0.73      0.76        41\n",
            "           4       0.71      0.60      0.65        20\n",
            "           5       0.72      0.74      0.73        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.66      0.88      0.75        56\n",
            "           8       0.66      0.49      0.56        47\n",
            "           9       0.67      0.58      0.62        45\n",
            "\n",
            "    accuracy                           0.74       637\n",
            "   macro avg       0.76      0.74      0.74       637\n",
            "weighted avg       0.74      0.74      0.73       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 69 [320/3527 (9%)]\tLoss: 1.057421\tLR:0.000035204\n",
            "Train Epoch: 69 [640/3527 (18%)]\tLoss: 0.866423\tLR:0.000035204\n",
            "Train Epoch: 69 [960/3527 (27%)]\tLoss: 0.872741\tLR:0.000035204\n",
            "Train Epoch: 69 [1280/3527 (36%)]\tLoss: 0.976122\tLR:0.000035204\n",
            "Train Epoch: 69 [1600/3527 (45%)]\tLoss: 0.809206\tLR:0.000035204\n",
            "Train Epoch: 69 [1920/3527 (54%)]\tLoss: 1.018290\tLR:0.000035204\n",
            "Train Epoch: 69 [2240/3527 (63%)]\tLoss: 2.018159\tLR:0.000035204\n",
            "Train Epoch: 69 [2560/3527 (72%)]\tLoss: 0.943367\tLR:0.000035204\n",
            "Train Epoch: 69 [2880/3527 (81%)]\tLoss: 0.980020\tLR:0.000035204\n",
            "Train Epoch: 69 [3200/3527 (90%)]\tLoss: 0.932047\tLR:0.000035204\n",
            "Train Epoch: 69 [3520/3527 (99%)]\tLoss: 0.849872\tLR:0.000035204\n",
            "6\n",
            "epoch:69,loss:0.9184738032452695\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8923, Accuracy: 475/637 (75%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.80      0.79        69\n",
            "           1       0.81      0.70      0.75        56\n",
            "          10       0.71      0.70      0.71        43\n",
            "          11       0.73      0.79      0.76        47\n",
            "          12       0.68      0.80      0.73        55\n",
            "          13       0.55      0.40      0.46        30\n",
            "          14       0.94      0.98      0.96        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.70      0.68      0.69        41\n",
            "           4       0.62      0.65      0.63        20\n",
            "           5       0.70      0.74      0.72        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.79      0.75      0.77        56\n",
            "           8       0.60      0.60      0.60        47\n",
            "           9       0.67      0.69      0.68        45\n",
            "\n",
            "    accuracy                           0.75       637\n",
            "   macro avg       0.75      0.75      0.75       637\n",
            "weighted avg       0.75      0.75      0.74       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 70 [320/3527 (9%)]\tLoss: 0.752407\tLR:0.000042756\n",
            "Train Epoch: 70 [640/3527 (18%)]\tLoss: 0.849240\tLR:0.000042756\n",
            "Train Epoch: 70 [960/3527 (27%)]\tLoss: 0.817419\tLR:0.000042756\n",
            "Train Epoch: 70 [1280/3527 (36%)]\tLoss: 0.805637\tLR:0.000042756\n",
            "Train Epoch: 70 [1600/3527 (45%)]\tLoss: 2.058997\tLR:0.000042756\n",
            "Train Epoch: 70 [1920/3527 (54%)]\tLoss: 2.430013\tLR:0.000042756\n",
            "Train Epoch: 70 [2240/3527 (63%)]\tLoss: 1.184597\tLR:0.000042756\n",
            "Train Epoch: 70 [2560/3527 (72%)]\tLoss: 2.987876\tLR:0.000042756\n",
            "Train Epoch: 70 [2880/3527 (81%)]\tLoss: 1.019026\tLR:0.000042756\n",
            "Train Epoch: 70 [3200/3527 (90%)]\tLoss: 1.511545\tLR:0.000042756\n",
            "Train Epoch: 70 [3520/3527 (99%)]\tLoss: 0.855032\tLR:0.000042756\n",
            "6\n",
            "epoch:70,loss:0.9792451085271062\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8942, Accuracy: 475/637 (75%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.67      0.74        69\n",
            "           1       0.67      0.75      0.71        56\n",
            "          10       0.73      0.77      0.75        43\n",
            "          11       0.73      0.81      0.77        47\n",
            "          12       0.56      0.85      0.68        55\n",
            "          13       0.79      0.50      0.61        30\n",
            "          14       0.94      0.96      0.95        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.78      0.71      0.74        41\n",
            "           4       0.73      0.55      0.63        20\n",
            "           5       0.73      0.77      0.75        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.81      0.82      0.81        56\n",
            "           8       0.59      0.57      0.58        47\n",
            "           9       0.74      0.56      0.63        45\n",
            "\n",
            "    accuracy                           0.75       637\n",
            "   macro avg       0.77      0.75      0.75       637\n",
            "weighted avg       0.76      0.75      0.74       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 71 [320/3527 (9%)]\tLoss: 0.843325\tLR:0.000050500\n",
            "Train Epoch: 71 [640/3527 (18%)]\tLoss: 0.841978\tLR:0.000050500\n",
            "Train Epoch: 71 [960/3527 (27%)]\tLoss: 0.844811\tLR:0.000050500\n",
            "Train Epoch: 71 [1280/3527 (36%)]\tLoss: 0.762640\tLR:0.000050500\n",
            "Train Epoch: 71 [1600/3527 (45%)]\tLoss: 0.918389\tLR:0.000050500\n",
            "Train Epoch: 71 [1920/3527 (54%)]\tLoss: 0.873018\tLR:0.000050500\n",
            "Train Epoch: 71 [2240/3527 (63%)]\tLoss: 0.975374\tLR:0.000050500\n",
            "Train Epoch: 71 [2560/3527 (72%)]\tLoss: 0.766754\tLR:0.000050500\n",
            "Train Epoch: 71 [2880/3527 (81%)]\tLoss: 0.763283\tLR:0.000050500\n",
            "Train Epoch: 71 [3200/3527 (90%)]\tLoss: 0.930731\tLR:0.000050500\n",
            "Train Epoch: 71 [3520/3527 (99%)]\tLoss: 1.059789\tLR:0.000050500\n",
            "6\n",
            "epoch:71,loss:0.9702771746360503\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8985, Accuracy: 467/637 (73%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.71      0.75        69\n",
            "           1       0.76      0.79      0.77        56\n",
            "          10       0.79      0.72      0.76        43\n",
            "          11       0.77      0.77      0.77        47\n",
            "          12       0.80      0.73      0.76        55\n",
            "          13       0.67      0.40      0.50        30\n",
            "          14       0.94      0.94      0.94        47\n",
            "           2       1.00      0.50      0.67         2\n",
            "           3       0.59      0.71      0.64        41\n",
            "           4       0.47      0.70      0.56        20\n",
            "           5       0.83      0.69      0.75        35\n",
            "           6       0.93      0.91      0.92        44\n",
            "           7       0.65      0.88      0.75        56\n",
            "           8       0.49      0.53      0.51        47\n",
            "           9       0.74      0.64      0.69        45\n",
            "\n",
            "    accuracy                           0.73       637\n",
            "   macro avg       0.75      0.71      0.72       637\n",
            "weighted avg       0.75      0.73      0.73       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 72 [320/3527 (9%)]\tLoss: 0.872352\tLR:0.000058244\n",
            "Train Epoch: 72 [640/3527 (18%)]\tLoss: 0.962020\tLR:0.000058244\n",
            "Train Epoch: 72 [960/3527 (27%)]\tLoss: 0.940197\tLR:0.000058244\n",
            "Train Epoch: 72 [1280/3527 (36%)]\tLoss: 0.857295\tLR:0.000058244\n",
            "Train Epoch: 72 [1600/3527 (45%)]\tLoss: 0.892716\tLR:0.000058244\n",
            "Train Epoch: 72 [1920/3527 (54%)]\tLoss: 1.001718\tLR:0.000058244\n",
            "Train Epoch: 72 [2240/3527 (63%)]\tLoss: 0.780844\tLR:0.000058244\n",
            "Train Epoch: 72 [2560/3527 (72%)]\tLoss: 0.961098\tLR:0.000058244\n",
            "Train Epoch: 72 [2880/3527 (81%)]\tLoss: 0.862588\tLR:0.000058244\n",
            "Train Epoch: 72 [3200/3527 (90%)]\tLoss: 2.720809\tLR:0.000058244\n",
            "Train Epoch: 72 [3520/3527 (99%)]\tLoss: 0.823452\tLR:0.000058244\n",
            "6\n",
            "epoch:72,loss:1.000467726239213\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9182, Accuracy: 470/637 (74%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.71      0.75        69\n",
            "           1       0.73      0.68      0.70        56\n",
            "          10       0.71      0.74      0.73        43\n",
            "          11       0.85      0.72      0.78        47\n",
            "          12       0.64      0.80      0.71        55\n",
            "          13       0.52      0.43      0.47        30\n",
            "          14       0.92      0.94      0.93        47\n",
            "           2       1.00      0.50      0.67         2\n",
            "           3       0.73      0.80      0.77        41\n",
            "           4       0.62      0.75      0.68        20\n",
            "           5       0.76      0.71      0.74        35\n",
            "           6       0.95      0.95      0.95        44\n",
            "           7       0.71      0.88      0.78        56\n",
            "           8       0.55      0.57      0.56        47\n",
            "           9       0.75      0.53      0.62        45\n",
            "\n",
            "    accuracy                           0.74       637\n",
            "   macro avg       0.75      0.72      0.72       637\n",
            "weighted avg       0.74      0.74      0.74       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 73 [320/3527 (9%)]\tLoss: 1.531138\tLR:0.000065796\n",
            "Train Epoch: 73 [640/3527 (18%)]\tLoss: 0.924412\tLR:0.000065796\n",
            "Train Epoch: 73 [960/3527 (27%)]\tLoss: 0.890290\tLR:0.000065796\n",
            "Train Epoch: 73 [1280/3527 (36%)]\tLoss: 0.849585\tLR:0.000065796\n",
            "Train Epoch: 73 [1600/3527 (45%)]\tLoss: 0.896574\tLR:0.000065796\n",
            "Train Epoch: 73 [1920/3527 (54%)]\tLoss: 0.968240\tLR:0.000065796\n",
            "Train Epoch: 73 [2240/3527 (63%)]\tLoss: 0.806497\tLR:0.000065796\n",
            "Train Epoch: 73 [2560/3527 (72%)]\tLoss: 0.727025\tLR:0.000065796\n",
            "Train Epoch: 73 [2880/3527 (81%)]\tLoss: 0.909876\tLR:0.000065796\n",
            "Train Epoch: 73 [3200/3527 (90%)]\tLoss: 0.844307\tLR:0.000065796\n",
            "Train Epoch: 73 [3520/3527 (99%)]\tLoss: 0.990915\tLR:0.000065796\n",
            "6\n",
            "epoch:73,loss:1.0001301910426166\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9091, Accuracy: 464/637 (73%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.64      0.70        69\n",
            "           1       0.58      0.75      0.66        56\n",
            "          10       0.78      0.74      0.76        43\n",
            "          11       0.82      0.77      0.79        47\n",
            "          12       0.55      0.95      0.70        55\n",
            "          13       0.67      0.40      0.50        30\n",
            "          14       0.91      0.89      0.90        47\n",
            "           2       1.00      0.50      0.67         2\n",
            "           3       0.78      0.61      0.68        41\n",
            "           4       0.67      0.50      0.57        20\n",
            "           5       0.81      0.71      0.76        35\n",
            "           6       0.95      0.95      0.95        44\n",
            "           7       0.82      0.82      0.82        56\n",
            "           8       0.53      0.60      0.56        47\n",
            "           9       0.82      0.60      0.69        45\n",
            "\n",
            "    accuracy                           0.73       637\n",
            "   macro avg       0.76      0.70      0.71       637\n",
            "weighted avg       0.75      0.73      0.73       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 74 [320/3527 (9%)]\tLoss: 0.680073\tLR:0.000072973\n",
            "Train Epoch: 74 [640/3527 (18%)]\tLoss: 1.141307\tLR:0.000072973\n",
            "Train Epoch: 74 [960/3527 (27%)]\tLoss: 1.109055\tLR:0.000072973\n",
            "Train Epoch: 74 [1280/3527 (36%)]\tLoss: 0.974968\tLR:0.000072973\n",
            "Train Epoch: 74 [1600/3527 (45%)]\tLoss: 0.896786\tLR:0.000072973\n",
            "Train Epoch: 74 [1920/3527 (54%)]\tLoss: 0.979626\tLR:0.000072973\n",
            "Train Epoch: 74 [2240/3527 (63%)]\tLoss: 1.239369\tLR:0.000072973\n",
            "Train Epoch: 74 [2560/3527 (72%)]\tLoss: 0.796066\tLR:0.000072973\n",
            "Train Epoch: 74 [2880/3527 (81%)]\tLoss: 0.772565\tLR:0.000072973\n",
            "Train Epoch: 74 [3200/3527 (90%)]\tLoss: 0.911266\tLR:0.000072973\n",
            "Train Epoch: 74 [3520/3527 (99%)]\tLoss: 0.956863\tLR:0.000072973\n",
            "6\n",
            "epoch:74,loss:0.9780685821095029\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9034, Accuracy: 472/637 (74%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.77      0.79        69\n",
            "           1       0.68      0.75      0.71        56\n",
            "          10       0.75      0.70      0.72        43\n",
            "          11       0.80      0.79      0.80        47\n",
            "          12       0.63      0.82      0.71        55\n",
            "          13       0.52      0.40      0.45        30\n",
            "          14       0.88      0.96      0.92        47\n",
            "           2       1.00      0.50      0.67         2\n",
            "           3       0.75      0.73      0.74        41\n",
            "           4       0.73      0.55      0.63        20\n",
            "           5       0.79      0.74      0.76        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.66      0.91      0.77        56\n",
            "           8       0.55      0.47      0.51        47\n",
            "           9       0.86      0.56      0.68        45\n",
            "\n",
            "    accuracy                           0.74       637\n",
            "   macro avg       0.76      0.71      0.72       637\n",
            "weighted avg       0.75      0.74      0.74       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 75 [320/3527 (9%)]\tLoss: 0.811083\tLR:0.000079595\n",
            "Train Epoch: 75 [640/3527 (18%)]\tLoss: 0.785728\tLR:0.000079595\n",
            "Train Epoch: 75 [960/3527 (27%)]\tLoss: 1.031140\tLR:0.000079595\n",
            "Train Epoch: 75 [1280/3527 (36%)]\tLoss: 0.884365\tLR:0.000079595\n",
            "Train Epoch: 75 [1600/3527 (45%)]\tLoss: 0.793269\tLR:0.000079595\n",
            "Train Epoch: 75 [1920/3527 (54%)]\tLoss: 0.974868\tLR:0.000079595\n",
            "Train Epoch: 75 [2240/3527 (63%)]\tLoss: 0.946045\tLR:0.000079595\n",
            "Train Epoch: 75 [2560/3527 (72%)]\tLoss: 0.945181\tLR:0.000079595\n",
            "Train Epoch: 75 [2880/3527 (81%)]\tLoss: 0.797908\tLR:0.000079595\n",
            "Train Epoch: 75 [3200/3527 (90%)]\tLoss: 0.993554\tLR:0.000079595\n",
            "Train Epoch: 75 [3520/3527 (99%)]\tLoss: 0.949540\tLR:0.000079595\n",
            "6\n",
            "epoch:75,loss:1.0145239846126453\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8889, Accuracy: 482/637 (76%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.87      0.78        69\n",
            "           1       0.79      0.59      0.67        56\n",
            "          10       0.69      0.79      0.74        43\n",
            "          11       0.85      0.74      0.80        47\n",
            "          12       0.67      0.84      0.74        55\n",
            "          13       0.59      0.43      0.50        30\n",
            "          14       0.94      0.96      0.95        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.71      0.71      0.71        41\n",
            "           4       0.73      0.40      0.52        20\n",
            "           5       0.89      0.71      0.79        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.80      0.88      0.84        56\n",
            "           8       0.54      0.66      0.60        47\n",
            "           9       0.79      0.67      0.72        45\n",
            "\n",
            "    accuracy                           0.76       637\n",
            "   macro avg       0.78      0.75      0.75       637\n",
            "weighted avg       0.76      0.76      0.75       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 76 [320/3527 (9%)]\tLoss: 2.771379\tLR:0.000085502\n",
            "Train Epoch: 76 [640/3527 (18%)]\tLoss: 1.008813\tLR:0.000085502\n",
            "Train Epoch: 76 [960/3527 (27%)]\tLoss: 1.146839\tLR:0.000085502\n",
            "Train Epoch: 76 [1280/3527 (36%)]\tLoss: 1.017522\tLR:0.000085502\n",
            "Train Epoch: 76 [1600/3527 (45%)]\tLoss: 1.012171\tLR:0.000085502\n",
            "Train Epoch: 76 [1920/3527 (54%)]\tLoss: 0.897687\tLR:0.000085502\n",
            "Train Epoch: 76 [2240/3527 (63%)]\tLoss: 0.920452\tLR:0.000085502\n",
            "Train Epoch: 76 [2560/3527 (72%)]\tLoss: 0.993033\tLR:0.000085502\n",
            "Train Epoch: 76 [2880/3527 (81%)]\tLoss: 0.820523\tLR:0.000085502\n",
            "Train Epoch: 76 [3200/3527 (90%)]\tLoss: 1.371905\tLR:0.000085502\n",
            "Train Epoch: 76 [3520/3527 (99%)]\tLoss: 0.972030\tLR:0.000085502\n",
            "6\n",
            "epoch:76,loss:1.0042629655417021\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.8863, Accuracy: 472/637 (74%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.83      0.81        69\n",
            "           1       0.81      0.62      0.71        56\n",
            "          10       0.59      0.79      0.67        43\n",
            "          11       0.88      0.77      0.82        47\n",
            "          12       0.74      0.82      0.78        55\n",
            "          13       0.68      0.43      0.53        30\n",
            "          14       0.88      0.96      0.92        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.65      0.63      0.64        41\n",
            "           4       0.83      0.50      0.62        20\n",
            "           5       0.81      0.71      0.76        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.68      0.89      0.78        56\n",
            "           8       0.54      0.55      0.55        47\n",
            "           9       0.59      0.58      0.58        45\n",
            "\n",
            "    accuracy                           0.74       637\n",
            "   macro avg       0.76      0.74      0.74       637\n",
            "weighted avg       0.75      0.74      0.74       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 77 [320/3527 (9%)]\tLoss: 1.057443\tLR:0.000090546\n",
            "Train Epoch: 77 [640/3527 (18%)]\tLoss: 1.023115\tLR:0.000090546\n",
            "Train Epoch: 77 [960/3527 (27%)]\tLoss: 0.875869\tLR:0.000090546\n",
            "Train Epoch: 77 [1280/3527 (36%)]\tLoss: 0.982046\tLR:0.000090546\n",
            "Train Epoch: 77 [1600/3527 (45%)]\tLoss: 0.872964\tLR:0.000090546\n",
            "Train Epoch: 77 [1920/3527 (54%)]\tLoss: 0.885998\tLR:0.000090546\n",
            "Train Epoch: 77 [2240/3527 (63%)]\tLoss: 0.781910\tLR:0.000090546\n",
            "Train Epoch: 77 [2560/3527 (72%)]\tLoss: 0.989677\tLR:0.000090546\n",
            "Train Epoch: 77 [2880/3527 (81%)]\tLoss: 1.072234\tLR:0.000090546\n",
            "Train Epoch: 77 [3200/3527 (90%)]\tLoss: 0.894773\tLR:0.000090546\n",
            "Train Epoch: 77 [3520/3527 (99%)]\tLoss: 0.930848\tLR:0.000090546\n",
            "6\n",
            "epoch:77,loss:0.9856861700882783\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9096, Accuracy: 469/637 (74%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.78      0.78        69\n",
            "           1       0.75      0.70      0.72        56\n",
            "          10       0.73      0.74      0.74        43\n",
            "          11       0.90      0.74      0.81        47\n",
            "          12       0.71      0.76      0.74        55\n",
            "          13       0.52      0.47      0.49        30\n",
            "          14       0.90      0.96      0.93        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.71      0.66      0.68        41\n",
            "           4       0.62      0.40      0.48        20\n",
            "           5       0.68      0.77      0.72        35\n",
            "           6       0.98      0.93      0.95        44\n",
            "           7       0.78      0.82      0.80        56\n",
            "           8       0.48      0.62      0.54        47\n",
            "           9       0.67      0.62      0.64        45\n",
            "\n",
            "    accuracy                           0.74       637\n",
            "   macro avg       0.75      0.73      0.74       637\n",
            "weighted avg       0.74      0.74      0.74       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 78 [320/3527 (9%)]\tLoss: 0.873047\tLR:0.000094605\n",
            "Train Epoch: 78 [640/3527 (18%)]\tLoss: 0.873934\tLR:0.000094605\n",
            "Train Epoch: 78 [960/3527 (27%)]\tLoss: 0.909329\tLR:0.000094605\n",
            "Train Epoch: 78 [1280/3527 (36%)]\tLoss: 0.743193\tLR:0.000094605\n",
            "Train Epoch: 78 [1600/3527 (45%)]\tLoss: 0.751627\tLR:0.000094605\n",
            "Train Epoch: 78 [1920/3527 (54%)]\tLoss: 0.870367\tLR:0.000094605\n",
            "Train Epoch: 78 [2240/3527 (63%)]\tLoss: 0.846351\tLR:0.000094605\n",
            "Train Epoch: 78 [2560/3527 (72%)]\tLoss: 0.769826\tLR:0.000094605\n",
            "Train Epoch: 78 [2880/3527 (81%)]\tLoss: 0.783979\tLR:0.000094605\n",
            "Train Epoch: 78 [3200/3527 (90%)]\tLoss: 2.653258\tLR:0.000094605\n",
            "Train Epoch: 78 [3520/3527 (99%)]\tLoss: 0.762210\tLR:0.000094605\n",
            "6\n",
            "epoch:78,loss:0.9666082483154159\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9728, Accuracy: 450/637 (71%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.71      0.73        69\n",
            "           1       0.62      0.57      0.59        56\n",
            "          10       0.77      0.79      0.78        43\n",
            "          11       0.65      0.83      0.73        47\n",
            "          12       0.69      0.69      0.69        55\n",
            "          13       0.53      0.30      0.38        30\n",
            "          14       0.88      0.96      0.92        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.73      0.66      0.69        41\n",
            "           4       0.48      0.55      0.51        20\n",
            "           5       0.73      0.54      0.62        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.79      0.82      0.81        56\n",
            "           8       0.48      0.64      0.55        47\n",
            "           9       0.66      0.60      0.63        45\n",
            "\n",
            "    accuracy                           0.71       637\n",
            "   macro avg       0.72      0.71      0.71       637\n",
            "weighted avg       0.71      0.71      0.70       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 79 [320/3527 (9%)]\tLoss: 0.935930\tLR:0.000097577\n",
            "Train Epoch: 79 [640/3527 (18%)]\tLoss: 0.690810\tLR:0.000097577\n",
            "Train Epoch: 79 [960/3527 (27%)]\tLoss: 0.944517\tLR:0.000097577\n",
            "Train Epoch: 79 [1280/3527 (36%)]\tLoss: 0.953928\tLR:0.000097577\n",
            "Train Epoch: 79 [1600/3527 (45%)]\tLoss: 1.197939\tLR:0.000097577\n",
            "Train Epoch: 79 [1920/3527 (54%)]\tLoss: 1.038234\tLR:0.000097577\n",
            "Train Epoch: 79 [2240/3527 (63%)]\tLoss: 1.609123\tLR:0.000097577\n",
            "Train Epoch: 79 [2560/3527 (72%)]\tLoss: 0.993562\tLR:0.000097577\n",
            "Train Epoch: 79 [2880/3527 (81%)]\tLoss: 1.022121\tLR:0.000097577\n",
            "Train Epoch: 79 [3200/3527 (90%)]\tLoss: 0.822824\tLR:0.000097577\n",
            "Train Epoch: 79 [3520/3527 (99%)]\tLoss: 1.104349\tLR:0.000097577\n",
            "6\n",
            "epoch:79,loss:1.0358611871530343\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9273, Accuracy: 467/637 (73%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.75      0.78        69\n",
            "           1       0.71      0.64      0.67        56\n",
            "          10       0.62      0.74      0.67        43\n",
            "          11       0.89      0.72      0.80        47\n",
            "          12       0.65      0.71      0.68        55\n",
            "          13       0.50      0.47      0.48        30\n",
            "          14       0.90      0.98      0.94        47\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       0.83      0.59      0.69        41\n",
            "           4       0.70      0.70      0.70        20\n",
            "           5       0.76      0.71      0.74        35\n",
            "           6       0.98      0.91      0.94        44\n",
            "           7       0.70      0.91      0.79        56\n",
            "           8       0.56      0.60      0.58        47\n",
            "           9       0.68      0.67      0.67        45\n",
            "\n",
            "    accuracy                           0.73       637\n",
            "   macro avg       0.75      0.74      0.74       637\n",
            "weighted avg       0.74      0.73      0.73       637\n",
            "\n",
            "3527 111\n",
            "Train Epoch: 80 [320/3527 (9%)]\tLoss: 0.762775\tLR:0.000099391\n",
            "Train Epoch: 80 [640/3527 (18%)]\tLoss: 0.811795\tLR:0.000099391\n",
            "Train Epoch: 80 [960/3527 (27%)]\tLoss: 0.849583\tLR:0.000099391\n",
            "Train Epoch: 80 [1280/3527 (36%)]\tLoss: 0.861167\tLR:0.000099391\n",
            "Train Epoch: 80 [1600/3527 (45%)]\tLoss: 1.865506\tLR:0.000099391\n",
            "Train Epoch: 80 [1920/3527 (54%)]\tLoss: 0.998656\tLR:0.000099391\n",
            "Train Epoch: 80 [2240/3527 (63%)]\tLoss: 0.990703\tLR:0.000099391\n",
            "Train Epoch: 80 [2560/3527 (72%)]\tLoss: 1.147334\tLR:0.000099391\n",
            "Train Epoch: 80 [2880/3527 (81%)]\tLoss: 0.872372\tLR:0.000099391\n",
            "Train Epoch: 80 [3200/3527 (90%)]\tLoss: 0.773158\tLR:0.000099391\n",
            "Train Epoch: 80 [3520/3527 (99%)]\tLoss: 0.874899\tLR:0.000099391\n",
            "6\n",
            "epoch:80,loss:1.0550980508864463\n",
            "637 20\n",
            "\n",
            "Val set: Average loss: 0.9169, Accuracy: 481/637 (76%)\n",
            "\n",
            "Training complete in 0m 39s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.80      0.78        69\n",
            "           1       0.73      0.68      0.70        56\n",
            "          10       0.89      0.74      0.81        43\n",
            "          11       0.74      0.83      0.78        47\n",
            "          12       0.72      0.80      0.76        55\n",
            "          13       0.52      0.43      0.47        30\n",
            "          14       0.90      0.96      0.93        47\n",
            "           2       0.67      1.00      0.80         2\n",
            "           3       0.65      0.63      0.64        41\n",
            "           4       0.68      0.65      0.67        20\n",
            "           5       0.78      0.71      0.75        35\n",
            "           6       0.98      0.95      0.97        44\n",
            "           7       0.76      0.80      0.78        56\n",
            "           8       0.63      0.72      0.67        47\n",
            "           9       0.74      0.62      0.67        45\n",
            "\n",
            "    accuracy                           0.76       637\n",
            "   macro avg       0.74      0.76      0.75       637\n",
            "weighted avg       0.76      0.76      0.75       637\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#v0\n",
        "# 计算模型的总参数量\n",
        "total_params = sum(p.numel() for p in model_ft_v0.parameters())\n",
        "\n",
        "# 计算可训练的参数量\n",
        "trainable_params = sum(p.numel() for p in model_ft_v0.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Parameters: {total_params}\")\n",
        "print(f\"Trainable Parameters: {trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clwY0Z08vqls",
        "outputId": "f9b2e5d2-c170-4ba7-a0d5-ae80eef7c70e"
      },
      "id": "clwY0Z08vqls",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 5024240\n",
            "Trainable Parameters: 5024240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#v1\n",
        "# 计算模型的总参数量\n",
        "total_params = sum(p.numel() for p in model_ft_v1.parameters())\n",
        "\n",
        "# 计算可训练的参数量\n",
        "trainable_params = sum(p.numel() for p in model_ft_v1.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Parameters: {total_params}\")\n",
        "print(f\"Trainable Parameters: {trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBiRpf8IaSe_",
        "outputId": "81dd7edd-b39e-4469-f992-13b0eb263743"
      },
      "id": "lBiRpf8IaSe_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 5103262\n",
            "Trainable Parameters: 5103262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#v2\n",
        "# 计算模型的总参数量\n",
        "total_params = sum(p.numel() for p in model_ft_v2.parameters())\n",
        "\n",
        "# 计算可训练的参数量\n",
        "trainable_params = sum(p.numel() for p in model_ft_v2.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Parameters: {total_params}\")\n",
        "print(f\"Trainable Parameters: {trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1TCmUMmaRIP",
        "outputId": "f9d10638-3860-476a-edf1-cb5505d9233b"
      },
      "id": "W1TCmUMmaRIP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 4772382\n",
            "Trainable Parameters: 4772382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#v15\n",
        "# 计算模型的总参数量\n",
        "total_params = sum(p.numel() for p in model_ft_v1_5.parameters())\n",
        "\n",
        "# 计算可训练的参数量\n",
        "trainable_params = sum(p.numel() for p in model_ft_v1_5.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Parameters: {total_params}\")\n",
        "print(f\"Trainable Parameters: {trainable_params}\")"
      ],
      "metadata": {
        "id": "Zb04ehm-eAE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca3cc137-fa69-4d3c-d281-a67335f10dd1"
      },
      "id": "Zb04ehm-eAE2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 4700400\n",
            "Trainable Parameters: 4700400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8fc1351-0919-48ef-b993-8c5f235a6b58",
      "metadata": {
        "id": "e8fc1351-0919-48ef-b993-8c5f235a6b58"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "model = torch.load(\"/content/drive/MyDrive/myvit/mv2/modelv2_m15_0.99.pth\",map_location='cpu')\n",
        "model.eval()\n",
        "model.to(DEVICE)\n",
        "example = torch.rand(1, 3, 256, 256)\n",
        "traced_script_module = torch.jit.trace(model, example)\n",
        "optimized_traced_model = optimize_for_mobile(traced_script_module)\n",
        "optimized_traced_model._save_for_lite_interpreter(\"/content/drive/MyDrive/myvit/mv2/v2_1.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "# 准备数据加载器\n",
        "# transform_test = transforms.Compose([\n",
        "#     transforms.Resize((256, 256)),\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "# dataset_test = datasets.ImageFolder('/content/TSRD/TSRD_dataset/Test', transform=transform_test)\n",
        "# test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 函数定义: 评估模型准确率\n",
        "def evaluate_model(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "IWYm0hX4ghAr"
      },
      "id": "IWYm0hX4ghAr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cpu\")\n",
        "# 加载模型\n",
        "model1 = torch.load('/content/drive/MyDrive/myvit/mv0_tsrd/modelv0_TSRD.pth',map_location='cpu')\n",
        "# 设置为评估模式\n",
        "model1.eval()\n",
        "\n",
        "# 评估两个模型\n",
        "accuracy1 = evaluate_model(model1, test_loader)\n",
        "\n",
        "# 准确率列表，用于统计检验\n",
        "# 假设通过多次评估收集数据\n",
        "accuracies_model1 = [accuracy1] * 10"
      ],
      "metadata": {
        "id": "OJ9npsSletl4"
      },
      "id": "OJ9npsSletl4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cpu\")\n",
        "model2 = torch.load('/content/drive/MyDrive/myvit/mv2_tsrd/modelv2_TSRD.pth',map_location='cpu')\n",
        "model2.eval()\n",
        "accuracy2 = evaluate_model(model2, test_loader)\n",
        "accuracies_model2 = [accuracy2] * 10"
      ],
      "metadata": {
        "id": "jxepAW-dgMsf"
      },
      "id": "jxepAW-dgMsf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T检验\n",
        "t_stat, p_value = stats.ttest_rel(accuracies_model1, accuracies_model2)\n",
        "print(\"T检验结果：t统计量 =\", t_stat, \", p值 =\", p_value)\n",
        "\n",
        "# Wilcoxon符号秩检验\n",
        "wilcoxon_stat, wilcoxon_p = stats.wilcoxon(accuracies_model1, accuracies_model2)\n",
        "print(f'Wilcoxon检验结果：统计量 = {wilcoxon_stat}, p值 = {wilcoxon_p}')"
      ],
      "metadata": {
        "id": "eIsFzN-_gaLm"
      },
      "id": "eIsFzN-_gaLm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}