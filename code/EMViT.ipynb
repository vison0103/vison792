{"cells":[{"cell_type":"markdown","id":"qVnj4d_hEcE_","metadata":{"id":"qVnj4d_hEcE_"},"source":["E-MobileViT is modified and enhanced from the original MobileViT. See the paper for detailed changes.\n","The original MobileViT code references the code published by the official team, which can be found at:\n","\n","https://github.com/apple/ml-cvnets/blob/main/cvnets/models/classification/mobilevit.py\n"]},{"cell_type":"code","execution_count":null,"id":"Z4oAHOR6Eb1P","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32304,"status":"ok","timestamp":1717579084345,"user":{"displayName":"宋世骐","userId":"10822952913289090944"},"user_tz":-720},"id":"Z4oAHOR6Eb1P","outputId":"59b6c17a-ec98-4747-d379-012ff7a71408"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os"]},{"cell_type":"code","execution_count":null,"id":"QZTmwROb-Wjn","metadata":{"id":"QZTmwROb-Wjn"},"outputs":[],"source":["#GTSRB\n","import zipfile\n","with zipfile.ZipFile(\"/content/drive/MyDrive/myvit/datasets.zip\", 'r') as zip_ref:\n","    zip_ref.extractall(\"/content/\") # 我使用的解压位置"]},{"cell_type":"code","execution_count":null,"id":"gNCFGun_aaBR","metadata":{"id":"gNCFGun_aaBR"},"outputs":[],"source":["#BTC\n","import zipfile\n","with zipfile.ZipFile(\"/content/drive/MyDrive/myvit/BelgiumTSC.zip\", 'r') as zip_ref:\n","    zip_ref.extractall(\"/content/BelgiumTSC\") # 我使用的解压位置"]},{"cell_type":"code","execution_count":null,"id":"O28cIVXIok8q","metadata":{"id":"O28cIVXIok8q"},"outputs":[],"source":["#TSRD\n","import zipfile\n","with zipfile.ZipFile(\"/content/drive/MyDrive/myvit/TSRD_dataset.zip\", 'r') as zip_ref:\n","    zip_ref.extractall(\"/content/TSRD\") # 我使用的解压位置"]},{"cell_type":"code","execution_count":null,"id":"gAMxFB_1Mr-p","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"elapsed":4764,"status":"ok","timestamp":1717579105246,"user":{"displayName":"宋世骐","userId":"10822952913289090944"},"user_tz":-720},"id":"gAMxFB_1Mr-p","outputId":"cb1f7781-59fa-4fa5-a478-62f779a24a23"},"outputs":[{"name":"stdout","output_type":"stream","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2023 NVIDIA Corporation\n","Built on Tue_Aug_15_22:02:13_PDT_2023\n","Cuda compilation tools, release 12.2, V12.2.140\n","Build cuda_12.2.r12.2/compiler.33191640_0\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'12.1'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["!nvcc -V\n","import torch\n","torch.version.cuda"]},{"cell_type":"code","execution_count":null,"id":"EqdFnqUUjg5j","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6270,"status":"ok","timestamp":1717579119525,"user":{"displayName":"宋世骐","userId":"10822952913289090944"},"user_tz":-720},"id":"EqdFnqUUjg5j","outputId":"db1cb33b-3ee9-406c-aa8c-7ff2783d3fbf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.0)\n"]}],"source":["!pip install packaging\n","!pip install timm\n","!pip install einops"]},{"cell_type":"code","execution_count":null,"id":"f5c742b9-e2b3-4530-9baa-2e951fe0ee47","metadata":{"id":"f5c742b9-e2b3-4530-9baa-2e951fe0ee47"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.optim as optim\n","import torch.utils.data\n","import torch.utils.data.distributed\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from sklearn.metrics import classification_report\n","from timm.data.mixup import Mixup\n","from timm.loss import SoftTargetCrossEntropy\n","from timm.models.mobilevit import mobilevit_s\n","#from apex import amp\n","import warnings\n","import json\n","warnings.filterwarnings(\"ignore\")\n","import time\n","import os\n","import torch\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from einops import rearrange\n","from torch import nn\n","from torch.utils.data import random_split"]},{"cell_type":"code","execution_count":null,"id":"21fd1282-7415-42f1-b9a2-92284d6d2ca1","metadata":{"id":"21fd1282-7415-42f1-b9a2-92284d6d2ca1"},"outputs":[],"source":["# 设置全局参数\n","model_lr = 1e-4\n","BATCH_SIZE = 32\n","EPOCHS = 100\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","use_amp=False #是否使用混合精度\n","#classes=54\n","classes=43\n","#classes=53\n","CLIP_GRAD=5.0\n","# 数据预处理7\n","#p.value.\n","#t.TEST使用如t检验或Wilcoxon符号秩检验"]},{"cell_type":"code","execution_count":null,"id":"xOZFmLZ9LL_p","metadata":{"id":"xOZFmLZ9LL_p"},"outputs":[],"source":["from torchvision import transforms\n","\n","transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),  # 调整明暗度、对比度、饱和度\n","    transforms.RandomRotation(degrees=15),  # 在[-15, 15]度范围内随机旋转\n","    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 在x和y方向上各随机平移图像的10%\n","    transforms.ToTensor()\n","])\n","\n","\n","transform_test = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","])\n","\n","mixup_fn = Mixup(\n","    mixup_alpha=0.8, cutmix_alpha=0.0, cutmix_minmax=None,\n","    prob=0.1, switch_prob=0.5, mode='batch',\n","    label_smoothing=0.1, num_classes=classes)\n"]},{"cell_type":"code","execution_count":null,"id":"ga-A8f4M_7TL","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1717579298160,"user":{"displayName":"宋世骐","userId":"10822952913289090944"},"user_tz":-720},"id":"ga-A8f4M_7TL","outputId":"cea4f791-346b-48b3-e2c2-cc78efec771c"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'CarefulBikes': 0, 'CarefulChildren': 1, 'CarefulDangerous': 2, 'CarefulPedestrians': 3, 'CarefulSlipping': 4, 'CarefulSnow': 5, 'CarefulTrafficLight': 6, 'ConstructionRoad': 7, 'LeftTurn': 8, 'MainRoutePriority': 9, 'MustGoStraight': 10, 'MustGoStraightOrLeft': 11, 'MustGoStraightOrRight': 12, 'MustLeftSide': 13, 'MustRightSide': 14, 'MustTurnLeft': 15, 'MustTurnRight': 16, 'NeedsToYield': 17, 'NoAccess': 18, 'NoEntry': 19, 'NoOvertaking': 20, 'NoOvertakingRemove': 21, 'NoOvertakingTrucks': 22, 'NoOvertakingTrucksRemove': 23, 'NoTrucks': 24, 'PriorityRoad': 25, 'RightTurn': 26, 'RoadNarrowing': 27, 'SharpTurn': 28, 'SpeedLimit100': 29, 'SpeedLimit120': 30, 'SpeedLimit20': 31, 'SpeedLimit30': 32, 'SpeedLimit50': 33, 'SpeedLimit60': 34, 'SpeedLimit70': 35, 'SpeedLimit80': 36, 'SpeedLimit80Lifted': 37, 'Stop': 38, 'Turntable': 39, 'UnevenRoad': 40, 'Unrestrict': 41, 'WildAnimals': 42}\n"]}],"source":["#德国\n","dataset_train = datasets.ImageFolder('/content/datasets/train', transform=transform)\n","dataset_test = datasets.ImageFolder('/content/datasets/test', transform=transform_test)\n","train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n","#德国\n","print(dataset_train.class_to_idx)\n","with open('/content/datasets/class.txt','w') as file:\n","    file.write(str(dataset_train.class_to_idx))\n","with open('/content/datasets/class.json','w',encoding='utf-8') as file:\n","    file.write(json.dumps(dataset_train.class_to_idx))"]},{"cell_type":"code","execution_count":null,"id":"_Ah8BgI-bzKd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":375,"status":"ok","timestamp":1716089741835,"user":{"displayName":"宋世骐","userId":"10822952913289090944"},"user_tz":-720},"id":"_Ah8BgI-bzKd","outputId":"2e338dc0-5ef2-4487-fd8d-f2db093dd62a"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'00000': 0, '00001': 1, '00002': 2, '00003': 3, '00004': 4, '00005': 5, '00006': 6, '00007': 7, '00008': 8, '00010': 9, '00012': 10, '00013': 11, '00014': 12, '00016': 13, '00017': 14, '00018': 15, '00019': 16, '00020': 17, '00021': 18, '00022': 19, '00023': 20, '00024': 21, '00025': 22, '00027': 23, '00028': 24, '00029': 25, '00030': 26, '00031': 27, '00032': 28, '00034': 29, '00035': 30, '00037': 31, '00038': 32, '00039': 33, '00040': 34, '00041': 35, '00042': 36, '00043': 37, '00044': 38, '00045': 39, '00046': 40, '00047': 41, '00049': 42, '00051': 43, '00053': 44, '00054': 45, '00055': 46, '00056': 47, '00057': 48, '00058': 49, '00059': 50, '00060': 51, '00061': 52}\n"]}],"source":["#比利时\n","dataset_train = datasets.ImageFolder('/content/BelgiumTSC/BelgiumTSC/Training', transform=transform)\n","dataset_test = datasets.ImageFolder('/content/BelgiumTSC/BelgiumTSC/Testing', transform=transform_test)\n","train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n","\n","print(dataset_train.class_to_idx)\n","with open('/content/BelgiumTSC/BelgiumTSC/class.txt','w') as file:\n","    file.write(str(dataset_train.class_to_idx))\n","with open('/content/BelgiumTSC/BelgiumTSC/class.json','w',encoding='utf-8') as file:\n","    file.write(json.dumps(dataset_train.class_to_idx))"]},{"cell_type":"code","execution_count":null,"id":"_29uBPNup0nI","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":316,"status":"ok","timestamp":1716092906847,"user":{"displayName":"宋世骐","userId":"10822952913289090944"},"user_tz":-720},"id":"_29uBPNup0nI","outputId":"faada686-943d-4c1f-d1aa-12b64f6f9828"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'000': 0, '001': 1, '002': 2, '003': 3, '004': 4, '005': 5, '006': 6, '007': 7, '008': 8, '010': 9, '011': 10, '012': 11, '013': 12, '014': 13, '015': 14, '016': 15, '017': 16, '020': 17, '021': 18, '022': 19, '023': 20, '024': 21, '025': 22, '026': 23, '027': 24, '028': 25, '029': 26, '030': 27, '031': 28, '032': 29, '034': 30, '035': 31, '036': 32, '037': 33, '038': 34, '039': 35, '040': 36, '041': 37, '042': 38, '043': 39, '044': 40, '045': 41, '046': 42, '047': 43, '048': 44, '049': 45, '050': 46, '051': 47, '052': 48, '053': 49, '054': 50, '055': 51, '056': 52, '057': 53}\n"]}],"source":["#中国\n","dataset_train = datasets.ImageFolder('/content/TSRD/TSRD_dataset/Train', transform=transform)\n","\n","train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n","dataset_test = datasets.ImageFolder('/content/TSRD/TSRD_dataset/Test', transform=transform_test)\n","test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n","\n","print(dataset_train.class_to_idx)\n","with open('/content/TSRD/TSRD_dataset/class.txt','w') as file:\n","    file.write(str(dataset_train.class_to_idx))\n","with open('/content/TSRD/TSRD_dataset/class.json','w',encoding='utf-8') as file:\n","    file.write(json.dumps(dataset_train.class_to_idx))"]},{"cell_type":"code","execution_count":null,"id":"76Se5oQfPV0-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":529,"status":"ok","timestamp":1717579303606,"user":{"displayName":"宋世骐","userId":"10822952913289090944"},"user_tz":-720},"id":"76Se5oQfPV0-","outputId":"866853ae-d55b-43b1-8062-5837b19700de"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of classes in training set: 43\n","Number of classes in testing set: 43\n","Both datasets have the same number of classes.\n","All classes match between training and testing sets.\n"]}],"source":["# 检查训练集和测试集的类别数量\n","train_classes = dataset_train.class_to_idx\n","test_classes = dataset_test.class_to_idx\n","\n","# 打印类别数量\n","print(\"Number of classes in training set:\", len(train_classes))\n","print(\"Number of classes in testing set:\", len(test_classes))\n","\n","# 检查类别数量是否相等\n","if len(train_classes) == len(test_classes):\n","    print(\"Both datasets have the same number of classes.\")\n","else:\n","    print(\"Mismatch in the number of classes between datasets.\")\n","\n","# 检查具体的类别是否一致\n","train_class_set = set(train_classes.keys())\n","test_class_set = set(test_classes.keys())\n","\n","if train_class_set == test_class_set:\n","    print(\"All classes match between training and testing sets.\")\n","else:\n","    print(\"There are differences in class names between the datasets.\")\n","    # 找出不匹配的类别\n","    only_in_train = train_class_set - test_class_set\n","    only_in_test = test_class_set - train_class_set\n","    print(\"Classes only in training set:\", only_in_train)\n","    print(\"Classes only in testing set:\", only_in_test)\n"]},{"cell_type":"markdown","id":"KVIf6xvdUoHD","metadata":{"id":"KVIf6xvdUoHD"},"source":["E-MobileViT"]},{"cell_type":"code","execution_count":null,"id":"c5ze0HpbKwCZ","metadata":{"id":"c5ze0HpbKwCZ"},"outputs":[],"source":["#E-MobileViT\n","def _make_divisible(v, divisor, min_value=None):\n","\tif min_value is None:\n","\t\tmin_value = divisor\n","\tnew_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n","\t# Make sure that round down does not go down by more than 10%.\n","\tif new_v < 0.9 * v:\n","\t\tnew_v += divisor\n","\treturn new_v\n","\n","\n","def Conv_BN_ReLU(inp, oup, kernel, stride=1):\n","\treturn nn.Sequential(\n","\t\tnn.Conv2d(inp, oup, kernel_size=kernel, stride=stride, padding=1, bias=False),\n","\t\tnn.BatchNorm2d(oup),\n","\t\tnn.ReLU6(inplace=True)\n","\t)\n","\n","\n","def conv_1x1_bn(inp, oup):\n","\treturn nn.Sequential(\n","\t\tnn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n","\t\tnn.BatchNorm2d(oup),\n","\t\tnn.ReLU6(inplace=True)\n","\t)\n","\n","\n","class PreNorm(nn.Module):\n","\tdef __init__(self, dim, fn):\n","\t\tsuper().__init__()\n","\t\tself.norm = nn.LayerNorm(dim)\n","\t\tself.fn = fn\n","\n","\tdef forward(self, x, **kwargs):\n","\t\treturn self.fn(self.norm(x), **kwargs)\n","\n","\n","class FeedForward(nn.Module):\n","\tdef __init__(self, dim, hidden_dim, dropout=0.):\n","\t\tsuper().__init__()\n","\t\tself.ffn = nn.Sequential(\n","\t\t\tnn.Linear(dim, hidden_dim),\n","\t\t\tnn.SiLU(),\n","\t\t\tnn.Dropout(dropout),\n","\t\t\tnn.Linear(hidden_dim, dim),\n","\t\t\tnn.Dropout(dropout)\n","\t\t)\n","\n","\tdef forward(self, x):\n","\t\treturn self.ffn(x)\n","\n","\n","class Attention(nn.Module):\n","\tdef __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n","\t\tsuper().__init__()\n","\t\tinner_dim = dim_head * heads\n","\t\tproject_out = not (heads == 1 and dim_head == dim)\n","\n","\t\tself.heads = heads\n","\t\tself.scale = dim_head ** -0.5\n","\n","\t\tself.attend = nn.Softmax(dim=-1)\n","\t\tself.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n","\n","\t\tself.to_out = nn.Sequential(\n","\t\t\tnn.Linear(inner_dim, dim),\n","\t\t\tnn.Dropout(dropout)\n","\t\t) if project_out else nn.Identity()\n","\n","\tdef forward(self, x):\n","\t\tqkv = self.to_qkv(x).chunk(3, dim=-1)\n","\t\tq, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h=self.heads), qkv)\n","\n","\t\tdots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n","\t\tattn = self.attend(dots)\n","\t\tout = torch.matmul(attn, v)\n","\t\tout = rearrange(out, 'b p h n d -> b p n (h d)')\n","\t\treturn self.to_out(out)\n","\n","\n","class Transformer(nn.Module):\n","\tdef __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n","\t\tsuper().__init__()\n","\t\tself.layers = nn.ModuleList([])\n","\t\tfor _ in range(depth):\n","\t\t\tself.layers.append(nn.ModuleList([\n","\t\t\t\tPreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n","\t\t\t\tPreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n","\t\t\t]))\n","\n","\tdef forward(self, x):\n","\t\tfor attn, ff in self.layers:\n","\t\t\tx = attn(x) + x\n","\t\t\tx = ff(x) + x\n","\t\treturn x\n","\n","class ChannelAttention(nn.Module):\n","    def __init__(self, in_planes, ratio=16):\n","        super(ChannelAttention, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.max_pool = nn.AdaptiveMaxPool2d(1)\n","        self.fc = nn.Sequential(\n","            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n","        )\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = self.fc(self.avg_pool(x))\n","        max_out = self.fc(self.max_pool(x))\n","        out = avg_out + max_out\n","        return self.sigmoid(out)\n","\n","class SpatialAttention(nn.Module):\n","    def __init__(self, kernel_size=7):\n","        super(SpatialAttention, self).__init__()\n","        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n","        padding = 3 if kernel_size == 7 else 1\n","        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = torch.mean(x, dim=1, keepdim=True)\n","        max_out, _ = torch.max(x, dim=1, keepdim=True)\n","        x = torch.cat([avg_out, max_out], dim=1)\n","        x = self.conv(x)\n","        return self.sigmoid(x)\n","\n","class CBAM(nn.Module):\n","    def __init__(self, in_planes, ratio=16, kernel_size=7):\n","        super(CBAM, self).__init__()\n","        self.ca = ChannelAttention(in_planes, ratio)\n","        self.sa = SpatialAttention(kernel_size)\n","\n","    def forward(self, x):\n","        out = x * self.ca(x)\n","        return out * self.sa(out)\n","\n","class MV2Block(nn.Module):\n","    def __init__(self, inp, oup, stride=1, expand_ratio=4):\n","        super(MV2Block, self).__init__()\n","        assert stride in [1, 2]\n","\n","        hidden_dim = round(inp * expand_ratio)\n","        self.identity = stride == 1 and inp == oup\n","\n","        if expand_ratio == 1:\n","            self.conv = nn.Sequential(\n","                # dw\n","                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n","                nn.BatchNorm2d(hidden_dim),\n","                # nn.ReLU6(inplace=True),\n","                nn.SiLU(),\n","                # CBAM 注意力机制放置在 3x3 卷积层之后\n","                CBAM(hidden_dim),  # 添加 CBAM 注意力机制\n","                # pw-linear\n","                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n","                nn.BatchNorm2d(oup),\n","            )\n","        else:\n","            self.conv = nn.Sequential(\n","                # pw\n","                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n","                nn.BatchNorm2d(hidden_dim),\n","                # nn.ReLU6(inplace=True),\n","                nn.SiLU(),\n","                # dw\n","                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n","                nn.BatchNorm2d(hidden_dim),\n","                # nn.ReLU6(inplace=True),\n","                nn.SiLU(),\n","                # CBAM 注意力机制放置在 3x3 卷积层之后\n","                CBAM(hidden_dim),  # 添加 CBAM 注意力机制\n","                # pw-linear\n","                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n","                nn.BatchNorm2d(oup),\n","            )\n","\n","    def forward(self, x):\n","        if self.identity:\n","            return x + self.conv(x)\n","        else:\n","            return self.conv(x)\n","\n","# 实现ELA注意力机制\n","class ELAAttention(nn.Module):\n","    def __init__(self, channel, kernel_size):\n","        super().__init__()\n","        self.kernel_size = kernel_size\n","        self.pad = kernel_size // 2\n","        self.conv = nn.Conv1d(channel, channel, kernel_size, padding=self.pad, groups=channel, bias=False)\n","        self.gn = nn.GroupNorm(16, channel)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        b, c, h, w = x.size()\n","        x_h = torch.mean(x, dim=3, keepdim=True).view(b, c, h)\n","        x_w = torch.mean(x, dim=2, keepdim=True).view(b, c, w)\n","\n","        x_h = self.sigmoid(self.gn(self.conv(x_h))).view(b, c, h, 1)\n","        x_w = self.sigmoid(self.gn(self.conv(x_w))).view(b, c, 1, w)\n","        return x * x_h * x_w\n","\n","\n","\n","class MobileViTBlock(nn.Module):\n","    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n","        super().__init__()\n","        self.ph, self.pw = patch_size\n","\n","        self.conv1 = Conv_BN_ReLU(channel, channel, kernel_size)\n","        self.conv2 = conv_1x1_bn(channel, dim)\n","\n","        # Transformer module includes attention mechanism internally\n","        self.transformer = Transformer(dim, depth, 1, 32, mlp_dim, dropout)\n","\n","        # ELA Attention module\n","        self.ela_attention = ELAAttention(channel, kernel_size)\n","\n","        # 1x1 convolution to adjust the channel size of y to dim\n","        self.conv_y = conv_1x1_bn(channel, dim)\n","\n","        # We will apply Transformer and ELAAttention in parallel to the feature map\n","        # Then, we fuse their outputs along with the original feature map from conv2\n","\n","        # Since we're fusing three feature maps: original, Transformer output, and ELA output,\n","        # we'll have 3 * channel output channels before reducing it back to the original channel size.\n","        self.conv3 = conv_1x1_bn(dim * 2 + channel, channel)  # Adjust for the tripled channel size\n","        self.conv4 = Conv_BN_ReLU(channel, channel, kernel_size)  # Process fused feature map\n","\n","    def forward(self, x):\n","        y = x.clone()\n","\n","        # Local representations\n","        x_local = self.conv1(x)\n","        x_local = self.conv2(x_local)\n","\n","        # 计算原始特征图的空间维度\n","        _, _, h, w = x_local.shape\n","\n","        # Transformer representations\n","        # 调整 x_local 的形状以适应 Transformer 的输入要求\n","        x_transformed = rearrange(x_local, 'b d (h ph) (w pw) -> b (h w) (ph pw) d', ph=self.ph, pw=self.pw)\n","        x_transformed = self.transformer(x_transformed)\n","        # 将 Transformer 的输出重新排列回原始的空间维度\n","        x_transformed = rearrange(x_transformed, 'b (h w) (ph pw) d -> b d (h ph) (w pw)', ph=self.ph, pw=self.pw, h=h // self.ph, w=w // self.pw)\n","\n","        # ELA Attention representations\n","        x_ela = self.ela_attention(y)\n","\n","        y = self.conv_y(y)\n","\n","        # Fusion of features from both attentions and the original\n","        # 确保x_ela, x_transformed, 和 x_local 在空间维度上一致\n","        x_fused = torch.cat((y, x_ela, x_transformed), 1)\n","\n","        # Reduce channel size and process\n","        x_reduced = self.conv3(x_fused)\n","        x_final = self.conv4(x_reduced)\n","\n","        return x_final\n","\n","\n","\n","\n","class MobileVit_v2(nn.Module):\n","\tdef __init__(self, image_size, dims, channels, num_classes, expansion=4, kernel_size=3, patch_size=(2, 2)):\n","\t\tsuper().__init__()\n","\t\tih, iw = image_size\n","\t\tph, pw = patch_size\n","\t\tassert ih % ph == 0 and iw % pw == 0\n","\n","\t\tL = [2, 4, 3]\n","\n","\t\tself.conv1 = Conv_BN_ReLU(3, channels[0], kernel=3, stride=2)\n","\n","\t\tself.mv2 = nn.ModuleList([])\n","\t\tself.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n","\t\tself.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n","\t\tself.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n","\t\tself.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))  # Repeat\n","\t\tself.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n","\t\tself.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n","\t\tself.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n","\n","\t\tself.mvit = nn.ModuleList([])\n","\t\tself.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0] * 2)))\n","\t\tself.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1] * 4)))\n","\t\tself.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2] * 4)))\n","\n","\t\tself.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n","\n","\t\tself.pool = nn.AvgPool2d(ih // 32, 1)\n","\t\tself.fc = nn.Linear(channels[-1], num_classes, bias=False)\n","\n","\tdef forward(self, x):\n","\t\tx = self.conv1(x)\n","\t\tx = self.mv2[0](x)\n","\n","\t\tx = self.mv2[1](x)\n","\t\tx = self.mv2[2](x)\n","\t\tx = self.mv2[3](x)  # Repeat\n","\n","\t\tx = self.mv2[4](x)\n","\t\tx = self.mvit[0](x)\n","\n","\t\tx = self.mv2[5](x)\n","\t\tx = self.mvit[1](x)\n","\n","\t\tx = self.mv2[6](x)\n","\t\tx = self.mvit[2](x)\n","\t\tx = self.conv2(x)\n","\n","\t\tx = self.pool(x).view(-1, x.shape[1])\n","\t\tx = self.fc(x)\n","\t\treturn x\n","\n","\n","def mobilevit_xxs_v2():\n","\tdims = [64, 80, 96]\n","\tchannels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320]\n","\treturn MobileVit_v2((256, 256), dims, channels, num_classes=1000, expansion=2)\n","\n","\n","def mobilevit_xs_v2():\n","\tdims = [96, 120, 144]\n","\tchannels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384]\n","\treturn MobileVit_v2((256, 256), dims, channels, num_classes=1000)\n","\n","\n","def mobilevit_s_v2():\n","\tdims = [144, 192, 240]\n","\tchannels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640]\n","\treturn MobileVit_v2((256, 256), dims, channels, num_classes=43)"]},{"cell_type":"code","execution_count":null,"id":"YXIq2L8Puir8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":553,"status":"ok","timestamp":1717580430600,"user":{"displayName":"宋世骐","userId":"10822952913289090944"},"user_tz":-720},"id":"YXIq2L8Puir8","outputId":"b88f9ab6-0f60-45f7-ae0d-ee6af85f06cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final output shape: torch.Size([1, 43])\n"]}],"source":["# Example usage\n","model = mobilevit_s_v2()\n","dummy_input = torch.randn(1, 3, 256, 256)\n","output = model(dummy_input)\n","print(f'Final output shape: {output.shape}')"]},{"cell_type":"code","execution_count":null,"id":"K7e7jG_vOwii","metadata":{"id":"K7e7jG_vOwii"},"outputs":[],"source":["# 实例化模型并且移动到GPU\n","#criterion_train = SoftTargetCrossEntropy()# 训练用的loss\n","criterion_train = torch.nn.CrossEntropyLoss()# 训练用的loss\n","criterion_val = torch.nn.CrossEntropyLoss()# 验证用的loss\n","model_ft_v2 = mobilevit_s_v2()# 定义模型，并设置预训练# 修改类别\n","model_ft_v2.to(DEVICE)\n","# 选择简单暴力的Adam优化器，学习率调低\n","optimizer = optim.Adam(model_ft_v2.parameters(), lr=model_lr)\n","cosine_schedule = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=20, eta_min=1e-6)# 使用余弦退火算法调整学习率\n","if use_amp: #如果使用混合精度训练，则初始化amp。\n","    model, optimizer = amp.initialize(model_ft_v2, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”\n","if torch.cuda.device_count() > 1: #检测是否存在多张显卡，如果存在则使用DP的方式并行训练\n","    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n","    model_ft = torch.nn.DataParallel(model_ft_v2)"]},{"cell_type":"code","execution_count":null,"id":"K5GwEXRkO0KZ","metadata":{"id":"K5GwEXRkO0KZ"},"outputs":[],"source":["def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    sum_loss = 0\n","    total_num = len(train_loader.dataset)\n","    print(total_num, len(train_loader))\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        if len(data) % 2 != 0:\n","            if len(data) < 2:\n","                continue\n","            data = data[0:len(data) - 1]\n","            target = target[0:len(target) - 1]\n","            print(len(data))\n","        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n","        samples, targets = mixup_fn(data, target)\n","        output = model(data)\n","        loss = criterion_train(output, targets)\n","        optimizer.zero_grad()\n","        if use_amp:\n","            with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                scaled_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), CLIP_GRAD)\n","        else:\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n","        optimizer.step()\n","        lr = optimizer.state_dict()['param_groups'][0]['lr']\n","        print_loss = loss.data.item()\n","        sum_loss += print_loss\n","        if (batch_idx + 1) % 10 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR:{:.9f}'.format(\n","                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n","                       100. * (batch_idx + 1) / len(train_loader), loss.item(), lr))\n","    ave_loss = sum_loss / len(train_loader)\n","    print('epoch:{},loss:{}'.format(epoch, ave_loss))"]},{"cell_type":"code","execution_count":null,"id":"t4IvMeRyO7-0","metadata":{"id":"t4IvMeRyO7-0"},"outputs":[],"source":["ACC = 0"]},{"cell_type":"code","execution_count":null,"id":"SQEDGukyO_jt","metadata":{"id":"SQEDGukyO_jt"},"outputs":[],"source":["#ACC = 0\n","# 验证过程\n","def val(model, device, test_loader):\n","    global ACC\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    total_num = len(test_loader.dataset)\n","    print(total_num, len(test_loader))\n","    val_list = []\n","    pred_list = []\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            for t in target:\n","                val_list.append(t.data.item())\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            loss = criterion_val(output, target)\n","            _, pred = torch.max(output.data, 1)\n","            for p in pred:\n","                pred_list.append(p.data.item())\n","            correct += torch.sum(pred == target)\n","            print_loss = loss.data.item()\n","            test_loss += print_loss\n","        correct = correct.data.item()\n","        acc = correct / total_num\n","        avgloss = test_loss / len(test_loader)\n","        print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","            avgloss, correct, len(test_loader.dataset), 100 * acc))\n","        if acc > ACC:\n","            if isinstance(model, torch.nn.DataParallel):\n","                torch.save(model.module, 'modelv2_GT_m' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n","            else:\n","                torch.save(model, 'modelv2_GT_m' + str(epoch) + '_' + str(round(acc, 3)) + '.pth')\n","            ACC = acc\n","    return val_list, pred_list"]},{"cell_type":"code","execution_count":null,"id":"cp5uFgLqPP4k","metadata":{"id":"cp5uFgLqPP4k"},"outputs":[],"source":["# 训练\n","torch.cuda.empty_cache()\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","is_set_lr = False\n","for epoch in range(1, EPOCHS + 1):\n","    since = time.time()\n","    train(model_ft_v2, DEVICE, train_loader, optimizer, epoch)\n","    if epoch < 600:\n","        cosine_schedule.step()\n","    else:\n","        if is_set_lr:\n","            continue\n","        for param_group in optimizer.param_groups:\n","            param_group[\"lr\"] = 1e-6\n","            is_set_lr = True\n","    val_list, pred_list = val(model_ft_v2, DEVICE, test_loader)\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print(classification_report(val_list, pred_list, target_names=dataset_train.class_to_idx))\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"id":"W1TCmUMmaRIP","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":537,"status":"ok","timestamp":1717580452817,"user":{"displayName":"宋世骐","userId":"10822952913289090944"},"user_tz":-720},"id":"W1TCmUMmaRIP","outputId":"f9d10638-3860-476a-edf1-cb5505d9233b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Parameters: 4772382\n","Trainable Parameters: 4772382\n"]}],"source":["#v2\n","# 计算模型的总参数量\n","total_params = sum(p.numel() for p in model_ft_v2.parameters())\n","\n","# 计算可训练的参数量\n","trainable_params = sum(p.numel() for p in model_ft_v2.parameters() if p.requires_grad)\n","\n","print(f\"Total Parameters: {total_params}\")\n","print(f\"Trainable Parameters: {trainable_params}\")"]},{"cell_type":"code","execution_count":null,"id":"e8fc1351-0919-48ef-b993-8c5f235a6b58","metadata":{"id":"e8fc1351-0919-48ef-b993-8c5f235a6b58"},"outputs":[],"source":["import torch\n","import torchvision\n","from torch.utils.mobile_optimizer import optimize_for_mobile\n","DEVICE = torch.device(\"cpu\")\n","model = torch.load(\"/content/drive/MyDrive/myvit/mv2/modelv2_m15_0.99.pth\",map_location='cpu')\n","model.eval()\n","model.to(DEVICE)\n","example = torch.rand(1, 3, 256, 256)\n","traced_script_module = torch.jit.trace(model, example)\n","optimized_traced_model = optimize_for_mobile(traced_script_module)\n","optimized_traced_model._save_for_lite_interpreter(\"/content/drive/MyDrive/myvit/mv2/v2_1.pt\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":5}
